title,selftext,subreddit,upvotes,num_comments,created_utc,id,author,url,flair
"[D] The ""it"" in AI models is really just the dataset?",,MachineLearning,1313,272,1714819651.0,1cjxh9u,vijayabhaskar96,https://i.redd.it/uadactn53eyc1.png,Discussion
[N] 2024 Nobel Prize for Physics goes to ML and DNN researchers J. Hopfield and G. Hinton,"Announcement: https://x.com/NobelPrize/status/1843589140455272810

Our boys John Hopfield and Geoffrey Hinton were rewarded for their foundational contributions to machine learning and deep learning with the Nobel prize for physics 2024!

I hear furious Schmidhuber noises in the distance!

On a more serious note, despite the very surprising choice, I am generally happy - as a physicist myself with strong interest in ML, I love this physics-ML cinematic universe crossover.

The restriction to Hopfield and Hinton will probably spark discussions about the relative importance of {Hopfield, Hinton, LeCun, Schmidhuber, Bengio, Linnainmaa, ...} for the success of modern ML/DL/AI.
A discussion especially Schmidhuber very actively engages in.

The response from the core physics community however is rather mixed, as shown in the [/r/physics thread](https://www.reddit.com/r/Physics/comments/1fyw12p/the_2024_nobel_prize_in_physics_is_awarded_to/). There, the missing link/connection to physics research is noted and the concurrent ""loss"" of the '24 prize for physics researchers.",MachineLearning,1153,306,1728383190.0,1fywi9h,PrittEnergizer,https://www.reddit.com/r/MachineLearning/comments/1fywi9h/n_2024_nobel_prize_for_physics_goes_to_ml_and_dnn/,News
[D] Why do PhD Students in the US seem like overpowered final bosses ,"Hello,

I'm a PhD student in a European university, working on AI/ML/CV ..etc. my PhD is 4 years. The first year I literally just spent learning how to actually do research, teaching one course to learn how things work...etc. Second year, I published my first publication as a co-author in CVPR. By third year, I can manage research projects, I understand how to do grants applications, how funding works, the politics of it all ...etc. I added to my CV, 2 publications, one journal and another conference as first author. I'm very involved in industry and I also write a lot of production grade code in regard to AI, systems architecture, backend, cloud, deployment, etc for companies that have contracts with my lab.

The issue is when I see PhD students similar to me in the US, they be having 10 publications, 5 of them 1st author, all of them are either CVPR, ICML, ICLR, NeurIPS ...etc. I don't understand, do these people not sleep ? How are they able to achieve this crazy amount of work and still have 3 publications every year in A\* journals ?

I don't think these people are smarter than I, usually I get ideas and I look up if something exists, and I can see that something was just published by some PhD student in Stanford or DeepMind ..etc like 1 month ago, So I can see that my reasoning isn't late in regard to SOTA. but the concepts that you would need to grasp to just have one of those publications + the effort and the time you need to invest and the resources to get everything done, wouldn't be possible for 2\~3 months project. How is it possible for these people to do this ?

Thank you !",MachineLearning,1111,275,1729358853.0,1g7dzkp,None,https://www.reddit.com/r/MachineLearning/comments/1g7dzkp/d_why_do_phd_students_in_the_us_seem_like/,Discussion
[D]Stuck in AI Hell: What to do in post LLM world,"
Hey Reddit,

I’ve been in an AI/ML role for a few years now, and I’m starting to feel disconnected from the work. When I started, deep learning models were getting good, and I quickly fell in love with designing architectures, training models, and fine-tuning them for specific use cases. Seeing a loss curve finally converge, experimenting with layers, and debugging training runs—it all felt like a craft, a blend of science and creativity. I enjoyed implementing research papers to see how things worked under the hood. Backprop, gradients, optimization—it was a mental workout I loved.

But these days, it feels like everything has shifted. LLMs dominate the scene, and instead of building and training models, the focus is on using pre-trained APIs, crafting prompt chains, and setting up integrations. Sure, there’s engineering involved, but it feels less like creating and more like assembling. I miss the hands-on nature of experimenting with architectures and solving math-heavy problems.

It’s not just the creativity I miss. The economics of this new era also feel strange to me. Back when I started, compute was a luxury. We had limited GPUs, and a lot of the work was about being resourceful—quantizing models, distilling them, removing layers, and squeezing every bit of performance out of constrained setups. Now, it feels like no one cares about cost. We’re paying by tokens. Tokens! Who would’ve thought we’d get to a point where we’re not designing efficient models but feeding pre-trained giants like they’re vending machines?

I get it—abstraction has always been part of the field. TensorFlow and PyTorch abstracted tensor operations, Python abstracts C. But deep learning still left room for creation. We weren’t just abstracting away math; we were solving it. We could experiment, fail, and tweak. Working with LLMs doesn’t feel the same. It’s like fitting pieces into a pre-defined puzzle instead of building the puzzle itself.

I understand that LLMs are here to stay. They’re incredible tools, and I respect their potential to revolutionize industries. Building real-world products with them is still challenging, requiring a deep understanding of engineering, prompt design, and integrating them effectively into workflows. By no means is it an “easy” task. But the work doesn’t give me the same thrill. It’s not about solving math or optimization problems—it’s about gluing together APIs, tweaking outputs, and wrestling with opaque systems. It’s like we’ve traded craftsmanship for convenience.

Which brings me to my questions:

1. Is there still room for those of us who enjoy the deep work of model design and training? Or is this the inevitable evolution of the field, where everything converges on pre-trained systems?


2. What use cases still need traditional ML expertise? Are there industries or problems that will always require specialized models instead of general-purpose LLMs?


3. Am I missing the bigger picture here? LLMs feel like the “kernel” of a new computing paradigm, and we don’t fully understand their second- and third-order effects. Could this shift lead to new, exciting opportunities I’m just not seeing yet?


4. How do you stay inspired when the focus shifts? I still love AI, but I miss the feeling of building something from scratch. Is this just a matter of adapting my mindset, or should I seek out niches where traditional ML still thrives?



I’m not asking this to rant (though clearly, I needed to get some of this off my chest). I want to figure out where to go next from here. If you’ve been in AI/ML long enough to see major shifts—like the move from feature engineering to deep learning—how did you navigate them? What advice would you give someone in my position?

And yeah, before anyone roasts me for using an LLM to structure this post (guilty!), I just wanted to get my thoughts out in a coherent way. Guess that’s a sign of where we’re headed, huh?

Thanks for reading, and I’d love to hear your thoughts!

TL;DR: I entered AI during the deep learning boom, fell in love with designing and training models, and thrived on creativity, math, and optimization. Now it feels like the field is all about tweaking prompts and orchestrating APIs for pre-trained LLMs. I miss the thrill of crafting something unique. Is there still room for people who enjoy traditional ML, or is this just the inevitable evolution of the field? How do you stay inspired amidst such shifts?

Update: Wow, this blew up. Thanks everyone for your comments and suggestions. I really like some of those. This thing was on my mind for a long time, glad that I put it here. Thanks again!",MachineLearning,836,217,1733431797.0,1h7jg87,Educational_News_371,https://www.reddit.com/r/MachineLearning/comments/1h7jg87/dstuck_in_ai_hell_what_to_do_in_post_llm_world/,Discussion
[D] The winner of the NeurIPS 2024 Best Paper Award  sabotaged the other teams,"Presumably, the winner of the NeurIPS 2024 Best Paper Award (a guy from ByteDance, the creators of Tiktok) sabotaged the other teams to derail their research and redirect their resources to his own. Plus he was at meetings debugging his colleagues' code, so he was always one step ahead. There's a call to withdraw his paper.

[https://var-integrity-report.github.io/](https://var-integrity-report.github.io/)

I have not checked the facts themselves, so if you can verify what is asserted and if this is true this would be nice to confirm.",MachineLearning,703,104,1734032501.0,1hctf36,LelouchZer12,https://www.reddit.com/r/MachineLearning/comments/1hctf36/d_the_winner_of_the_neurips_2024_best_paper_award/,Discussion
"[D] Can we please stop using ""is all we need"" in titles?","As the title suggests. We need to stop or decrease the usage of ""... is all we need"" in paper titles. It's slowly getting a bit ridiculous. There is most of the time no actual scientific value in it. It has become a bad practice of attention grabbing for attentions' sake.",MachineLearning,694,103,1735040239.0,1hlbtrs,H4RZ3RK4S3,https://www.reddit.com/r/MachineLearning/comments/1hlbtrs/d_can_we_please_stop_using_is_all_we_need_in/,Discussion
[D] What happened at NeurIPS?,,MachineLearning,628,587,1734159607.0,1hdxbru,howtorewriteaname,https://i.redd.it/k0q9frsuir6e1.jpeg,Discussion
[P] I made wut – a CLI that explains your last command using a LLM,,MachineLearning,550,31,1734280140.0,1hew6wy,jsonathan,https://i.redd.it/nwo0a660h17e1.gif,Project
[N] AI engineers report burnout and rushed rollouts as ‘rat race’ to stay competitive hits tech industry,"[AI engineers report burnout and rushed rollouts as ‘rat race’ to stay competitive hits tech industry](https://www.cnbc.com/2024/05/03/ai-engineers-face-burnout-as-rat-race-to-stay-competitive-hits-tech.html)

Summary from article: 

- *Artificial intelligence engineers at top tech companies told CNBC that the pressure to roll out AI tools at breakneck speed has come to define their jobs.*

- *They say that much of their work is assigned to appease investors rather than to solve problems for end users, and that they are often chasing OpenAI.*

- *Burnout is an increasingly common theme as AI workers say their employers are pursuing projects without regard for the technology’s effect on climate change, surveillance and other potential real-world harms.*

An especially poignant quote from the article:

> An AI engineer who works at a retail surveillance startup told CNBC that he’s the only AI engineer at a company of 40 people and that he handles any responsibility related to AI, which is an overwhelming task. He said the company’s investors have inaccurate views on the capabilities of AI, often asking him to build certain things that are “impossible for me to deliver.”",MachineLearning,436,91,1714755503.0,1cjdmr5,bregav,https://www.reddit.com/r/MachineLearning/comments/1cjdmr5/n_ai_engineers_report_burnout_and_rushed_rollouts/,News
[R] Must-Read ML Theory Papers,"Hello,

I’m a CS PhD student, and I’m looking to deepen my understanding of machine learning theory. My research area focuses on vision-language models, but I’d like to expand my knowledge by reading foundational or groundbreaking ML theory papers.

Could you please share a list of must-read papers or personal recommendations that have had a significant impact on ML theory?

Thank you in advance!
",MachineLearning,434,102,1731773968.0,1gsqqns,AntelopeWilling2928,https://www.reddit.com/r/MachineLearning/comments/1gsqqns/r_mustread_ml_theory_papers/,Research
[P] Analysis of why UMAP is so fast ,"Hi, I recently spent some time to understand the core implementation of the UMAP algorithm from the point of view how it was implemented and why it's so fast (even though it's in python). I decided to decompose the algorithm into smaller steps in which I add some minor improvements to the code (one by one), so that at the end the final results are very similar to what I can get from the UMAP. 

To my surprise, most of these changes were just tricks in the optimization code to run things faster or update less important things less often. Of course, my implementation does not reproduce the UMAP algorithm in 100% as it was done in the educational purposes.

I provided a detailed explanation in my project of what I had to add in each step to move towards UMAP like algorithm. Here is the project page: [https://github.com/kmkolasinski/nano-umap](https://github.com/kmkolasinski/nano-umap)

If you are a person like, who likes to optimize the code for performance you may find this interesting. Here is a demo what I was able to get: 

https://preview.redd.it/eww57c3x881e1.png?width=1921&format=png&auto=webp&s=ed4a345e40b47782ddf39cb93eb9d03207db1160

**TLDR: in UMAP they:**

* use ANN library to quickly find top k-NN,
* use good initialization method which makes things more stable and algorithm requires less updates (UMAP uses fast spectral initialization),
* use random negative sampling, which is a naive approach but works very well in practice,
* squeeze the numba performance (by replacing [np.dot](http://np.dot) or np.clip with custom implementations to make code run much faster),
* use some sort of adaptive sampling which will make that the algorithm will spend more time on more important vectors saving your CPU time on less important ones

",MachineLearning,419,42,1731747730.0,1gsjfq9,kmkolasinski,https://www.reddit.com/r/MachineLearning/comments/1gsjfq9/p_analysis_of_why_umap_is_so_fast/,Project
[N] The 2024 Nobel Prize in Chemistry goes to the people Google Deepmind's AlphaFold. One half to David Baker and the other half jointly to Demis Hassabis and John M. Jumper.,Announcement: https://twitter.com/NobelPrize/status/1843951197960777760,MachineLearning,417,112,1728468562.0,1fznxyr,aagg6,https://www.reddit.com/r/MachineLearning/comments/1fznxyr/n_the_2024_nobel_prize_in_chemistry_goes_to_the/,News
[D] I feel like ever since LLM APIs have become a thing the quality of discussion regarding ML and ML products has gone down drastically.,"Been working as a MLE for the past few years after finishing my master's and am currently working at a company with really smart colleagues. The problem is, my company doesn't have the resources to train our own LLM and therefore has to resort to using various APIs for models.

Discussion regarding how to improve our products often feels unproductive and pointless. It usually resorts to ""how can we make this LLM (that we don't even have control over) do this thing by prompt engineering?""

I personally don't even think ""prompt engineering"" is a reliable or real thing, and feel like because most discussions devolve to that it feels like we're not able to really enhance our products either.

Just wondering if anyone else feels similarly.",MachineLearning,418,70,1726812415.0,1fl5be0,Seankala,https://www.reddit.com/r/MachineLearning/comments/1fl5be0/d_i_feel_like_ever_since_llm_apis_have_become_a/,Discussion
[D] How did OpenAI go from doing exciting research to a big-tech-like company?,"I was recently revisiting OpenAI’s paper on [DOTA2 Open Five](https://cdn.openai.com/dota-2.pdf), and it’s so impressive what they did there from both engineering and research standpoint. Creating a distributed system of 50k CPUs for the rollout, 1k GPUs for training while taking between 8k and 80k actions from 16k observations per 0.25s—how crazy is that?? They also were doing “surgeries” on the RL model to recover weights as their reward function, observation space, and even architecture has changed over the couple months of training. Last but not least, they beat the OG team (world champions at the time) and deployed the agent to play live with other players online. 

Fast forward a couple of years, they are predicting the next token in a sequence. Don’t get me wrong, the capabilities of gpt4 and its omni version are truly amazing feat of engineering and research (probably much more useful), but they don’t seem to be as interesting (from the research perspective) as some of their previous work.

So, now I am wondering how did the engineers and researchers transition throughout the years? Was it mostly due to their financial situation and need to become profitable or is there a deeper reason for their transition?
",MachineLearning,409,139,1716137172.0,1cvslyc,UnluckyNeck3925,https://www.reddit.com/r/MachineLearning/comments/1cvslyc/d_how_did_openai_go_from_doing_exciting_research/,Discussion
[R] KAN: Kolmogorov-Arnold Networks,"**Paper**: [https://arxiv.org/abs/2404.19756](https://arxiv.org/abs/2404.19756)

**Code**: [https://github.com/KindXiaoming/pykan](https://github.com/KindXiaoming/pykan)

**Quick intro**: [https://kindxiaoming.github.io/pykan/intro.html](https://kindxiaoming.github.io/pykan/intro.html)

**Documentation**: [https://kindxiaoming.github.io/pykan/](https://kindxiaoming.github.io/pykan/)

**Abstract**:

>Inspired by the Kolmogorov-Arnold representation theorem, we propose **Kolmogorov-Arnold Networks** (**KANs**) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have *fixed* activation functions on *nodes* (""neurons""), KANs have *learnable* activation functions on *edges* (""weights""). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs.

https://preview.redd.it/r7vjmp31juxc1.png?width=2326&format=png&auto=webp&s=a2c722cf733510194659b9aaec24269a7f9e5d47",MachineLearning,378,77,1714583003.0,1chrafb,None,https://www.reddit.com/r/MachineLearning/comments/1chrafb/r_kan_kolmogorovarnold_networks/,Research
You need everything other than ML to win a ML hackathon [D],"Basically a rant on condition of offline hackathons hosted my big MNCs and institues. 

Tired of participating in hackathons aimed to ""develope cutting edge solution"" and end up losing to a guy who have never studied machine learning but expert in ""bussiness informatics"" and really good while pitching the solution within given time limit. 

How can a sane mind who worked on idea, a prototype and a model for 2-3 days non-stop only gets to talk about it just for 3-5 minutes? I've literally seen people cloning github repos somewhat related to the problem statement and sell it like a some kind of state of the art product. I agree that this skills is more important in industry but then why name those hackathons as ""Machine Learning"" or ""AI"" hackathons? Better name it ""sell me some trash"".

Only option for someone really into developing a good product, a working model within limited time constraints and someone who loves competing (like me) is to participate online or in ""data"" competition. ",MachineLearning,362,69,1714341416.0,1cficmp,ade17_in,https://www.reddit.com/r/MachineLearning/comments/1cficmp/you_need_everything_other_than_ml_to_win_a_ml/,Discussion
[P] Drowning in Research Papers? 🐸,"We’re two engineers interested in AI research, but have been drowning in the flood of new papers on arXiv. So, we built Ribbit Ribbit, a research paper discovery tool.

* [https://apps.apple.com/us/app/ribbit-ribbit/id6529547956](https://apps.apple.com/us/app/ribbit-ribbit/id6529547956)
* [https://ribbitribbit.co](https://ribbitribbit.co)

It curates personalized paper recommendations and turns them into tweet-sized summaries, so you can scroll through like it’s Twitter. You can also listen to the updates just like a podcast made just for you. We’ve added a lighthearted touch, hoping it adds a bit of joy to the whole paper-reading process, which, let’s be real, can get pretty dry and dull :p.

https://preview.redd.it/evoemobinlud1.png?width=1179&format=png&auto=webp&s=4dff5b2b60f2a1272b6ac04347f661ceacff2aa5

",MachineLearning,356,117,1728858266.0,1g31hfd,haoyuan8,https://www.reddit.com/r/MachineLearning/comments/1g31hfd/p_drowning_in_research_papers/,Project
[N] Jurgen Schmidhuber on 2024 Physics Nobel Prize,"The NobelPrizeinPhysics2024 for Hopfield & Hinton rewards plagiarism and incorrect attribution in computer science. It's mostly about Amari's ""Hopfield network"" and the ""Boltzmann Machine.""

 

1. The Lenz-Ising recurrent architecture with neuron-like elements was published in 1925 . In 1972, Shun-Ichi Amari made it adaptive such that it could learn to associate input patterns with output patterns by changing its connection weights. However, Amari is only briefly cited in the ""Scientific Background to the Nobel Prize in Physics 2024."" Unfortunately, Amari's net was later called the ""Hopfield network."" Hopfield republished it 10 years later, without citing Amari, not even in later papers.

2. The related Boltzmann Machine paper by Ackley, Hinton, and Sejnowski (1985) was about learning internal representations in hidden units of neural networks (NNs) [S20]. It didn't cite the first working algorithm for deep learning of internal representations by Ivakhnenko & Lapa. It didn't cite Amari's separate work (1967-68) on learning internal representations in deep NNs end-to-end through stochastic gradient descent (SGD). Not even the later surveys by the authors nor the ""Scientific Background to the Nobel Prize in Physics 2024"" mention these origins of deep learning. ([BM] also did not cite relevant prior work by Sherrington & Kirkpatrick & Glauber)

3. The Nobel Committee also lauds Hinton et al.'s 2006 method for layer-wise pretraining of deep NNs (2006). However, this work neither cited the original layer-wise training of deep NNs by Ivakhnenko & Lapa, nor the original work on unsupervised pretraining of deep NNs (1991).

4. The ""Popular information"" says: “At the end of the 1960s, some discouraging theoretical results caused many researchers to suspect that these neural networks would never be of any real use."" However, deep learning research was obviously alive and kicking in the 1960s-70s, especially outside of the Anglosphere.

5. Many additional cases of plagiarism and incorrect attribution can be found in the following reference [DLP], which also contains the other references above. One can start with Sec. 3:  J. Schmidhuber (2023). How 3 Turing awardees republished key methods and ideas whose creators they failed to credit. Technical Report IDSIA-23-23, Swiss AI Lab IDSIA, 14 Dec 2023. https://people.idsia.ch/~juergen/ai-priority-disputes.html… See also the following reference [DLH] for a history of the field: [DLH] J. Schmidhuber (2022). Annotated History of Modern AI and Deep Learning. Technical Report IDSIA-22-22, IDSIA, Lugano, Switzerland, 2022. Preprint arXiv:2212.11279. https://people.idsia.ch/~juergen/deep-learning-history.html… (This extends the 2015 award-winning survey https://people.idsia.ch/~juergen/deep-learning-overview.html…)


Twitter post link: https://x.com/schmidhuberai/status/1844022724328394780?s=46&t=Eqe0JRFwCu11ghm5ZqO9xQ",MachineLearning,353,146,1728492762.0,1fzw5b1,optimization_ml,https://www.reddit.com/r/MachineLearning/comments/1fzw5b1/n_jurgen_schmidhuber_on_2024_physics_nobel_prize/,News
"[D] AI Agents: too early, too expensive, too unreliable","[**Reference: Full blog post**](https://www.kadoa.com/blog/ai-agents-hype-vs-reality)

There has been a lot of hype about the promise of autonomous agent-based LLM workflows. By now, all major LLMs are capable of interacting with external tools and functions, letting the LLM perform sequences of tasks automatically.

But reality is proving more challenging than anticipated.

The [WebArena leaderboard](https://docs.google.com/spreadsheets/d/1M801lEpBbKSNwP-vDBkC_pF7LdyGU1f_ufZb_NWNBZQ/edit#gid=0), which benchmarks LLMs agents against real-world tasks, shows that even the best-performing models have a success rate of only 35.8%.

# Challenges in Practice

After seeing many attempts to AI agents, I believe it's too early, too expensive, too slow, too unreliable.  
It feels like many AI agent startups are waiting for a model breakthrough that will start the race to productize agents.

* Reliability: As we all know, LLMs are prone to hallucinations and inconsistencies. Chaining multiple AI steps compounds these issues, especially for tasks requiring exact outputs.
* Performance and costs: GPT-4o, Gemini-1.5, and Claude Opus are working quite well with tool usage/function calling, but they are still slow and expensive, particularly if you need to do loops and automatic retries.
* Legal concerns: Companies may be held liable for the mistakes of their agents. A [recent example](https://www.theguardian.com/world/2024/feb/16/air-canada-chatbot-lawsuit) is Air Canada being ordered to pay a customer who was misled by the airline's chatbot.
* User trust: The ""black box"" nature of AI agents and stories like the above makes it hard for users to understand and trust their outputs. Gaining user trust for sensitive tasks involving payments or personal information will be hard (paying bills, shopping, etc.).

# Real-World Attempts

Several startups are tackling the AI agent space, but most are still experimental or invite-only:

* [adept.ai](https://www.adept.ai/) - $350M funding, but access is still very limited
* [MultiOn](https://www.multion.ai) - funding unknown, their API-first approach seems promising
* [HypeWrite](https://www.hyperwriteai.com/personal-assistant) - $2.8M funding, started with an AI writing assistant and expanded into the agent space
* [minion.ai](https://minion.ai) - created some initial buzz but has gone quiet now, waitlist only

Only MultiOn seems to be pursuing the ""give it instructions and watch it go"" approach, which is more in line with the promise of AI agents.  
All others are going down the record-and-replay RPA route, which may be necessary for reliability at this stage.

Large players are also bringing AI capabilities to desktops and browsers, and it looks like we'll get native AI integrations on a system level:

* OpenAI announced their Mac desktop app that can interact  with the OS screen.
* At Google I/O, Google demonstrated Gemini [automatically processing a shopping return](https://www.youtube.com/watch?v=zRY_T-hBp74).
* Microsoft [announced Copilot Studio](https://www.microsoft.com/en-us/microsoft-copilot/microsoft-copilot-studio), which will let developers build AI agent bots.

Screenshot Screenshot

These tech demos are impressive, but we'll see how well these agent capabilities will work when released publicly and tested against real-world scenarios instead of hand-picked demo cases.

# The Path Forward

AI agents overhyped and it's too early.  
However, the underlying models continue to advance quickly, and we can expect to see more successful real-world applications.  
Instead of trying to have one large general purpose agent that is hard to control and test, we can use many smaller agents that basically just pick the right strategy for a specific sub-task in our workflows. These ""agents"" can be thought of as medium-sized LLM prompts with a) context and b) a set of functions available to call.

The most promising path forward likely looks like this:

1. Narrowly scoped, well testable automations that use AI as an augmentation tool rather than pursuing full autonomy
2. Human-in-the-loop approaches that keep humans involved for oversight and handling edge cases
3. Setting realistic expectations about current capabilities and limitations

By combining tightly constrained agents, good evaluation data, human-in-the-loop oversight, and traditional engineering methods, we can achieve reliably good results for automating medium-complex tasks.

Will AI agents automate tedious repetitive work, such as web scraping, form filling, and data entry? Yes, absolutely.

Will AI agents autonomously book your vacation without your intervention? Unlikely, at least in the near future.",MachineLearning,336,89,1716388040.0,1cy1kn9,madredditscientist,https://www.reddit.com/r/MachineLearning/comments/1cy1kn9/d_ai_agents_too_early_too_expensive_too_unreliable/,Discussion
"[N] Sama, an AI sweatshop, pays workers in Kenya $2 an hour to filter and label porn, beastiality, suicide, child abuse, for hours on end!!",,MachineLearning,329,119,1733557088.0,1h8nhbh,BotherBubbly5096,https://youtu.be/qZS50KXjAX0,News
[D] Transformers are a type of CNN,"https://arxiv.org/abs/2309.10713

I was randomly googling Dynamic Convolutions since I thought they were cool and found this paper that shows transformers are equivalent to a type of CNN that uses dynamic convolutions. The dynamic convolution paper (https://arxiv.org/abs/1912.03458) was released in 2019 so it did come after the attention is all you need paper.

Sadly this paper has only one citation. I think it's incredible. Knowing that transformers can be viewed as a CNN gives them insight into optimising its design, including removing the softmax activation and replacing it with a Relu+normalisation layer. I think there's a ton more improvements that can be made by continuing their work.",MachineLearning,330,65,1729758666.0,1gaxscv,Ozqo,https://www.reddit.com/r/MachineLearning/comments/1gaxscv/d_transformers_are_a_type_of_cnn/,Discussion
[D] Kolmogorov-Arnold Network is just an MLP,"It turns out, that you can write Kolmogorov-Arnold Network as an MLP, with some repeats and shift before ReLU.

[https://colab.research.google.com/drive/1v3AHz5J3gk-vu4biESubJdOsUheycJNz](https://colab.research.google.com/drive/1v3AHz5J3gk-vu4biESubJdOsUheycJNz)",MachineLearning,321,99,1714979066.0,1clcu5i,osamc,https://www.reddit.com/r/MachineLearning/comments/1clcu5i/d_kolmogorovarnold_network_is_just_an_mlp/,Discussion
[P] I made Termite – a CLI that can generate terminal UIs from simple text prompts,,MachineLearning,317,32,1735488160.0,1hoyzao,jsonathan,https://i.redd.it/0gpln74u8t9e1.gif,Project
"[D] LLMs aren't interesting, anyone else?","I'm not an ML researcher. When I think of cool ML research what comes to mind is stuff like [OpenAI Five](https://openai.com/index/openai-five-defeats-dota-2-world-champions/), or AlphaFold. Nowadays the buzz is around LLMs and scaling transformers, and while there's absolutely some research and optimization to be done in that area, it's just not as interesting to me as the other fields. For me, the interesting part of ML is training models end-to-end for your use case, but SOTA LLMs these days can be steered to handle a lot of use cases. Good data + lots of compute = decent model. That's it? 

  
I'd probably be a lot more interested if I could train these models with a fraction of the compute, but doing this is unreasonable. Those without compute are limited to fine-tuning or prompt engineering, and the SWE in me just finds this boring. Is most of the field really putting their efforts into next-token predictors?

  
Obviously LLMs are disruptive, and have already changed a lot, but from a research perspective, they just aren't interesting to me. Anyone else feel this way? For those who were attracted to the field because of non-LLM related stuff, how do you feel about it? Do you wish that LLM hype would die down so focus could shift towards other research? Those who do research outside of the current trend: how do you deal with all of the noise?",MachineLearning,311,158,1722476455.0,1eh4llh,leetcodeoverlord,https://www.reddit.com/r/MachineLearning/comments/1eh4llh/d_llms_arent_interesting_anyone_else/,Discussion
[P] Illustrated book to learn about Transformers & LLMs,"I have seen several instances of folks on this subreddit being interested in long-form explanations of the inner workings of Transformers & LLMs.

This is a gap my twin brother and I have been aiming at filling for the past 3 1/2 years. Last week, we published “[Super Study Guide: Transformers & Large Language Models](https://superstudy.guide/transformers-large-language-models/)”, a 250-page book with more than 600 illustrations aimed at visual learners who have a strong interest in getting into the field.

This book covers the following topics in depth:

* **Foundations**: primer on neural networks and important deep learning concepts for training and evaluation.
* **Embeddings**: tokenization algorithms, word embeddings (word2vec) and sentence embeddings (RNN, LSTM, GRU).
* **Transformers**: motivation behind its self-attention mechanism, detailed overview on the encoder-decoder architecture and related variations such as BERT, GPT and T5, along with tips and tricks on how to speed up computations.
* **Large language models**: main techniques to tune Transformer-based models, such as prompt engineering, (parameter efficient) finetuning and preference tuning.
* **Applications**: most common problems including sentiment extraction, machine translation, retrieval-augmented generation and many more.

(In case you are wondering: this content follows the same vibe as the Stanford illustrated study guides we had shared on this subreddit 5-6 years ago about [CS 229: Machine Learning](https://www.reddit.com/r/MachineLearning/comments/98wrkw/illustrated_machine_learning_cheatsheets_covering/), [CS 230: Deep Learning](https://www.reddit.com/r/MachineLearning/comments/a0xfc2/p_illustrated_deep_learning_cheatsheets_covering/) and [CS 221: Artificial Intelligence](https://www.reddit.com/r/MachineLearning/comments/bse25u/p_illustrated_artificial_intelligence_cheatsheets/))

Happy learning!

https://preview.redd.it/n6zraaltemjd1.jpg?width=1905&format=pjpg&auto=webp&s=1110f750df0d8a60d5fdf1d4967b41e1b5617efe",MachineLearning,310,110,1724073292.0,1ew1hws,shervinea,https://www.reddit.com/r/MachineLearning/comments/1ew1hws/p_illustrated_book_to_learn_about_transformers/,Project
[D] PyTorch 2.5.0 released!,"https://github.com/pytorch/pytorch/releases/tag/v2.5.0

Highlights: We are excited to announce the release of PyTorch® 2.5! This release features a new CuDNN backend for SDPA, enabling speedups by default for users of SDPA on H100s or newer GPUs. As well, regional compilation of torch.compile offers a way to reduce the cold start up time for torch.compile by allowing users to compile a repeated nn.Module (e.g. a transformer layer in LLM) without recompilations. Finally, TorchInductor CPP backend offers solid performance speedup with numerous enhancements like FP16 support, CPP wrapper, AOT-Inductor mode, and max-autotune mode.
This release is composed of 4095 commits from 504 contributors since PyTorch 2.4. We want to sincerely thank our dedicated community for your contributions.

Some of my favorite improvements:

- Faster torch.compile compilation by re-using repeated modules

- torch.compile support for torch.istft

- FlexAttention: A flexible API that enables implementing various attention mechanisms such as Sliding Window, Causal Mask, and PrefixLM with just a few lines of idiomatic PyTorch code. This API leverages torch.compile to generate a fused FlashAttention kernel, which eliminates extra memory allocation and achieves performance comparable to handwritten implementations. Additionally, we automatically generate the backwards pass using PyTorch's autograd machinery. Furthermore, our API can take advantage of sparsity in the attention mask, resulting in significant improvements over standard attention implementations.",MachineLearning,308,25,1729203096.0,1g62vyh,parlancex,https://www.reddit.com/r/MachineLearning/comments/1g62vyh/d_pytorch_250_released/,Discussion
[P] Just-in-Time Implementation: A Python Library That Implements Your Code at Runtime,"Hey r/MachineLearning !

You know how we have Just-in-Time Compilation? Well, I thought, ""Why stop there?"" So I created Just-in-Time Implementation - a Python library that writes your code for you using AI. Yes, really!

Here's a taste of what it can do:

    from jit_implementation import implement
    
    @implement
    class Snake:
        """"""Snake game in pygame. Initializing launches the game.""""""
    
    if __name__ == ""__main__"":
        Snake()
    
    # Believe it or not, this actually works!

I started this as a joke, but then I got carried away and made it actually work. Now I'm not sure if I should be proud or terrified.

# How it works:

1. You write a function or class signature and a docstring.
2. You slap the `@implement` decorator on it.
3. The implementation is generated on-demand when you call the function or instantiate the class. Lazy coding at its finest!

# Some ""features"" I'm particularly amused by:

* It's the ultimate lazy programming tool. The code doesn't even exist until you run it!
* You can define tests in the decorator, and the AI will keep trying until it passes them. It's like having an intern that never sleeps!
* With sampling temperature set to 0, it's more reproducible than Docker images.
* Smart enough to skim your code for context, not dumb enough to read it all.

# Should you use this in production?

Only if you want to give your senior devs a heart attack. But hey, I'm not here to judge.

# Want to check it out?

Here's the GitHub repo: [JIT Implementation](https://github.com/JirkaKlimes/jit-implementation)

Feel free to star, fork, or just point and laugh. All reactions are valid!

I'd love to hear what you think. Is this the future of programming or a sign that I need to take a long vacation? Maybe both?

P.S. If any of you actually use this for something, please let me know. I'm really interested in how complex a codebase (or lack thereof) could be made using this.

# Important Notes

I made this entire thing in just under 4 hours, so please keep your expectations in check! (it's in beta)",MachineLearning,301,49,1727883878.0,1fujbuz,JirkaKlimes,https://www.reddit.com/r/MachineLearning/comments/1fujbuz/p_justintime_implementation_a_python_library_that/,Project
[D] Is it common for ML researchers to tweak code until it works and then fit the narrative (and math) around it? ,"As an aspiring ML researcher, I am interested in the opinion of fellow colleagues. And if and when true, does it make your work less fulfilling?",MachineLearning,291,117,1728969745.0,1g40i0h,Diligent-Ad8665,https://www.reddit.com/r/MachineLearning/comments/1g40i0h/d_is_it_common_for_ml_researchers_to_tweak_code/,Discussion
[P]: TensorHue – a tensor visualization library (info in comments),,MachineLearning,286,31,1725805753.0,1fbz318,epistoteles,https://www.reddit.com/gallery/1fbz318,Project
"[P] ChessGPT, 100,000x smaller than GPT-4, plays chess at 1500 Elo. By finding a skill vector, we can increase its win rate by 2.6x in out-of-distribution games.","A previous project trained ChessGPT, a set of 25M and 50M parameter GPT models that can play chess at 1500 Elo. These models are \~100,000x smaller than [GPT-4's 1.8T parameters](https://developer.nvidia.com/blog/demystifying-ai-inference-deployments-for-trillion-parameter-large-language-models/).

At Stockfish level 0, the 50M parameter model has a win rate of 70%. However, if the game is initialized with 20 random moves, its win rate drops to 17%. Is this because it can't generalize out of distribution? When considering the task of next-token prediction, a good next token predictor would predict legal but low skill moves if the game begins with random moves.

This is what we find with ChessGPT. By adding a skill vector to the model's activations, we can increase its win rate to 43%, or by 2.6x. We don't fully recover the performance gap, but it is a significant fraction. The intervention is very simple, and it's possible that a more sophisticated intervention could further increase its win rate.

This model is only trained to predict the next character in PGN strings (1.e4 e5 2.Nf3 …) and is never explicitly given the state of the board or the rules of chess. Despite this, in order to better predict the next character, it learns to compute the state of the board at any point of the game, and learns a diverse set of rules, including check, checkmate, castling, en passant, promotion, pinned pieces, etc. In addition, to better predict the next character it also learns to estimate latent variables such as the Elo rating of the players in the game.

We can also use interpretability methods to intervene on the model's internal board state.

This work was recently accepted to the 2024 Conference on Language Modeling (COLM) under the title ""[Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models](https://arxiv.org/abs/2403.15498)"".

More information is available in this post:

[https://adamkarvonen.github.io/machine\_learning/2024/03/20/chess-gpt-interventions.html](https://adamkarvonen.github.io/machine_learning/2024/03/20/chess-gpt-interventions.html)

And the code is here: [https://github.com/adamkarvonen/chess\_llm\_interpretability](https://github.com/adamkarvonen/chess_llm_interpretability)",MachineLearning,286,67,1721591949.0,1e8v2za,seraine,https://www.reddit.com/r/MachineLearning/comments/1e8v2za/p_chessgpt_100000x_smaller_than_gpt4_plays_chess/,Project
[P] I made a library for building agents that use tree search to solve problems,,MachineLearning,285,26,1732456278.0,1gyreq1,jsonathan,https://i.redd.it/qut9unu4su2e1.png,Project
"[D] Accepted NeurIPS 2024 paper claimed to be solving a novel problem as first work, but ignores 5 prior works","At NeurIPS 2024 I found a paper that got accepted that positions its main contribution in the form of “Existing algorithms for X ignore Y. We adapt algorithm Z for X to account for Y”.

On OpenReview I see that the reviewers in particular praised the novelty of the work, and recognised Y as an important aspect that had been ignored in the field of X.

Now the interesting bit: co-authors and I published a paper in Springer’s Machine Learning journal in 2023 that also proposes an algorithm for X that account for Y. We were also not the first to study the problem setting of X with Y: our paper’s related work section discusses 4 papers that have all proposed algorithms for X that account for Y. One is even from NeurIPS (2017), and the oldest one dates back to 2012 (an AAAI paper).

The authors of this 2024 NeurIPS paper completely missed all this prior literature and believed they were the first, and so did all the reviewers.

This week I e-mailed the authors of this NeurIPS 2024 paper and they acknowledged that these works (mine + the 4 others) indeed were all working on the same problem setting, mentioned that they were unaware of all these works, and acknowledged that they can no longer claim novelty of the problem setting.

NeurIPS allows updating the camera ready paper after the conference, and the authors promised to use this opportunity to incorporate those related works and modify their contribution statements to no longer claim novelty of a first solution of X with Y.

At the one hand, it makes me happy that our work will get credited appropriately.

At the other hand I have my doubts about the ethics of severely modifying contribution statements post-review. The authors will no longer claim novelty, but the reviewers in particular praised this novelty, which makes me uncertain whether reviewers would have recommended acceptance had they known that this paper will ultimately no longer be able to claim the novelty that it claimed to have in the reviewed version.

Moreover this makes me wonder about the experimental section. Almost surely, reviewers would have demanded comparison to those 5 prior works as baselines. This paper did not compare against baselines, which will have seemed reasonable to a reviewer who reviewed this work under the assumption that the problem setting was completely novel and no prior methods exist that could function as a baseline.

Asking the group here about any thoughts on how such cases should get resolved:
- should the paper be retracted?
- should the area chair / program committee be informed? who may or may not take action
- should the paper just get updated by authors in the way that was promised, and that is it?
- something else?

I redacted X, Y and Z in order to not publicly shame the authors, as they have engaged with my e-mails and I am convinced that there is no foul play and they truly were unaware of those works.",MachineLearning,278,62,1732327138.0,1gxooqv,TaXxER,https://www.reddit.com/r/MachineLearning/comments/1gxooqv/d_accepted_neurips_2024_paper_claimed_to_be/,Discussion
[D] OpenAI o3 87.5% High Score on ARC Prize Challenge,"https://arcprize.org/blog/oai-o3-pub-breakthrough

>OpenAI's new o3 system - trained on the ARC-AGI-1 Public Training set - has scored a breakthrough 75.7% on the Semi-Private Evaluation set at our stated public leaderboard $10k compute limit. A high-compute (172x) o3 configuration scored 87.5%.",MachineLearning,279,195,1734718847.0,1hiq3tz,currentscurrents,https://www.reddit.com/r/MachineLearning/comments/1hiq3tz/d_openai_o3_875_high_score_on_arc_prize_challenge/,Discussion
[D] Real talk about RAG,"Let’s be honest here. I know we all have to deal with these managers/directors/CXOs that come up with amazing idea to talk with the company data and documents.

But… has anyone actually done something truly useful? If so, how was its usefulness measured?

I have a feeling that we are being fooled by some very elaborate bs as the LLM can always generate something that sounds sensible in a way. But is it useful?",MachineLearning,268,143,1714240847.0,1cekoc7,None,https://www.reddit.com/r/MachineLearning/comments/1cekoc7/d_real_talk_about_rag/,Discussion
[D] What are your horror stories from being tasked impossible ML problems,"ML is very good at solving a niche set of problems, but most of the technical nuances are lost on tech bros and managers. What are some problems you have been told to solve which would be impossible (no data, useless data, unrealistic expectations) or a misapplication of ML (can you have this LLM do all of out accounting). ",MachineLearning,265,171,1714070726.0,1ccz2cq,LanchestersLaw,https://www.reddit.com/r/MachineLearning/comments/1ccz2cq/d_what_are_your_horror_stories_from_being_tasked/,Discussion
[P] I reproduced Anthropic's recent interpretability research ,"Not that many people are paying attention to LLM interpretability research when capabilities research is moving as fast as it currently is, but interpretability is really important and in my opinion, really interesting and exciting! Anthropic has made a lot of breakthroughs in recent months, the biggest one being ""Towards Monosemanticity"". The basic idea is that they found a way to train a sparse autoencoder to generate interpretable features based on transformer activations. This allows us to look at the activations of a language model during inference, and understand which parts of the model are most responsible for predicting each next token. Something that really stood out to me was that the autoencoders they train to do this are actually very small, and would not require a lot of compute to get working. This gave me the idea to try to replicate the research by training models on my M3 Macbook. After a lot of reading and experimentation, I was able to get pretty strong results! I wrote a more in-depth post about it on my blog here:

[https://jakeward.substack.com/p/monosemanticity-at-home-my-attempt](https://jakeward.substack.com/p/monosemanticity-at-home-my-attempt)

I'm now working on a few follow-up projects using this tech, as well as a minimal implementation that can run in a Colab notebook to make it more accessible. If you read my blog, I'd love to hear any feedback!",MachineLearning,264,34,1714585864.0,1chsg42,neverboosh,https://www.reddit.com/r/MachineLearning/comments/1chsg42/p_i_reproduced_anthropics_recent_interpretability/,Project
[D] What’s the most surprising or counterintuitive insight you’ve learned about machine learning recently?,ML often challenges assumptions. What’s something you learned that flipped your understanding or made you rethink a concept?,MachineLearning,263,85,1731973896.0,1gujfj2,BrechtCorbeel_,https://www.reddit.com/r/MachineLearning/comments/1gujfj2/d_whats_the_most_surprising_or_counterintuitive/,Discussion
[N] Ilya Sutskever and friends launch Safe Superintelligence Inc.,"With offices in Palo Alto and Tel Aviv, the company will be concerned with just building ASI. No product cycles.

https://ssi.inc",MachineLearning,263,205,1718825397.0,1djrs3n,we_are_mammals,https://www.reddit.com/r/MachineLearning/comments/1djrs3n/n_ilya_sutskever_and_friends_launch_safe/,News
[D] - Why MAMBA did not catch on? ,It felt like that MAMBA will replace transformer from all the hype. It was fast but still maintained performance of transformer. O(N) during training and O(1) during inference and gave pretty good accuracy. So why it didn't became dominant? Also what is state of state space models?,MachineLearning,252,92,1735537182.0,1hpg91o,TwoSunnySideUp,https://www.reddit.com/r/MachineLearning/comments/1hpg91o/d_why_mamba_did_not_catch_on/,Discussion
[D] What's the endgame for AI labs that are spending billions on training generative models?,"Given the current craze around LLMs and generative models, frontier AI labs are burning through billions of dollars of VC funding to build GPU clusters, train models, give free access to their models, and get access to licensed data. But what is their game plan for when the excitement dies off and the market readjusts?

There are a few challenges that make it difficult to create a profitable business model with current LLMs:

- The near-equal performance of all frontier models will commoditize the LLM market and force providers to compete over prices, slashing profit margins. Meanwhile, the training of new models remains extremely expensive.

- Quality training data is becoming increasingly expensive. You need subject matter experts to manually create data or review synthetic data. This in turn makes each iteration of model improvement even more expensive.

- Advances in open source and open weight models will probably take a huge part of the enterprise market of private models.

- Advances in on-device models and integration with OS might reduce demand for cloud-based models in the future.

- The fast update cycles of models gives AI companies a very short payback window to recoup the huge costs of training new models.

What will be the endgame for labs such as Anthropic, Cohere, Mistral, Stability, etc. when funding dries up? Will they become more entrenched with big tech companies (e.g., OpenAI and Microsoft) to scale distribution? Will they find other business models? Will they die or be acquired (e.g., Inflection AI)?

Thoughts?",MachineLearning,253,113,1719820930.0,1dsnk1k,bendee983,https://www.reddit.com/r/MachineLearning/comments/1dsnk1k/d_whats_the_endgame_for_ai_labs_that_are_spending/,Discussion
[R] Were RNNs All We Needed?,"https://arxiv.org/abs/2410.01201

The authors (including Y. Bengio) propose simplified versions of LSTM and GRU that allow parallel training, and show strong results on some benchmarks.",MachineLearning,248,56,1727984225.0,1fvg7qr,we_are_mammals,https://www.reddit.com/r/MachineLearning/comments/1fvg7qr/r_were_rnns_all_we_needed/,Research
[R] Training models with multiple losses,"Instead of using gradient descent to minimize a single loss, we propose to use *Jacobian descent* to minimize multiple losses simultaneously. Basically, this algorithm updates the parameters of the model by reducing the Jacobian of the (vector-valued) objective function into an update vector.

To make it accessible to everyone, we have developed *TorchJD*: a library extending autograd to support Jacobian descent. After a simple `pip install torchjd`, transforming a PyTorch-based training function is very easy. With the recent release v0.2.0, TorchJD finally supports multi-task learning!

Github: [https://github.com/TorchJD/torchjd](https://github.com/TorchJD/torchjd)  
Documentation: [https://torchjd.org](https://torchjd.org)  
Paper: [https://arxiv.org/pdf/2406.16232](https://arxiv.org/pdf/2406.16232)

We would love to hear some feedback from the community. If you want to support us, a star on the repo would be grealy appreciated! We're also open to discussion and criticism.",MachineLearning,243,82,1725795830.0,1fbvuhs,Skeylos2,https://www.reddit.com/r/MachineLearning/comments/1fbvuhs/r_training_models_with_multiple_losses/,Research
[N] Llama 3.1 405B launches,"https://llama.meta.com/

* Comparable to GPT-4o and Claude 3.5 Sonnet, according to the benchmarks
* The weights are publicly available
* 128K context",MachineLearning,244,82,1721748559.0,1eaaq05,we_are_mammals,https://www.reddit.com/r/MachineLearning/comments/1eaaq05/n_llama_31_405b_launches/,News
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time
","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!",MachineLearning,241,56,1715592805.0,1cqv5y4,CriticalofReviewer2,https://www.reddit.com/r/MachineLearning/comments/1cqv5y4/r_our_new_classification_algorithm_outperforms/,Research
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?",MachineLearning,239,73,1715831821.0,1ct45hk,mtmttuan,https://www.reddit.com/r/MachineLearning/comments/1ct45hk/d_whats_up_with_papers_without_code/,Discussion
[D] Theory behind modern diffusion models,"Hi everyone,

I recently attended some lectures at university regarding diffusion models. Those explained all the math behind the original DDPM (Denoiding Diffusion Probabilistic Model) in great detail (especially in the appendices), actually better than anything else I have found online. So it has been great for learning the basics behind diffusion models (slides are available in the link in the readme here if you are interesed: https://github.com/julioasotodv/ie-C4-466671-diffusion-models)

However, I am struggling to find resources with similar level of detail for modern approaches—such as flow matching/rectified flows, how the different ODE solvers for sampling work, etc. There are some, but everything that I have found is either quite outdated (like from 2023 or so) or very superficial—like for non-technical or scientific audiences.

Therefore, I am wondering: has anyone encountered a good compendium of theoretical eplanations beyond the basic diffusion model (besides the original papers)? The goal is to let my team deep dive into the actual papers should they desire, but giving 70% of what those deliver in one or more decent compilations.

I really believe that SEO is making any search a living nightmare nowadays. Either that or my googling skills are tanking for some reason.

Thank you all!",MachineLearning,234,27,1732800448.0,1h1vxe1,bgighjigftuik,https://www.reddit.com/r/MachineLearning/comments/1h1vxe1/d_theory_behind_modern_diffusion_models/,Discussion
Meta AI (FAIR) latest paper integrates system-1 and system-2 thinking into reasoning models. [R],"Meta AI (FAIR) latest paper integrates system-1 and system-2 thinking into reasoning models.

Basically, it introduces the term ""Dualformer"" which integrates both system-1 (fast-thinking) and system-2 (slow-thinking) into the transformer to improve its reasoning capability. The high level idea is to train the model with ""randomized trace"", which randomly drop parts of the reasoning tokens. This approach improves model's inference speed, accuracy, and diversity. It also enables model to perform system-1 and system-2 thinking in a controllable fashion. 

The paper's link here:

[https://arxiv.org/html/2410.09918v1](https://arxiv.org/html/2410.09918v1)",MachineLearning,234,54,1729636687.0,1g9v7ag,Proof-Raise-9151,https://www.reddit.com/r/MachineLearning/comments/1g9v7ag/meta_ai_fair_latest_paper_integrates_system1_and/,Research
"[D] Why do juniors (undergraduates or first- to second-year PhD students) have so many papers at major machine learning conferences like ICML, ICLR, NeurIPS, etc.?","Hello everyone, today the ICML results are out, congratulations to all those who have papers accepted here. I'm not an academic myself, but sometimes I read papers at these conferences for work, and it's really interesting. I just have a question: why do juniors have so many papers at these conferences? I thought this was something you would have to learn throughout your 5 years of PhD and almost only achieve in the final years of your PhD. Furthermore, I've heard that to get into top PhD programs in the US, you need to have some papers beforehand. So, if a junior can publish papers early like that, why do they have to spend 5 long years pursuing a PhD?",MachineLearning,234,66,1714651000.0,1cidsz7,ShiftStrange1701,https://www.reddit.com/r/MachineLearning/comments/1cidsz7/d_why_do_juniors_undergraduates_or_first_to/,Discussion
[D] How would you diagnose these spikes in the training loss? ,,MachineLearning,232,94,1714304669.0,1cf4gw9,NumberGenerator,https://i.redd.it/bk7wbahyh7xc1.png,Discussion
[R] Differential Transformer,"### [Paper](https://arxiv.org/abs/2410.05258)

### Abstract

> Transformer tends to overallocate attention to irrelevant context. 
> In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. 
> Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps.
> The subtraction cancels noise, promoting the emergence of sparse attention patterns. [...]
> [...] it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. [...]",MachineLearning,230,16,1728627971.0,1g13gkd,fliiiiiiip,https://www.reddit.com/gallery/1g13gkd,Research
[D] Is there an alternative to Science Twitter/X?,"Hey folks,

I have been wondering if there is an alternative to the science community on Twitter/X, especially in the DS/ML sphere. I really liked that community before and during COVID, but I left Twitter shortly after Elon took charge, as the platform was already quite toxic then and became much worse since. 

I'm aware that there is a community active on LinkedIn, which is okay at times, but mostly full of influencers who try to sound/look intelligent and people hyping up every little new thing about LLMs. I know that other people left the science community on Twitter since then and was hence wondering if an alternative has evolved over the last years.

P.s. I will post this message in the DS community as well.",MachineLearning,228,70,1730634102.0,1gikys5,H4RZ3RK4S3,https://www.reddit.com/r/MachineLearning/comments/1gikys5/d_is_there_an_alternative_to_science_twitterx/,Discussion
[D] How do researchers in hot topics keep up?,"Yesterday night I was reading ""Training Language Models to Self-Correct via Reinforcement Learning"" (https://arxiv.org/abs/2409.12917) from Deepmind folks, which was released 2 days ago. The paper is about using RL to pre-train LLMs, but that is somehow irrelevant for my question.

The paper is interesting, but while I was reading I wondered: how do they have time to do all that is mentioned there? With this I mean:

- Based on the pretrained models that are used, most likely they only started working on it like 2-3 months ago

- Most references and citations are from the second half of 2024 (from May-June onwards), so less than 3 months old as well

So, during the course of those few months, they had to: read and thoroughly study all cited papers (which are around 45 in this case, and again: most of them are extremely recent), come up with the new idea, develop it, do experiments (which nowadays SFT is not a matter or 15 mins either), compile results, and write the actual paper. And this assumes that they are not concurrently working on other papers/endeavors…

As a solo researcher, I cannot even imagine doing something similar in that period of time, but even with a small team I find it almost impossible. My day has only 24 hours but it feels like other people's in the research world can stop time to get more done.

Am I just inefficient or dumb? To fully understand a novel paper it can take me up to one/two almost full days (6 hours a day) to reproduce, derive all (or most of) the math and get a deeper understanding on why it does/does not work.

Any insights are much appreciated, thanks!",MachineLearning,222,49,1726910357.0,1flz1vo,bgighjigftuik,https://www.reddit.com/r/MachineLearning/comments/1flz1vo/d_how_do_researchers_in_hot_topics_keep_up/,Discussion
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?",MachineLearning,216,31,1715505438.0,1cq3uh4,Standard_Natural1014,https://www.reddit.com/r/MachineLearning/comments/1cq3uh4/d_impact_of_solar_storm_on_qlora_rlhf_of_llama3_8b/,Discussion
[D] i sensed anxiety and frustration at NeurIPS’24 (kyunghyuncho blog),,MachineLearning,218,66,1734834161.0,1hjp5gc,hardmaru,https://kyunghyuncho.me/i-sensed-anxiety-and-frustration-at-neurips24/,Discusssion
[D] What ML Concepts Do People Misunderstand the Most?,"I’ve noticed that certain ML concepts, like the bias-variance tradeoff or regularization, often get misunderstood. What’s one ML topic you think is frequently misinterpreted, and how do you explain it to others?",MachineLearning,209,191,1734812560.0,1hjiana,AdHappy16,https://www.reddit.com/r/MachineLearning/comments/1hjiana/d_what_ml_concepts_do_people_misunderstand_the/,Discussion
[D] Why is there so little statistical analyses in ML research?,"Why is it so common in ML research to not do any statistical test to verify that the results are actually significant? Most of the times, a single outcome is presented, instead of doing multiple runs and performing something like a t-test or Mann Whitney U Test etc. Drawing conclusions based on a single sample would be impossible in other disciplines, like psychology or medicine, why is this not considered a problem in ML research?

  
Also, can someone recommend a book for exactly this, statistical tests in the context of ml?",MachineLearning,210,117,1728465666.0,1fznaa9,BommiBrainBug,https://www.reddit.com/r/MachineLearning/comments/1fznaa9/d_why_is_there_so_little_statistical_analyses_in/,Discussion
[R] Playable 20FPS Doom via a finetuned SD1.4 model from Google research team,,MachineLearning,210,68,1724820757.0,1f31uye,greentfrapp,https://arxiv.org/abs/2408.14837,Research
[D] Any OCR recommendations for illegible handwriting?,"
Has anyone had experience using an ML model to recognize handwriting like this? The notebook contains important information that could help me decode a puzzle I’m solving. I have a total of five notebooks, all from the same person, with consistent handwriting patterns. My goal is to use ML to recognize and extract the notes, then convert them into a digital format.

I was considering Google API after knowing that Tesseract might not work well with illegible samples like this. However, I’m not sure if Google API will be able to read it either. I read somewhere that OCR+ CNN might work, so I’m here asking for suggestions. Thanks! Any advice/suggestions are welcomed! ",MachineLearning,210,171,1733475183.0,1h7x5us,SpaceSheep23,https://www.reddit.com/gallery/1h7x5us,Discussion
[D] what is the hardest thing as a machine learning engineer,"I have just begun my journey into machine learning. For practice, I obtain data from [Kaggle.com](http://kaggle.com/), but I decided to challenge myself further by collecting data on my own. I discovered that gathering a substantial amount of data is quite challenging. How is data typically collected, and are there any thing harder than that?",MachineLearning,212,155,1722618981.0,1eifnvj,3ATAE,https://www.reddit.com/r/MachineLearning/comments/1eifnvj/d_what_is_the_hardest_thing_as_a_machine_learning/,Discussion
[P] mamba.np: pure NumPy implementation of Mamba,"[mamba.np](https://preview.redd.it/7ng5b1alvk4d1.jpg?width=1024&format=pjpg&auto=webp&s=0dd281b877bd01048ebd5914381bbeb88f978418)

Inspired by some awesome projects, I implemented Mamba from scratch in pure Numpy. The goal of the code is to be simple, readable, and lightweight as it can run on your local CPU.

[https://github.com/idoh/mamba.np](https://github.com/idoh/mamba.np)

I hope you find it useful :)",MachineLearning,208,25,1717516939.0,1d80t26,id0h,https://www.reddit.com/r/MachineLearning/comments/1d80t26/p_mambanp_pure_numpy_implementation_of_mamba/,Project
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web ",MachineLearning,212,160,1715622696.0,1cr5lv8,_puhsu,https://www.reddit.com/r/MachineLearning/comments/1cr5lv8/n_gpt4o/,News
[R] Meta releases SOTA video generation and audio generation that's less than 40 billion parameters.,"Today, Meta released SOTA set of text-to-video models. These are small enough to potentially run locally. Doesn't seem like they plan on releasing the code or dataset but they give virtually all details of the model. The fact that this model is this coherent already really points to how much quicker development is occurring.

  
[https://ai.meta.com/research/movie-gen/?utm\_source=linkedin&utm\_medium=organic\_social&utm\_content=video&utm\_campaign=moviegen](https://ai.meta.com/research/movie-gen/?utm_source=linkedin&utm_medium=organic_social&utm_content=video&utm_campaign=moviegen)

This suite of models (Movie Gen) contains many model architectures but it's very interesting to see training by synchronization with sounds and pictures. That actually makes a lot of sense from a training POV.

https://preview.redd.it/047ddxdb7vsd1.png?width=1116&format=png&auto=webp&s=a7cd628a8b2dde9824b27983a430217123c297d8

  
",MachineLearning,209,45,1728102366.0,1fwic4m,AIAddict1935,https://www.reddit.com/r/MachineLearning/comments/1fwic4m/r_meta_releases_sota_video_generation_and_audio/,Research
"[D] Coworkers recently told me that the people who think ""LLMs are capable of thinking/understanding"" are the ones who started their ML/NLP career with LLMs. Curious on your thoughts.","I haven't exactly been in the field for a long time myself. I started my master's around 2016-2017 around when Transformers were starting to become a thing. I've been working in industry for a while now and just recently joined a company as a MLE focusing on NLP.

At work we recently had a debate/discussion session regarding whether or not LLMs are able to possess capabilities of understanding and thinking. We talked about Emily Bender and Timnit Gebru's paper regarding LLMs being stochastic parrots and went off from there.

The opinions were roughly half and half: half of us (including myself) believed that LLMs are simple extensions of models like BERT or GPT-2 whereas others argued that LLMs are indeed capable of understanding and comprehending text. The interesting thing that I noticed after my senior engineer made that comment in the title was that the people arguing that LLMs are able to think are either the ones who entered NLP after LLMs have become the sort of de facto thing, or were originally from different fields like computer vision and switched over.

I'm curious what others' opinions on this are. I was a little taken aback because I hadn't expected the LLMs are conscious understanding beings opinion to be so prevalent among people actually in the field; this is something I hear more from people not in ML. These aren't just novice engineers either, everyone on my team has experience publishing at top ML venues.",MachineLearning,205,322,1719673227.0,1drd3tv,Seankala,https://www.reddit.com/r/MachineLearning/comments/1drd3tv/d_coworkers_recently_told_me_that_the_people_who/,Discussion
[D] Best survey papers of 2024?,"As an AI researcher who is starting out, I usually start by seeing survey papers related to a field, then creating a roadmap to further deep dive into my research topic. I am eager to see the sub's viewpoint of the best survey papers they came across in 2024.",MachineLearning,202,41,1734507135.0,1hgwjqu,arinjay_11020,https://www.reddit.com/r/MachineLearning/comments/1hgwjqu/d_best_survey_papers_of_2024/,Discussion
[D] Why does it seem like Google's TPU isn't a threat to nVidia's GPU?,"Even though Google is using their TPU for a lot of their internal AI efforts, it seems like it hasn't propelled their revenue nearly as much as nVidia's GPUs have. Why is that? Why hasn't having their own AI-designed processor helped them as much as nVidia and why does it seem like all the other AI-focused companies still only want to run their software on nVidia chips...even if they're using Google data centers?",MachineLearning,206,140,1728693895.0,1g1okem,kugelblitz_100,https://www.reddit.com/r/MachineLearning/comments/1g1okem/d_why_does_it_seem_like_googles_tpu_isnt_a_threat/,Discussion
[R] Differential Transformer (Microsoft Research),"Abstract: Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.",MachineLearning,201,41,1728396625.0,1fz0pya,Decent_Action2959,https://arxiv.org/abs/2410.05258,Research
[D] NeurIPS 2024 Paper Reviews,"NeurIPS 2024 paper reviews are supposed to be released today. I thought to create a discussion thread for us to discuss any issue/complain/celebration or anything else.

There is so much noise in the reviews every year. Some good work that the authors are proud of might get a low score because of the noisy system, given that NeurIPS is growing so large these years. We should keep in mind that the work is still valuable no matter what the score is.",MachineLearning,199,476,1722343373.0,1efscr2,zy415,https://www.reddit.com/r/MachineLearning/comments/1efscr2/d_neurips_2024_paper_reviews/,Discussion
"[P] I was struggle how Stable Diffusion works, so I decided to write my own from scratch with math explanation 🤖",,MachineLearning,198,27,1720827505.0,1e1w2rg,jurassimo,https://www.reddit.com/gallery/1e1w2rg,Project
[D] What's your All-Time Favorite Deep Learning Paper?,"I'm looking for interesting deep learning paper, especially regarding architectural improvement in computer vision tasks.",MachineLearning,199,90,1717012806.0,1d3lbep,research_pie,https://www.reddit.com/r/MachineLearning/comments/1d3lbep/d_whats_your_alltime_favorite_deep_learning_paper/,Discussion
[D] What’s a machine learning paper or research breakthrough from the last year that everyone should know about?,Share a paper or idea that really stood out to you and why it matters to the field.,MachineLearning,195,54,1731973961.0,1gujge8,BrechtCorbeel_,https://www.reddit.com/r/MachineLearning/comments/1gujge8/d_whats_a_machine_learning_paper_or_research/,Discussion
[D] Why ML PhD is so competitive?,"In recent years, ML PhD admissions at top schools or relatively top schools getting out of the blue. Most programs require prior top-tier papers to get in. Which considered as a bare minimum.

On the other hand, post PhD Industry ML RS roles are also extremely competitive as well.

But if you see, EE jobs at Intel, NVIDIA, Qualcomm and others are relatively easy to get, publication requirements to get into PhD or get the PhD degree not tight at all compared to ML. And I don’t see these EE jobs require “highly-skilled” people who know everything like CS people (don’t get me wrong that I devalued an EE PhD). Only few skills that all you need and those are not that hard to grasp (speaking from my experience as a former EE graduate).

I graduated with an EE degree, later joined a CS PhD at a moderate school (QS < 150). But once I see my friends, I just regret to do the CS PhD rather following the traditional path to join in EE PhD. ML is too competitive, despite having a better profile than my EE PhD friends, I can’t even think of a good job (RS is way too far considering my profile). 

They will get a job after PhD, and most will join at top companies as an Engineer. And I feel, interviews at EE roles as not as difficult as solving leetcode for years to crack CS roles. And also less number of rounds in most cases. ",MachineLearning,196,88,1731949655.0,1gu9os9,AntelopeWilling2928,https://www.reddit.com/r/MachineLearning/comments/1gu9os9/d_why_ml_phd_is_so_competitive/,Discussion
[D] LLMs: Why does in-context learning work? What exactly is happening from a technical perspective?,"Everywhere I look for the answer to this question, the responses do little more than anthropomorphize the model. They invariably make claims like:

> *Without examples, the model must infer context and rely on its knowledge to deduce what is expected. This could lead to misunderstandings.*

> *One-shot prompting reduces this cognitive load by offering a specific example, helping to anchor the model's interpretation and focus on a narrower task with clearer expectations.*

> *The example serves as a reference or hint for the model, helping it understand the type of response you are seeking and triggering memories of similar instances during training.*

> *Providing an example allows the model to identify a pattern or structure to replicate. It establishes a cue for the model to align with, reducing the guesswork inherent in zero-shot scenarios.*

These are real excerpts, btw.

But these models don’t “understand” anything. They don’t “deduce”, or “interpret”, or “focus”, or “remember training”, or “make guesses”, or have literal “cognitive load”. They are just statistical token generators. Therefore pop-sci explanations like these are kind of meaningless when seeking a concrete understanding of the exact mechanism by which in-context learning improves accuracy.

Can someone offer an explanation that explains things in terms of the actual model architecture/mechanisms and how the provision of additional context leads to better output? I can “talk the talk”, so spare no technical detail please.

I could make an educated guess - Including examples in the input which use tokens that approximate the kind of output you want leads the attention mechanism and final dense layer to weight more highly tokens which are similar in some way to these examples, increasing the odds that these desired tokens will be sampled at the end of each forward pass; like fundamentally I’d guess it’s a similarity/distance thing, where explicitly exemplifying the output I want increases the odds that the output get will be similar to it - but I’d prefer to hear it from someone else with deep knowledge of these models and mechanisms.",MachineLearning,199,84,1714129298.0,1cdih0a,synthphreak,https://www.reddit.com/r/MachineLearning/comments/1cdih0a/d_llms_why_does_incontext_learning_work_what/,Discussion
[D] OpenAI new reasoning model called o1,"OpenAI has released a new model that is allegedly better at reasoning what is your opinion ?

[https://x.com/OpenAI/status/1834278217626317026](https://x.com/OpenAI/status/1834278217626317026)",MachineLearning,193,128,1726162609.0,1ff8f7v,IIAKAD,https://www.reddit.com/r/MachineLearning/comments/1ff8f7v/d_openai_new_reasoning_model_called_o1/,Discussion
[R] How Google Overcame Training Data Issues For Medical AI,"TLDR; They turned 3D images into vector embeddings, saving preprocessing time and reducing training data sizes.

Over 70 million Computed Tomography exams are conducted each year in the USA alone, but that data wasn't effective for Google's training.  
Google Research had embedding APIs for radiology, digital pathology, and dermatology-- but all of these are limited to 2D imaging. Physicians typically rely on 3D imaging for more complex diagnostics.

Why?

CT scans have a 3D structure, meaning larger file sizes, and the need for more data than 2D images.  
Looking through engineering blogs, they just released something to finally work with 3D medical data. It's called CT Foundation-- it turns CT scans to small and information-rich embeddings to train AI for cheap

How?

Exams are taken in standard medical imaging format (DICOM) and turned into vectors with 1,408 values— key details captured include organs, tissues, and abnormalities.

These concise embeddings can then be used to train AI models, such as logistic regression or multilayer perceptrons, using much less data compared to typical models that take 3D images and require preprocessing. The final classifier is smaller, reducing compute costs so training is more efficient and affordable.

Final Results?

CT Foundation was evaluated for data efficiency across seven tasks to classify:  
\- intracranial hemorrhage  
\- chest and heart calcifications  
\- lung cancer prediction  
\- suspicious abdominal lesions  
\- nephrolithiasis  
\- abdominal aortic aneurysm, and  
\- body parts

Despite limited training data, the models achieved over 0.8 AUC on all but one of the more challenging tasks, meaning a strong predictive performance and accuracy.  
The model, using 1,408-dimensional embeddings, required only a CPU for training, all within a Colab Python notebook.

TLDR;

Google Research launched a tool to effectively train AI on 3D CT scans, by converting them into compact 1,408-dimensional embeddings for efficient model training. It's called CT Foundation, requires less data and processing, and achieved over 0.8 AUC in seven classification tasks, demonstrating strong predictive performance with minimal compute resources.  
There's a colab notebook [available](https://colab.research.google.com/github/Google-Health/imaging-research/blob/master/ct-foundation/CT_Foundation_Demo.ipynb).

**PS**: Learned this by working on a personal project to keep up with tech-- if you'd like to know more, check [techtok today](https://techtok.today/)",MachineLearning,188,27,1729789905.0,1gb7twh,TechTok_Newsletter,https://www.reddit.com/r/MachineLearning/comments/1gb7twh/r_how_google_overcame_training_data_issues_for/,Research
"[D] Is the new norm for NLP papers ""prompt engineering"" papers?","So many papers seem to essentially be ""how can we make LLM 1 do this without training?"" I haven't published in a while and have been in industry for the past few years. I recently joined a new company in a slightly more research-y position and am working with research scientists and graduate interns. I've noticed that every single one of them is working on something that I would have been reprimanded by my PI for in graduate school. Basically, ""how can we make LLMs do this really complicated task without doing any training?"" And perhaps somewhat unsurprisingly, in many cases, you can't. I think that's why these days there are so many negative result papers in NLP.

Is this the new norm? It's become a pain to go through the CL section of arXiv. 98% of the papers are something like ""how come LLaMA can't understand numbers?""

I'm wondering if I'm just being the senile old man in the corner of the bar or if everyone else feels the same.",MachineLearning,184,61,1722603447.0,1ei9e3l,Seankala,https://www.reddit.com/r/MachineLearning/comments/1ei9e3l/d_is_the_new_norm_for_nlp_papers_prompt/,Discussion
[D] What are the (un)written rules of deep learning training ,"Disclaimer: I posted this in r/learnmachinelearing first, but the sub seems to be more concerned with very basic questions, courses and hiring, so feel free to remove it if it doesn't fit here (tho I think that also fits this sub as a discussion).

I now have a few years of experience building and training different model architectures, I know most of the basic theory and am able to follow most papers. So my question goes into a more methodological direction. While I am able to successfully build models for a number of applications, a lot of the time this is to a large extend guesswork. I try out different stuff and see what sticks. I know there is a lot of research in the direction of interpretability going on, but this is not directly the direction I want to go with this. Instead I want to ask you all what general advice you have on the training process, what are some practical observations, rules of thumb, approaches you take that are not described in a paper or theoretical ml class. For example:

- How do you analyze gradients in your model. I know how to do some very basic plots in this regard, but would be interested in your methods and how you read them from a practical perspective?

- How do you visualize temporal instabilities between optimizer steps resulting from e.g. a too large learning rate?

- How do you determine appropriate regularization?

- What are your rules of thumb for diminisheing returns during a training run?

- How do you tune your hyperparameters? I eyeballed them more or less and also used optuna for this in the past.

- What are some important intuitions, unwritten rules and pitfalls during training in your opinion?

- What are your debugging steps when a model does not perform as expected?

- What tricks do you actually use? There are lots of small tricks (EMA, obscure activation functions, ...) that promise some gains, but what do you actually use?

- How does your approach differ when you do a transformer, CNN, diffusion model, ...

- Some general opinions or tips that I might have missed above. 

University classes and online resources mostly teach the basics or theoretical foundation, which is very important, but in practice only part of the story. Real world experience also helps, but you only get so far with trial and error and might miss something useful.
I am aware of the blog posts by Karpathy on the training of neural networks and look for more resources in this direction.

I am happy to here your replies on this arguably broad topic. ",MachineLearning,185,46,1734172142.0,1he07vr,floriv1999,https://www.reddit.com/r/MachineLearning/comments/1he07vr/d_what_are_the_unwritten_rules_of_deep_learning/,Discussion
[R] Dynamic Attention-Guided Diffusion for Image Super-Resolution,"I'm glad to share that our paper ""Dynamic Attention-Guided Diffusion for Image Super-Resolution"" got accepted for WACV2025:  
[https://arxiv.org/abs/2308.07977](https://arxiv.org/abs/2308.07977)

The goal of this work was to introduce a new attention-guided diffusion mechanism to focus image refinement on essential areas that benefit the most from deep refinement :)",MachineLearning,185,8,1730169070.0,1gekbg3,Maleficent_Stay_7737,https://www.reddit.com/r/MachineLearning/comments/1gekbg3/r_dynamic_attentionguided_diffusion_for_image/,Research
[D] Has ML actually moved the needle on human health?,"We've been hearing about ML for drug discovery, precision medicine, personalized treatment, etc. for quite some time. What are some ways ML has actually moved the needle on human health?

It seems like most treatments and diagnostics are still based on decades of focused biology research rather than some kind of unbiased ML approach. Radiology is one notable exception that benefited from advances in machine vision, but even they seem slow to accept AI as clinical practice.",MachineLearning,181,108,1716243995.0,1cwsbyw,Potential_Athlete238,https://www.reddit.com/r/MachineLearning/comments/1cwsbyw/d_has_ml_actually_moved_the_needle_on_human_health/,Discussion
Built gpt2 in C [P],"Implementation of the GPT-2 paper by OpenAI from first principles in plain C language.
1. Forward propagation and backpropagation of various GPT components like LayerNorm, Multi-Layer Perceptron (MLP), and Causal Attention are implemented from scratch.
2. No autograd engine like PyTorch is used; gradients of the model weights are computed using hand-derived derivatives. This method reduces memory usage by almost 20 GB by not saving unnecessary activation values.
3. Memory management of activations and model weights is handled through memory mapping of files.
4. The purpose of this project is to explore the low-level inner workings of PyTorch and deep learning.
5. Anyone with a basic understanding of C can easily comprehend and implement other large language models (LLMs) like LLaMA, BERT, etc.

Repo link:https://github.com/shaRk-033/ai.c",MachineLearning,179,39,1726425819.0,1fhjtyo,Silly-Dig-3312,https://www.reddit.com/r/MachineLearning/comments/1fhjtyo/built_gpt2_in_c_p/,Project
"[D] ""Grok"" means way too many different things","I am tired of seeing this word everywhere and it has a different meaning in the same field everytime. First for me was when Elon Musk was introducing and hyping up Twitter's new (not new now but was then) ""Grok AI"", then I read more papers and I found a pretty big bombshell discovery that apparently everyone on Earth had known about besides me for awhile which was that after a certain point overfit models begin to be able to generalize, which destroys so many preconceived notions I had and things I learned in school and beyond. But this phenomenon is also known as ""Grok"", and then there was this big new ""GrokFast"" paper which was based on this definition of Grok, and there's ""Groq"" not to be confused with these other two ""Grok"" and not to even mention Elon Musk makes his AI outfit named ""xAI"" which mechanistic interpretability people were already using that term as a shortening of ""explainable AI"", it's too much for me",MachineLearning,178,114,1719597569.0,1dqpyb3,Traditional_Land3933,https://www.reddit.com/r/MachineLearning/comments/1dqpyb3/d_grok_means_way_too_many_different_things/,Discussion
[Research] xLSTM: Extended Long Short-Term Memory,"Abstract:

In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.

Link: [xLSTM: Extended Long Short-Term Memory](https://arxiv.org/abs/2405.04517)",MachineLearning,175,48,1715144800.0,1cmwljs,Background_Thanks604,https://www.reddit.com/r/MachineLearning/comments/1cmwljs/research_xlstm_extended_long_shortterm_memory/,Research
[P] Curated list of LLM papers 2024,,MachineLearning,176,12,1734187946.0,1he4htl,seraschka,https://magazine.sebastianraschka.com/p/llm-research-papers-the-2024-list,Project
"[D] AMA: I’m Head of AI at a firm in the UK, advising Gov., industry, etc. ","Ask me anything about AI adoption in the UK, tech stack, how to become an AI/ML Engineer or Data Scientist etc, career development you name it. ",MachineLearning,175,153,1731486020.0,1gq899s,Psychological_Dare93,https://www.reddit.com/r/MachineLearning/comments/1gq899s/d_ama_im_head_of_ai_at_a_firm_in_the_uk_advising/,Discussion
[R] What is your Recipe for Training Neural Networks in 2024?,"You may already know the [Recipe for Training Neural Networks](http://karpathy.github.io/2019/04/25/recipe/) bible from Karpathy 2019

While most of the advices are still valid, the landscape of Deep Learning model/method has changed a lot since. Karpathy's advices work well in the supervised learning setting, he does mention it:

>stick with supervised learning. Do not get over-excited about unsupervised pretraining. Unlike what that blog post from 2008 tells you, as far as I know, no version of it has reported strong results in modern computer vision (though NLP seems to be doing pretty well with BERT and friends these days, quite likely owing to the more deliberate nature of text, and a higher signal to noise ratio).

I've been training a few image diffusion models recently, and I find it harder to make data driven decisions in the unsupervised setting. Metrics are less reliable, sometimes I train models with better losses but when I look at the samples they look worse

Do you know more modern recipes to train neural network in 2024? (and not just LLMs)",MachineLearning,176,43,1730646344.0,1giovxi,Even_Information4853,https://www.reddit.com/r/MachineLearning/comments/1giovxi/r_what_is_your_recipe_for_training_neural/,Research
[D] Isn't hallucination a much more important study than safety for LLMs at the current stage?,"Why do I feel like safety is so much emphasized compared to hallucination for LLMs?

Isn't ensuring the generation of accurate information given the highest priority at the current stage?

why it seems like not the case to me",MachineLearning,171,168,1716951996.0,1d329nt,xiikjuy,https://www.reddit.com/r/MachineLearning/comments/1d329nt/d_isnt_hallucination_a_much_more_important_study/,Discussion
[R] Are you a reviewer for NeurIPS'24? Please read this,"Hello!

I am currently serving as an area chair (AC) for [NeurIPS'24](https://neurips.cc/). The number of submissions is extremely high, and assigning qualified reviewers to these papers is tough.

**Why is it tough**, you may ask. At a high-level, it's because we, as AC, have not enough information to gauge whether a paper is assigned to _a sufficient number_ (at least 3) of _qualified reviewers_ (i.e., individuals who can deliver an informative assessment of the paper). Indeed, as AC, we can only use the following criteria to decide whether to assign a reviewer to any given paper: (i) their bids; (ii) the ""affinity"" score; (iii) their personal OpenReview profile. However

* Only a fraction of those who signed up as reviewers have bid on the papers. To give an idea, among the papers in my stack, 30% had no reviewer who bid on them; actually, most of the papers had only 3-4 bids (not necessarily ""positive""). 
* When no bids are entered, the next indicator is the ""affinity"" score. However, this metric is computed in an automatic way and works poorly (besides, one may be an expert of a domain but they may be unwilling to review a certain paper, e.g., due to personal bias).
* The last indicator we can use is the ""background"" of the reviewer, but this requires us (i.e., the ACs) to manually check the OpenReview profile of each reviewer---which is time consuming. To make things worse, for this year's NeurIPS there is a (relatively) high number of reviewers who are undergrads or MS students, and whose OpenReview's profile is _completely empty_.

Due to the above, I am writing this post to _ask for your cooperation_. If you're a reviewer for NeurIPS, **please ensure that your OpenReview profile is up to date**. If you are an undergrad/MS student, please include a link to a webpage that can show if you have any expertise in reviewing, or if you work in a lab with some ""expert researchers"" (who can potentially help you by giving tips on how to review). The same also applies for PhD students or PostDocs: ensure that the information available on OpenReview reflects your expertise and preferences.

Bottom line: you have accepted to serve as a reviewer of (arguably the top) a premier ML conference. **Please, take this duty seriously.** If you are assigned to the right papers, you will be able to provide more helpful reviews and the reviewing process will also be smoother. Helpful reviews are useful to the authors and to the ACs. By doing a good job, you may even be awarded with ""top reviewer"" acknowledgements.",MachineLearning,171,92,1717695984.0,1d9o8tn,hihey54,https://www.reddit.com/r/MachineLearning/comments/1d9o8tn/r_are_you_a_reviewer_for_neurips24_please_read/,Research
[D] Modern best coding practices for Pytorch (for research)?,"Hi all, I've been using Pytorch since 2019, and it has changed a lot in that time (especially since huggingface). 

Are there any modern guides/style-docs/example-repos  you would recommend? For example, are namedtensors a good/common practice? Is Pytorch Lightning recommended? What are the best config management tools these days? How often do you use torch.script or torch.compile?",MachineLearning,172,36,1714598682.0,1chxpka,SirBlobfish,https://www.reddit.com/r/MachineLearning/comments/1chxpka/d_modern_best_coding_practices_for_pytorch_for/,Discussion
[R] I got my first publication!,"A little more than a year ago a childhood friend of mine who is a doctor called me out of the blue asking me if I'd be interested in implementing an idea he had about screening and selecting liver cancer patients for transplant using ML and I said why not.



Last weekend I received the email of our [journal publication](https://www.surgjournal.com/article/S0039-6060(24)00558-0/abstract) and I wanted to share the news :D



P.S - Anyone interested in reading the paper, please feel free to DM",MachineLearning,172,27,1724680723.0,1f1ove1,theahmedmustafa,https://www.reddit.com/r/MachineLearning/comments/1f1ove1/r_i_got_my_first_publication/,Research
[D] What makes a good PhD student in ML,Hey as I started my PhD (topic: Interpretable Object Detection) recently I would be really curious to know what set of features you think make a successfull PhD student,MachineLearning,172,69,1731420833.0,1gplmzb,RaeudigerRaffi,https://www.reddit.com/r/MachineLearning/comments/1gplmzb/d_what_makes_a_good_phd_student_in_ml/,Discussion
[D] What makes TikTok's recommendation algorithm so strong?,"General Discussion - now that they are about to be banned in the US, I'm becoming fascinated by the strength of their For You recommendations. To try and put some guard rails on what I mean, TikTok has shown itself to be able to match content to relevant audience at greater frequency and scale than any other app (YouTube included). Many creators can join the platform, post a single video, and have millions of views in 24 hours. This does happen on other apps, but TikTok seems to be the most consistent at scaling audience incredibly fast.

What models might they be basing their system on? What about their models creates their competitive advantage?",MachineLearning,165,54,1734021546.0,1hcp4xw,No_Collection_5509,https://www.reddit.com/r/MachineLearning/comments/1hcp4xw/d_what_makes_tiktoks_recommendation_algorithm_so/,Discussion
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w",MachineLearning,166,46,1715563463.0,1cqndld,TeamArrow,https://www.reddit.com/r/MachineLearning/comments/1cqndld/d_please_consider_signing_this_letter_to_open/,Discussion
[D] Reviewers you all need to stop being so lazy dog. Why are reviewers doing things so lazy man? ,"I submitted a paper.

Gets accepted to conference.

Got email from some random dude from \_insert\_university\_. Sending to both the chair and conference head.

Accuses me a plagarism and says 92% matching of publish papers...

Check cross reference. Title, authors (me and the mentor), data, conclusion, and almost the entire paper is highlighted.

Only source says Arkiv. I have my pre-print on there by chance. I followed their policies with pre-prints and put the notices.

Now, this is very stupid. I done a lot of due diligence and if its matching the authors, it has to be referencing my pre-print.

Why are reviewers so lazy and can do such drastic actions instead of just asking authors questions about these? I seriously don't understand some of these people. Do you have any suggestions about dealing with these situations?",MachineLearning,162,41,1715223825.0,1cnn54n,I_will_delete_myself,https://www.reddit.com/r/MachineLearning/comments/1cnn54n/d_reviewers_you_all_need_to_stop_being_so_lazy/,Discussion
[R] Waving Goodbye to Low-Res: A Diffusion-Wavelet Approach for Image Super-Resolution,"We are thrilled to share that we successfully presented DiWa at this year's International Joint Conference on Neural Networks (IJCNN 2024)! :-)

TL;DR: DiWa is a diffusion-wavelet technique for enhancing images. It merges diffusion models with discrete wavelet transformations and an initial regression-based predictor to achieve high-quality, detailed image reconstructions. Feel free to contact us about the paper, our findings, or future work!

arXiv: https://arxiv.org/abs/2304.01994",MachineLearning,161,2,1723218017.0,1eo2xs9,Maleficent_Stay_7737,https://www.reddit.com/r/MachineLearning/comments/1eo2xs9/r_waving_goodbye_to_lowres_a_diffusionwavelet/,Research
[D] What are issues in AI/ML that no one seems to talk about?,"I’m a graduate student studying Artificial Intelligence and I frequently come across a lot of similar talking points about concerns surrounding AI regulation, which usually touch upon something in the realm of either the need for high-quality unbiased data, model transparency, adequate governance, or other similar but relevant topics. All undoubtedly important and complex issues for sure.

However, I was curious if anyone in their practical, personal, or research experience has come across any unpopular or novel concerns that usually aren’t included in the AI discourse, but stuck with you for whatever reason. 

On the flip side, are there even issues that are frequently discussed but perhaps are grossly underestimated?

I am a student with a lot to learn and would appreciate any insight or discussion offered. Cheers.
",MachineLearning,161,205,1720040155.0,1dup0vs,mrstealyoursoulll,https://www.reddit.com/r/MachineLearning/comments/1dup0vs/d_what_are_issues_in_aiml_that_no_one_seems_to/,Discussion
[D] The Parallelism Tradeoff: Understanding Transformer Expressivity Through Circuit Complexity,"Talk: https://www.youtube.com/watch?v=7GVesfXD6_Q

Paper: https://aclanthology.org/2023.tacl-1.31/

TL;DR the author (Will Merrill) looks at transformers from a circuit complexity perspective and places them in the TC^0 complexity class - threshold circuits of constant depth. This is a relatively restricted complexity class that cannot solve many inherently sequential problems.

Their main point is that the expressive limitations of transformers come from their parallel nature, rather details of their architecture. Adding chain of thought allows transformers to solve problems from additional complexity classes, but at the cost of sacrificing parallelism and efficient training.

They suggest that this tradeoff between parallel and sequential computation cannot be avoided, and future architectures should be designed with the tradeoff in mind. They also look at an extension to state space models that makes the tradeoff more efficiently than transformers+CoT.",MachineLearning,162,8,1735329915.0,1hnnl6s,currentscurrents,https://www.reddit.com/r/MachineLearning/comments/1hnnl6s/d_the_parallelism_tradeoff_understanding/,Discussion
[D] What industry has the worst data?,"Curious to hear - what industry do you think has the worst quality data for ML, consistently? 

I'm not talking individual jobs that have no realistic and foreseeable ML applications like carpentry. I'm talking your larger industries, banking, pharma, telcos, tech (maybe a bit broad), agriculture, mining, etc, etc. 

Who's the deepest in the sh\*\*ter?",MachineLearning,159,176,1724333000.0,1eyj7vq,Standard_Natural1014,https://www.reddit.com/r/MachineLearning/comments/1eyj7vq/d_what_industry_has_the_worst_data/,Discussion
[R] Dynamic Gaussians Mesh,,MachineLearning,159,6,1714368567.0,1cfraht,XiaolongWang,https://i.redd.it/y9r6db5wtcxc1.gif,Research
[D] Has torch.compile killed the case for JAX?,"I love JAX, but I fully concede that you sacrifice ease of development for performance.

I've seen some buzz online about the speedups due to torch.compile, but I'm not really up to date. The is performance case for JAX dead now, or are the impressive GPU performance due to other factors like multi-GPU, etc.",MachineLearning,158,77,1730553048.0,1ghw330,internet_ham,https://www.reddit.com/r/MachineLearning/comments/1ghw330/d_has_torchcompile_killed_the_case_for_jax/,Discussion
[P] Updates on OpenCL backend for Pytorch,"I develop [the OpenCL backend for pytorch](https://github.com/artyom-beilis/pytorch_dlprim) - it allows to train your networks on AMD, NVidia and Intel GPUs on both Windows and Linux. Unlike cuda/cudnn based solution - it is cross platform and fully open source.

Updates:

1. With an assistance from pytorch core developers now pytorch 2.4 is supported
2. Now it is easy to install it - I provide now prebuild packages for Linux and Windows - just install whl package and you are good to go
3. Lots of other improvements

How do you use it:

- Download whl file from project page according to operating system, python version and pytorch version
- Install CPU version of pytorch and install whl you downloaded, for example `pytorch_ocl-0.1.0+torch2.4-cp310-none-linux_x86_64.whl` 
- Now just `import pytorch_ocl` and now you can train on OpenCL `ocl` devices: `torch.randn(10,10,dev='ocl:2')

How is the performance: while it isn't as good as native NVidia cuda or AMD rocm it still gives [reasonable performance](https://github.com/artyom-beilis/pytorch_dlprim/blob/master/benchmark.md) depending on platform, network - usually around 60-70% for training and 70-80% for inference.",MachineLearning,155,38,1723873957.0,1euamk8,artyombeilis,https://www.reddit.com/r/MachineLearning/comments/1euamk8/p_updates_on_opencl_backend_for_pytorch/,Project
[D] ML Researchers in Industry: How Do You Find Time to Publish Papers?,"Background: I work in computer vision at a FAANG company. I'm incredibly lucky that I get to work on applying relatively state of the art techniques. I generally attend at least one big conference per year, and I see a ton of industry scientists with talks/posters, and I have to ask: how??

I spend my 40 hours per week applying techniques to datasets/problems specific to my company. I'm good at my job, keep up to date with the most recent techniques, and generate a lot of value for my employer. The techniques may even be publishable, but it would require benchmarking the methods on open-source datasets. I can't imagine finding the additional time required to run all the experiments and writing, while still having a life and hobbies. 

Despite all this, I feel that it's expected of me. It seems normalized that the scientists I work with basically don't have lives outside of research (except maybe they go hiking on the weekend...).",MachineLearning,157,77,1718729812.0,1divk13,generating_loop,https://www.reddit.com/r/MachineLearning/comments/1divk13/d_ml_researchers_in_industry_how_do_you_find_time/,Discussion
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?",MachineLearning,158,44,1715711386.0,1crzdhd,Flowwwww,https://www.reddit.com/r/MachineLearning/comments/1crzdhd/d_gpt4o_natively_multimodal_what_does_this/,Discussion
[D] Why would such a simple sentence break an LLM?,"This is a prompt I entered into MS Copilot (GPT4 Turbo).

It's in german but it just means ""***Would there be any disadvantages if I took the full bath first?***""), so this can't be another SolidGoldMagikarp or similar, because the words clearly were in both tokenizer and training vocab.

Why would such a simple sentence cause this? Any guesses? (also tried with Claude Opus and LLama 3 70b, which worked fine)

&#x200B;

https://preview.redd.it/9x6mva7b6gwc1.png?width=1129&format=png&auto=webp&s=bb6ac52d1c52d981161e8a864c5d1dd3794ca392",MachineLearning,157,96,1713974381.0,1cc1v32,michael-relleum,https://www.reddit.com/r/MachineLearning/comments/1cc1v32/d_why_would_such_a_simple_sentence_break_an_llm/,Discussion
[D] Fine tuning large language models,"These articles explore the idea behind parameter-efficient fine-tuning, showcasing Low-Rank Adaptation (LoRA) implementation on a Multi-Layer Perceptron (MLP). Then also explain how fewer parameters are responsible for effective learning (Intrinsic Dimension) and techniques (random subspace training) to measure it for a given task.

1.[ Exploring LoRA — Part 1: The Idea Behind Parameter Efficient Fine-Tuning and LoRA](https://medium.com/inspiredbrilliance/exploring-lora-part-1-the-idea-behind-parameter-efficient-fine-tuning-and-lora-ec469d176c26)

2. [Exploring LoRA - Part 2: Analyzing LoRA through its Implementation on an MLP](https://medium.com/inspiredbrilliance/exploring-lora-part-2-analyzing-lora-through-its-implementation-on-an-mlp-fbc386036f6f#7b69)

3. [Intrinsic Dimension Part 1: How Learning in Large Models Is Driven by a Few Parameters and Its Impact on Fine-Tuning](https://medium.com/inspiredbrilliance/intrinsic-dimension-part-1-why-large-models-can-be-fine-tuned-with-fewer-parameters-d15702d2943e)

4. [Intrinsic Dimension Part 2: Measuring the True Complexity of a Model via Random Subspace Training](https://medium.com/inspiredbrilliance/intrinsic-dimension-part-2-measuring-the-true-complexity-of-a-model-via-random-subspace-training-bdf4ef27430c)",MachineLearning,154,5,1734961263.0,1hknz26,l1cache,https://www.reddit.com/r/MachineLearning/comments/1hknz26/d_fine_tuning_large_language_models/,Discussion
"[D] As a researcher, how do you become industry-ready?","Being a PhD student, much of my time is spent on supervising students, project management and writing ""quick and dirty"" code for prototyping. I intend to move to industry after the PhD, but I feel like I'm missing out on key software engineering skills and good coding practices. Does anyone else feel this way? How do you upskill yourself to be industry-ready while doing a PhD? ",MachineLearning,152,42,1730876843.0,1gksoi7,fullgoopy_alchemist,https://www.reddit.com/r/MachineLearning/comments/1gksoi7/d_as_a_researcher_how_do_you_become_industryready/,Discussion
"[R] ""How to train your VAE"" substantially improves the reported results for standard VAE models (ICIP 2024)","https://preview.redd.it/b1dmh67uroxd1.png?width=1025&format=png&auto=webp&s=3d42a65e2c0a946aa307f01886aebedfc4b88b8e

The proposed method redefines the Evidence Lower Bound (ELBO) with a mixture of Gaussians for the posterior probability, introduces a regularization term to prevent variance collapse, and employs a PatchGAN discriminator to enhance texture realism. The main contribution in this work is an ELBO that reduces the collapse of the posterior towards the anterior (observed as the generation of very similar, blurry images)

[https://arxiv.org/abs/2309.13160](https://arxiv.org/abs/2309.13160)  
[https://github.com/marianorivera/How2TrainUrVAE](https://github.com/marianorivera/How2TrainUrVAE)",MachineLearning,156,17,1730203701.0,1get08n,jarkkowork,https://www.reddit.com/r/MachineLearning/comments/1get08n/r_how_to_train_your_vae_substantially_improves/,Research
[D] Is anyone else absolutely besieged by papers and always on the verge of getting scooped?,"I'm a 1st year PhD student working on a hot area in ML (3 guesses as to what lol) and the past year has been absolutely brutal for me on a personal level. Every single weekday, I check the daily arxiv digest that hits my inbox, and there are consistently always 3-5 new papers that are relevant to my topic, especially recently given that everyone is now releasing their Neurips submissions.

No paper has directly scooped what I've been working on so far, but there were so many near-misses lately that I'm worried that either (a) it's only a matter of time, and I should work even faster to get a preprint out; or (b) even if I do get a paper out in the near future, it's one among a dozen similar titles that it won't get much traction. Some papers even have my advisor's name on them since she is a Big Famous Professor and is very amenable to collaboration (I sometimes think because she pitches the same ideas to multiple people, there is inevitably some local scooping going on). These circumstances drive up my anxiety, since I feel that speed is really the best comparative advantage here; it's all speed iteration from idea generation to execution to publication.

IDK, I felt like I was so prolific and accomplished and ahead of the curve as an undergrad, and now it's been a year and I'm still struggling to get a meaningful and novel idea out....is anyone else in the same boat? Does anyone have helpful advice...for dealing with the stress of fast publication cycles, or for generally struggling through the early years of research, or for how to think faster and better? Thanks for listening to my (possibly hideously naive) rant....",MachineLearning,152,56,1719550131.0,1dqbgw4,akardashian,https://www.reddit.com/r/MachineLearning/comments/1dqbgw4/d_is_anyone_else_absolutely_besieged_by_papers/,Discussion
[D] Everyone is so into LLMs but can the transformer architecture be used to improve more ‘traditional’ fields of machine learning ,"
i’m thinking things like recommendation algorithms, ones that rely on unsupervised learning or many other unsupervised algos 

i’ll look more into it but wanted to maybe get some thoughts on it ",MachineLearning,153,87,1735195247.0,1hmitcz,noithatweedisloud,https://www.reddit.com/r/MachineLearning/comments/1hmitcz/d_everyone_is_so_into_llms_but_can_the/,Discussion
[D] What’s hot for Machine Learning research in 2025?,"Which of the sub-fields/approaches within ML or related to ML, application areas are expected to gain much attention (pun unintended) in 2025?",MachineLearning,156,63,1734750009.0,1hj0p0y,ureepamuree,https://www.reddit.com/r/MachineLearning/comments/1hj0p0y/d_whats_hot_for_machine_learning_research_in_2025/,Discussion
[D] I’m an ML/programming educator - I was invited as ceo of codesmith to Berlin Global Dialogue (tech/AI insider conference) - see what they said behind closed doors - AMA,"**Edit 2:** Came back and answered a few more Qs - I’m going to do a vid to summarize some of the discussion at some point (will share) but in meantime if you want to talk more feel free to DM me here or on [https://x.com/willsentance](https://x.com/willsentance)

**Edit (5pm PT):** Thanks so much all for really great questions - I'm going to pause now but will take a look over next 24 hours and try to answer any more questions. V grateful for chance to do this and to others who helped answer some of the Qs too from their perspective (shoutout u/Rebeleleven)

\--

I'm Will Sentance - I recently had the opportunity to attend the Berlin Global Dialogue, which has been likened to Davos but with a stronger focus on technology and AI . The lineup was impressive: Hermann Hauser, the founder of ARM, executives from OpenAI and ASML, and a mix of founders from emerging startups tackling everything from quantum ML to supply chain optimization. Even leaders like President Macron and the German Vice Chancellor were there, engaging with critical tech issues that impact us all.

As the CEO of Codesmith – a small, independent tech school with a data science and machine learning research group (last year we contributed to TensorFlow) – I was invited to announce our latest endeavor: Codesmith’s AI & ML Technical Leadership Program.

I shared this experience in an AMA on r/technology and had a great conversation—but the depth of questions around ML/AI didn’t quite match what I’d hoped to explore. I spoke to the mods here and am grateful for them supporting this AMA. 

Proof: [https://imgur.com/a/bYkUiE7](https://imgur.com/a/bYkUiE7)

My real passion, inherited from my parents who were both educators, is teaching and making ML more accessible to a broader audience. I’m currently developing an AI/ML workshop for Frontend Masters, and I want to hear from those navigating the ML field. What’s the biggest challenge you're facing in this space?

A few of my takeaways from the event:

* **Chip manufacturers are shifting to new architectures** rather than further miniaturization due to physical limits. High-bandwidth memory (HBM) is a central focus for future roadmaps.
* Europe is fixated on finding a ‘tech champion,’ but there's a distinct emphasis on core industries rather than consumer internet—think ASML and ARM.
* **Quantum ML is gaining momentum** and receiving government support, particularly for applications like climate forecasting (e.g., Germany’s Klim-QML initiative). While promising, these efforts are still in the prototype phase.
* There was also, candidly, a lot of talk without much substance. Even OpenAI execs demonstrated a need for more leaders with deep technical insights.

Looking forward to diving deeper into these issues and the broader challenges in ML/AI in an AMA!",MachineLearning,154,43,1730316671.0,1gfv37y,WillSen,https://www.reddit.com/r/MachineLearning/comments/1gfv37y/d_im_an_mlprogramming_educator_i_was_invited_as/,Discussion
[D] Why so many of the most skilled people in the ML field are not working for big techs?,"I've seen so many people with degree from ivy league, research papers authors, prize winners, course teachers, book writers in the field, but you see their linkedin and the majority of those guys are not in big techs (MANGA companies) like Google, Microsoft, Amazon, Meta and you name it, they are often in small or medium size companies, i mean, a person that write a book about machine learning must know the thing, people with Cambrige or Harvard CS degree may know something about it, why there are so many out of big techs?

I know that a lot of these guys wanna focus on research and not industry, but big tech companies does produce state of the art research in ML, so to me is hard to know why those companies dont want these guys or why they dont want to work for big tech companies.",MachineLearning,150,140,1722205110.0,1eejd3b,millhouse056,https://www.reddit.com/r/MachineLearning/comments/1eejd3b/d_why_so_many_of_the_most_skilled_people_in_the/,Discussion
[R] Google Shopping 10M dataset for large scale multimodal product retrieval and ranking,"We have finally released the Marqo Google Shopping 10 million dataset on Hugging Face ([Marqo-GS-10M](https://huggingface.co/datasets/Marqo/marqo-GS-10M)). One of the largest and richest datasets for multimodal product retrieval! 

+ 10M rows of query, product title, image and rank (1-100) 

+ \~100k unique queries 

+ \~5M unique products across fashion and home 

+ Reflects real-world data and use cases and serves as a good benchmark for method development 

+ Proper data splits, in-domain, novel query, novel document and novel-document and novel query. 

The dataset features detailed relevance scores for each query-document pair to facilitate future research and evaluation.

    !pip install datasets
    from datasets import load_dataset
    ds = load_dataset(""Marqo/marqo-GS-10M"")

We curated this large-scale dataset as part of the publication of our training framework: Generalized Contrastive Learning (GCL). 

Dataset: [https://huggingface.co/datasets/Marqo/marqo-GS-10M](https://huggingface.co/datasets/Marqo/marqo-GS-10M)

GCL: [https://github.com/marqo-ai/GCL](https://github.com/marqo-ai/GCL)

Paper: [https://arxiv.org/abs/2404.08535](https://arxiv.org/abs/2404.08535)",MachineLearning,154,4,1729461101.0,1g8a3pv,Jesse_marqo,https://www.reddit.com/r/MachineLearning/comments/1g8a3pv/r_google_shopping_10m_dataset_for_large_scale/,Research
[P] A Visual Guide to Quantization,"Hi all! As more Large Language Models are being released and the need for quantization increases, I figured it was time to write an in-depth and visual guide to Quantization.  
  
From exploring how to represent values, (a)symmetric quantization, dynamic/static quantization, to post-training techniques (e.g., GPTQ and GGUF) and quantization-aware training (1.58-bit models with BitNet). 

[https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization)

With over 60 custom visuals, I went a little overboard but really wanted to include as many concepts as I possibly could! 

The visual nature of this guide allows for a focus on intuition, hopefully making all these techniques easily accessible to a wide audience, whether you are new to quantization or more experienced.",MachineLearning,152,9,1722256023.0,1eey89o,MaartenGr,https://www.reddit.com/r/MachineLearning/comments/1eey89o/p_a_visual_guide_to_quantization/,Project
[D] What do you do while your model is training?,"I am bascilly baby sitting my model while it is training, watch some *House M.D.* or play some minecraft. I have done all my literture review and paper writting, what should I do now while my model is training?",MachineLearning,151,88,1734243082.0,1hemhil,Striking-Warning9533,https://www.reddit.com/r/MachineLearning/comments/1hemhil/d_what_do_you_do_while_your_model_is_training/,Discussion
[D] Discussing Apple's Deployment of a 3 Billion Parameter AI Model on the iPhone 15 Pro - How Do They Do It?,"Hey everyone,

So, I've been working with running the Phi-3 mini locally, and honestly, it's been a bit of a ok . Despite all the tweaks and structured prompts in model files, it was normal, especially considering the laggy response times on a typical GPU setup. I was recently checking Apple's recent on -device model, they've got a nearly 3 billion parameter AI model running on an iPhone 15 Pro!

It's a forward in what's possible with AI on mobile devices. They’ve made up some tricks to make this work, and I just wanted to have discussion to dive into these with you all:

1. **Optimized Attention Mechanisms**: Apple has significantly reduced computational overhead by using a grouped-query-attention mechanism. This method batches queries, cutting down the necessary computations.
2. **Shared Vocabulary Embeddings**: Honestly I don't have much idea about this - I need to understand it more
3. **Quantization Techniques**: Adopting a mix of 2-bit and 4-bit quantization for model weights has effectively lowered both the memory footprint and power consumption.
4. **Efficient Memory Management**: dynamic loading of small, task-specific adapter are that can be loaded into the foundation model to specialize its functions without retraining the core parameters. These adapters are lightweight and used only when needed, flexibility and efficiency in memory use.
5. **Efficient Key-Value (KV) Cache Updates**: Even I don't know how this works
6. **Power and Latency Analysis Tools**: they were using tools like Talaria to analyze and optimize the model’s power consumption and latency in real-time. This allows them to make decisions about trade-offs between performance, power use, and speed, customizing bit rate selections for optimal operation under different conditions.: [Talaria demo video](https://mlr.cdn-apple.com/video/24_talaria_chi_video_preview_3c32bde921.mp4)
7. **Model Specialization via Adapters**: Instead of retraining the entire model, only specific adapter layers are trained for different tasks. maintaining high performance without the overhead of a full model retraining. Apple’s adapters let the AI switch gears on the fly for different tasks, all while keeping things light and fast.

For more detailed insights, check out Apple’s official documentation here: [Introducing Apple Foundation Models](https://machinelearning.apple.com/research/introducing-apple-foundation-models)

**Discussion Points**:

* How feasible is it to deploy such massive models on mobile devices?
* What are the implications of these techniques for future mobile applications?
* How do these strategies compare to those used in typical desktop GPU environments like my experience with Phi-3 mini?",MachineLearning,152,29,1718365836.0,1dfoykx,BriefAd4761,https://www.reddit.com/r/MachineLearning/comments/1dfoykx/d_discussing_apples_deployment_of_a_3_billion/,Discussion
[D] Rare skills of execptional ML Engineers,"Hello ML community!

Regardless the title you have(DS/Eng Manager/Eng Director/ML Eng ... ), what are the rare skills of ML Engineers in your workplace, that made them really stand out from the others (in both soft and hard skills areas)? If possible, please state your position - it could be potentially interesting how different roles sees this topic. 

Thanks!",MachineLearning,149,50,1720085370.0,1dv2thm,Avistian,https://www.reddit.com/r/MachineLearning/comments/1dv2thm/d_rare_skills_of_execptional_ml_engineers/,Discussion
[D] Why Gemma has such crazy big MLP hidden dim size?,,MachineLearning,150,18,1714978129.0,1clcluq,kiockete,https://i.redd.it/dgfe6rva6ryc1.png,Discussion
[D] Struggling to Transition to PhD,"**“Undergrad is about answering questions, while a PhD is about finding one.”** —Someone

I'm a first-year CS PhD student, but I feel stuck in the mindset of an undergrad. I excel at solving problems, as shown by my perfect GPA. However, when it comes to research, I struggle. If I enter a new area, **I typically read a lot of papers, take notes, and end up capable of writing a decent survey—but I rarely generate fresh ideas.**

Talking to other PhD students only adds to my frustration; one of them claims they can even come up with LLM ideas during a Latin class. My advisor says research is more about perseverance than talent, but I feel like I’m in a loop: I dive into a new field, produce a survey, and get stuck there.

I’m confident in my intelligence, but I’m questioning whether my workflow is flawed (e.g., maybe I should start experimenting earlier?) or if I’m just not cut out for research. Coming up with marginal improvements or applying A to B feels uninspiring, and I struggle to invest time in such ideas.

How do you CS (ML) PhD students come up with meaningful research ideas? Any advice on breaking out of this cycle?",MachineLearning,149,55,1732157451.0,1gw61tk,StraightSpeech9295,https://www.reddit.com/r/MachineLearning/comments/1gw61tk/d_struggling_to_transition_to_phd/,Discussion
What problems do Large Language Models (LLMs) actually solve very well? [D],"While there's growing skepticism about the AI hype cycle, particularly around chatbots and RAG systems, I'm interested in identifying specific problems where LLMs demonstrably outperform traditional methods in terms of accuracy, cost, or efficiency. Problems I can think of are:

\- words categorization

\- sentiment analysis of no-large body of text

\- image recognition (to some extent)

\- writing style transfer (to some extent)

what else?",MachineLearning,145,110,1730753568.0,1gjoxpi,Educational-String94,https://www.reddit.com/r/MachineLearning/comments/1gjoxpi/what_problems_do_large_language_models_llms/,Discussion
"[D] Llama-3 based OpenBioLLM-70B & 8B: Outperforms GPT-4, Gemini, Meditron-70B, Med-PaLM-1 & Med-PaLM-2 in Medical-domain","**Open Source Strikes Again**, We are thrilled to announce the release of OpenBioLLM-Llama3-70B & 8B. These models outperform industry giants like **Openai’s GPT-4, Google’s Gemini, Meditron-70B, Google’s Med-PaLM-1, and Med-PaLM-2** in the biomedical domain, setting a new state-of-the-art for models of their size. **The most capable openly available Medical-domain LLMs to date!** 🩺💊🧬



https://preview.redd.it/2h4ebhftf0xc1.png?width=2514&format=png&auto=webp&s=bbc3a583d45fb37b87a6fbbabe2d9e0f23c75d8b

🔥 OpenBioLLM-70B delivers SOTA performance, while the OpenBioLLM-8B model even surpasses GPT-3.5 and Meditron-70B!

The models underwent a rigorous two-phase fine-tuning process using the LLama-3 70B & 8B models as the base and leveraging Direct Preference Optimization (DPO) for optimal performance. 🧠



https://preview.redd.it/w41pv7mwf0xc1.png?width=5760&format=png&auto=webp&s=f3143919ef8472961f329bb8eb98937d8f8e41e0

**Results are available at Open Medical-L**LM Leaderboard: [https://huggingface.co/spaces/openlifescienceai/open\_medical\_llm\_leaderboard](https://huggingface.co/spaces/openlifescienceai/open_medical_llm_leaderboard)

Over \~4 months, we meticulously curated a diverse custom dataset, collaborating with medical experts to ensure the highest quality. The dataset spans 3k healthcare topics and 10+ medical subjects. 📚 OpenBioLLM-70B's remarkable performance is evident across 9 diverse biomedical datasets, achieving an impressive average score of 86.06% despite its smaller parameter count compared to GPT-4 & Med-PaLM. 📈



https://preview.redd.it/5ff2k9szf0xc1.png?width=5040&format=png&auto=webp&s=15dc4aa948f2608717f68ddf2cb27a6a2de03496

You can download the models directly from Huggingface today.

- 70B : [https://huggingface.co/aaditya/OpenBioLLM-Llama3-70B](https://huggingface.co/aaditya/OpenBioLLM-Llama3-70B)  
- 8B : [https://huggingface.co/aaditya/OpenBioLLM-Llama3-8B](https://huggingface.co/aaditya/OpenBioLLM-Llama3-8B)

This release is just the beginning! In the coming months, we'll introduce

* Expanded medical domain coverage,
* Longer context windows,
* Better benchmarks, and
* Multimodal capabilities.

More details can be found here: [https://twitter.com/aadityaura/status/1783662626901528803](https://twitter.com/aadityaura/status/1783662626901528803) Over the next few months, Multimodal will be made available for various medical and legal benchmarks.

I hope it's useful in your research 🔬 Have a wonderful weekend, everyone! 😊",MachineLearning,148,11,1714218702.0,1cecpvk,aadityaura,https://www.reddit.com/r/MachineLearning/comments/1cecpvk/d_llama3_based_openbiollm70b_8b_outperforms_gpt4/,Discussion
[D] I don't see a point in rebuttals anymore.,"This is a mixture of some contemplation and some rant but per the title, I just don't see a point in it. I recently got back results from a conference where I had two positive reviews and one negative. Then wrote a really nice rebuttal that addressed a fundamental misunderstanding of the reviewer (who, later, did increase their points so I guess the rebuttal was on mark?). But turns out, the meta-reviewer latched on to the negative review, didn't even read the rebuttal that addressed said review and rejected the paper.

What was even the point of me rebutting if concerned parties are \_not even going to read them\_? At this point, I am tempted to treat the rebuttal phase as an exercise in futility. Maybe I should withdraw papers in the first phase come any problems instead of trying to go through the agony of an ultimately meaningless labor.",MachineLearning,147,38,1734661121.0,1hi9jt2,pddpro,https://www.reddit.com/r/MachineLearning/comments/1hi9jt2/d_i_dont_see_a_point_in_rebuttals_anymore/,Discussion
[R] Open-TeleVision: Teleoperation with Immersive Active Visual Feedback,,MachineLearning,146,9,1720393793.0,1dxtsiq,XiaolongWang,https://i.redd.it/kls4r6b0i6bd1.gif,Research
"[D] Something I always think about, for top conferences like ICML, NeurIPS, CVPR,..etc. How many papers are really groundbreaking?
","I have some papers in top venus myself, but whenever I sit down and be brutually honest with myself. I feel my work is good but it is just not that impactful, like one more brick in the wall.
I wonder how often we can see something as impactful as ""Attention is all you need"" for example.",MachineLearning,146,36,1714675069.0,1cin6s8,oddhvdfscuyg,https://www.reddit.com/r/MachineLearning/comments/1cin6s8/d_something_i_always_think_about_for_top/,Discussion
[D] The popular theoretical explanation for VAE is inconsistent. Please change my mind.,"I had a really hard time understanding VAE / variational inference (VI) in theory, for years. I'd be really appreciated if anyone could clarify my confusions. Here's what I've got after reading many sources:

1. We want to establish a generative model p(x, z) (parameters are omitted for simplicity) for the observable variable x and the latent variable z. Alright, let's select appropriate parameters to maximize the marginal likelihood of the observed samples p(x).
2. According to basic probability theory (the law of total probability and the definition of conditional probability), we have: p(x)=∫ p(x ∣ z) p(z) dz (Eq. 1).
3. Here's the point that things becomes rather confusing: people now will claim that this integral is ***intractable*** because z is a continuous variable / z is a high-dimensional variable / p(x∣z) is too complex / or any other excuses.
4. What to do for the intractability of Eq. 1? Although we didn't mention the posterior p(z ∣ x) above, we will now bring it into the discussion. The posterior p(z ∣ x) is also intractable since p(z | x) = p(x | z) p(z) / p(x) and p(x) is intractable. So we will introduce another parameterized model q(z ∣ x) to approximate p(z | x).
5. After some derivation, we obtain a new optimization objective, commonly known as ELBO, which is the summation of:
    - the ""reconstruction"" term: ∫ log p(x ∣ z) q(z ∣ x) dz (Eq. 2);
    - KL divergence term between q(z | x) and p(z), which results in a closed-form.
6. So now we have to work on Eq. 2. Compared with Eq. 1, p(z) is replaced with q(z∣x), both of them are (usually) normal distributions, and p(x | z) is still there. Great! Clearly we have transformed an intractable integral into… another intractable integral?
7. Don’t worry, we can compute Eq. 2 using Monte Carlo sampling… Wait, since we can use Monte Carlo for this, why can’t we just handle Eq. 1 the same way without so much fuss?
8. Of course it is not a good idea. It can be shown that log p(x) = ELBO + D_KL(q(z ∣ x) || p(z ∣ x)). So we cannot estimate p(x) with Eq. 1 as it does not have such nice properties… Huh, it seems like that’s not how we started explaining this?

Questions:

1. When tackling the original problem, i.e., modeling p(x, z) by maximizing p(x)=∫ p(x ∣ z) p(z) dz, why do we want to involve the posterior p(z | x)?
    - Someone explains this with [""to narrow down the value space to facilitate faster search""](https://web.archive.org/web/20241202042731/https://lilianweng.github.io/posts/2018-08-12-vae) (with the approximation of p(z | x), q(z | x)). But again, please recall how the intractability of Eq. 1 is explained, I can't see anything improved under this argument.
2. The Eq. 1 and Eq. 2 are essentially similar, where either of them is the expectation of (log) p(z | x) with respect to the probability density function of some normal distribution. I can't see how the motivation based on the intractability of Eq. 1 could make sense.
    - Ironically, we still have to resort to Monte Carlo sampling when handling Eq. 2. But people appear to forget it when talking about the intractability of Eq. 1, but remember it when facing the same problem of Eq. 2.

Update: I have editted some typo.

Update 2: Question 2 seems to be resolved after some discussions: 
- It is not a good idea to sample on p(z) due to the high variance.
- In practice, we are usually working on log p(x), the log-likelihood of samples, and MC sampling for log ∫ p(x ∣ z) p(z) dz (Eq. 3) can be biased. 
- Apply Jensen's inequality on Eq. 3 and we will have log p(x) ≥ ∫ log p(x ∣ z) p(z) dz. This bound is very likely worse than ELBO, and still relying on sampling on p(z).

However, these points are still rarely found in existing articles. I hope we may think more carefully when introducing VAE in the future.",MachineLearning,143,68,1733199919.0,1h5f6co,function2,https://www.reddit.com/r/MachineLearning/comments/1h5f6co/d_the_popular_theoretical_explanation_for_vae_is/,Discussion
[R] What’s Really Going On in Machine Learning? Some Minimal Models (Stephen Wolfram),"A recent blog post by Stephen Wolfram with some interesting views about discrete neural nets, looking at the training from the perspective of automata:

https://writings.stephenwolfram.com/2024/08/whats-really-going-on-in-machine-learning-some-minimal-models/",MachineLearning,141,43,1724593096.0,1f0wj6s,hardmaru,https://www.reddit.com/r/MachineLearning/comments/1f0wj6s/r_whats_really_going_on_in_machine_learning_some/,Research
[D]LLM interview Q&A,"Hey guys! I'm a data scientist at Amazon Web Services (China). In the past year, I have interviewed for LLM positions at many companies. And I'm planning to compile a series of interview questions, drawing from my own experience in interviews, and provide what I consider to be the right answers. This article will focus on **fine-tuning,** and I'll keep it updated.",MachineLearning,144,7,1717436429.0,1d7af78,mlzoo,https://www.reddit.com/r/MachineLearning/comments/1d7af78/dllm_interview_qa/,Discussion
[D] Do you get to exercise your ML skills often at your job?,"I was hired original as an ML engineer/scientist a few years ago. And for the most part my day to day reflected that. But with the boom of LLMs my team seems to solely focus on using a lot of this tech ""out of the box"", including agentic wrappers. My work has been dumbed down to prompt engineering to force a huge general purpose model into our domain specific use case. The results are acceptable for the most part, not going to lie, but there's still a small proportion of the cases where a fine-tuned model would have won. The leadership does not seem to be interested in fine-tuning or coming up with something original. A lot of the wrappers especially are very raw and force you into the usage of specific patterns and models. But because they are considered ""out of the box"", that's what's pushed on us to use. I feel like we are trying to fit a cube into a round hole.",MachineLearning,145,34,1730992931.0,1glswpx,Tiger00012,https://www.reddit.com/r/MachineLearning/comments/1glswpx/d_do_you_get_to_exercise_your_ml_skills_often_at/,Discussion
[D] HuggingFace transformers - Bad Design?,"Hi,

I am currently working with HuggingFace's transformers library. The library is somewhat convenient to load models and it seems to be the only reasonable platform for sharing and loading models. But the deeper I go, the more difficulties arise and I got the impression that the api is not well designed and suffers a lot of serious problems.

The library allows for setting the same options at various places, and it is not documented how they interplay. For instance, it seems there is no uniform way to handle special tokens such as EOS. One can set these tokens 1. in the model, 2. in the tokenizer, and 3. in the pipeline. It is unclear to me how exactly these options interplay, and also the documentation does not say anything about it. Sometimes parameters are just ignored, and the library does not warn you about it. For instance, the parameter ""add\_eos\_token"" of the tokenizer seems to have no effect in some cases, and I am not the only one with this issue (https://github.com/huggingface/transformers/issues/30947). Even worse is that it seems the exact behavior often depends on the model, while the library pretends to provide a uniform interface. A look into the sourcecode confirms that they actually distingish depending on the currently loaded model.

Very similar observations concern the startup scripts for multi-threading, in particular: accelerate. I specify the number of cores, but this is just ignored. Without notification, without any obvious reason. I see in the system monitor that it still runs single-threaded. Even the samples taken from the website do not always work.

In summary, there seems to be an uncontrolled growth of configuration settings. Without a clear structure and so many effects influencing the library that large parts of its behavior are in fact undocumented. One could also say, it looks a bit unstable and experimental. Even the parts that work for me worry me as I have doubts if everything will work on another machine after deployment.

Anyone having thoughts like this?",MachineLearning,144,57,1723851030.0,1eu3auv,duffano,https://www.reddit.com/r/MachineLearning/comments/1eu3auv/d_huggingface_transformers_bad_design/,Discussion
[D] How reliable is RAG currently?,"At it's essence I guess RAG is about

1. retrieving relevant documents based on the prompt
2. putting the documents into the context window

Number 2 is very straight forward, while number 1 is where I guess more of the important stuff happens. IIRC, most often we do a similarity search here between the prompt embedding and the document embeddings, and retrieve the k-most similar documents.

Ok, at this point we have k documents and put them into context. Now it's time for the LLM to give me an answer based on my prompt and the k documents, which a good LLM should be able to do given that the correct documents were retrieved.

I tried doing some hobby projects with LlamaIndex but didn't get it to work so nicely. For example, I tried with NFL statistics as my data (one row per player, one column per feature) and hoped that GPT-4 together with these documents would be able to answer atleast 95% of my question correctly, but it was more like 70% which was surprisingly bad since I feel like this was a fairly basic project. Questions were of the kind ""how many touchdowns did player x do in season y"". Answers varied from being correct, to saying the information wasn't available, to hallucinating an incorrect answer.

Hopefully I'm just doing something in suboptimal way, but it got me thinking of how widely used RAG is in production around the world. What are some applications on the market that successfully utilizes RAG? I assume something like [perplexity.ai](https://perplexity.ai) is using it, and of course all other chatbots that uses browsing in some way. An obvious application mentioned is often embedding your company documents, and then having an internal chatbot that uses RAG. Is that deployed anywhere? Not at my company, but I could see it being useful.

Basically, is RAG mostly something that sounds good in theory and is currently hyped or is it actually something that is used in production around the world?",MachineLearning,143,98,1714830753.0,1ck0tnk,lapurita,https://www.reddit.com/r/MachineLearning/comments/1ck0tnk/d_how_reliable_is_rag_currently/,Discussion
[Discussion] What resource do you use to keep up to date on ML research? ,"In my day job, I work on recommender and search systems, and I find it hard to keep current on the latest developments relating to my work. I can find time to read maybe one new paper a week (unless it’s directly needed for my work) but disentangling the signal from the noise is the hard part. I’m curious how everyone else choose and find the relevant papers, blog posts, or articles to read for your specific domain? ",MachineLearning,142,42,1727841361.0,1fu7gls,PurpleAnnieOwl,https://www.reddit.com/r/MachineLearning/comments/1fu7gls/discussion_what_resource_do_you_use_to_keep_up_to/,Discussion
[R] Fine-Tuning 175B Parameter Language Models on a Single Consumer GPU through Optimized Memory Management,"The key technical advance here is enabling fine-tuning of 100B parameter models on a single consumer GPU through clever memory management and NVMe SSD utilization. The researchers developed a framework that optimizes data movement between GPU, CPU RAM, and storage while maintaining training quality.

Main technical contributions:
- Implementation of modified ZeRO-Infinity optimization for consumer hardware
- Three-tier memory hierarchy with dynamic parameter offloading
- Novel prefetching system that reduces memory access latency
- Optimization of data transfer patterns between storage tiers
- Memory bandwidth management across GPU/CPU/NVMe

Key results:
- 2.6x speedup compared to existing single-GPU methods
- 70% reduction in required GPU memory
- Successful fine-tuning of 100B parameter models
- Comparable training quality to multi-GPU setups
- Verified on consumer hardware configurations

I think this could make large model fine-tuning much more accessible to individual researchers and smaller labs. While it won't replace multi-GPU training for production scenarios, it enables rapid prototyping and experimentation without requiring expensive hardware clusters. The techniques here could also inform future work on memory-efficient training methods.

The trade-offs seem reasonable - slower training in exchange for massive cost reduction. However, I'd like to see more extensive testing across different model architectures and training tasks to fully validate the approach.

TLDR: New framework enables fine-tuning 100B parameter models on single consumer GPUs through optimized memory management and NVMe utilization, achieving 2.6x speedup over existing methods.

[Full summary is here](https://aimodels.fyi/papers/arxiv/lohan-low-cost-high-performance-framework-to). Paper [here](https://arxiv.org/abs/2403.06504).",MachineLearning,140,10,1735223105.0,1hmpck4,Successful-Western27,https://www.reddit.com/r/MachineLearning/comments/1hmpck4/r_finetuning_175b_parameter_language_models_on_a/,Research
[Discussion] Non compute hungry research publications that you really liked in the recent years?,"There are several pieces of fantastic works happening all across the industry and academia. But greater the hype around a work more resource/compute heavy it generally is. 

What about some works done in academia/industry/independently by a small group (or single author) that is really fundamental or impactful, yet required very little compute (a single or double GPU or sometimes even CPU)? 

Which works do you have in mind and why do you think they stand out? ",MachineLearning,137,17,1722321800.0,1efmmnn,HopeIsGold,https://www.reddit.com/r/MachineLearning/comments/1efmmnn/discussion_non_compute_hungry_research/,Discussion
"[P] [D] Hi I'm a senior machine learning engineer, looking for for buddies to build cool stuff with!","Hi, I'm a senior machine learning engineer, looking for buddies to build cool stuff with! I'm looking to explore and experiment with fellow passionate engineers. We can do Kaggle projects, LeetCode, or just interview brainstorming. Reach out if anyone would like to ideate and see what cool things we can create together.

**UPDATE: Thank you for the overwhelming response! I've received over 100 responses, and I appreciate your interest and willingness to contribute.**

**To ensure that I can effectively manage all the responses and filter potential serious candidates, I'll be creating a Google Form soon.**
Form: https://forms.gle/k3jzCfNJy3rgz4ec6

**Please bear with me as I work on setting this up. In the meantime, if you have any ideas or suggestions, please share them.**",MachineLearning,135,90,1718765122.0,1dj8pg6,Rude-Eye3588,https://www.reddit.com/r/MachineLearning/comments/1dj8pg6/p_d_hi_im_a_senior_machine_learning_engineer/,Discussion
[R] YOLOv10: Real-Time End-to-End Object Detection,"**Paper:** [https://arxiv.org/abs/2405.14458](https://arxiv.org/abs/2405.14458)

**Abstract:** Over the past years, YOLOs have emerged as the predominant paradigm in the field of real-time object detection owing to their effective balance between computational cost and detection performance. Researchers have explored the architectural designs, optimization objectives, data augmentation strategies, and others for YOLOs, achieving notable progress. However, the reliance on the non-maximum suppression (NMS) for post-processing hampers the end-to-end deployment of YOLOs and adversely impacts the inference latency. Besides, the design of various components in YOLOs lacks the comprehensive and thorough inspection, resulting in noticeable computational redundancy and limiting the model's capability. It renders the suboptimal efficiency, along with considerable potential for performance improvements. In this work, we aim to further advance the performance-efficiency boundary of YOLOs from both the post-processing and model architecture. To this end, we first present the consistent dual assignments for NMS-free training of YOLOs, which brings competitive performance and low inference latency simultaneously. Moreover, we introduce the holistic efficiency-accuracy driven model design strategy for YOLOs. We comprehensively optimize various components of YOLOs from both efficiency and accuracy perspectives, which greatly reduces the computational overhead and enhances the capability. The outcome of our effort is a new generation of YOLO series for real-time end-to-end object detection, dubbed YOLOv10. Extensive experiments show that YOLOv10 achieves state-of-the-art performance and efficiency across various model scales. For example, our YOLOv10-S is 1.8× faster than RT-DETR-R18 under the similar AP on COCO, meanwhile enjoying 2.8× smaller number of parameters and FLOPs. Compared with YOLOv9-C, YOLOv10-B has 46\\% less latency and 25\\% fewer parameters for the same performance.

**Visual Summary:**

[Method](https://preview.redd.it/sf69ll17nj2d1.png?width=970&format=png&auto=webp&s=242bc2565ea53526b4c0c5933606788de9fde799)

[Benchmarking](https://preview.redd.it/sllekaz9nj2d1.png?width=1229&format=png&auto=webp&s=6169c929fd635fbf3b442e3c4f3d529b9ddea334)

**Code:** [https://github.com/THU-MIG/yolov10](https://github.com/THU-MIG/yolov10)

",MachineLearning,140,15,1716630487.0,1d08hzz,StartledWatermelon,https://www.reddit.com/r/MachineLearning/comments/1d08hzz/r_yolov10_realtime_endtoend_object_detection/,Research
"[R] I ran 580 model-dataset experiments to show that, even if you try very hard, it is almost impossible to know that a model is degrading just by looking at data drift results","In my opinion, data drift detection methods are very useful when we want to understand what went wrong with a model, but they are not the right tools to know how my model's performance is doing. 

Essentially, using data drift as a proxy for performance monitoring is not a great idea.

  
I wanted to prove that by giving data drift methods a second chance and trying to get the most out of them. I built a technique that relies on drift signals to estimate model performance and compared its results against the current SoTA performance estimation methods ([PAPE \[arxiv link\]](https://arxiv.org/abs/2401.08348) and [CBPE \[docs link\]](https://nannyml.readthedocs.io/en/stable/how_it_works/performance_estimation.html#confidence-based-performance-estimation-cbpe)) to see which technique performs best.

  
To effectively compare data drift signals against performance estimation methods, I used an evaluation framework that emulates a typical production ML model and ran multiple dataset-model experiments.

As per data, I used datasets from the [Folktables package](https://github.com/socialfoundations/folktables). (Folktables preprocesses US census data to create a set of binary classification problems.) To make sure the results are not biased, in terms of the nature of the model, I trained different types of models (Linear, Ensemble Boosting) for multiple prediction tasks included in Folktables.

Then, I built a technique that relies on drift signals to estimate model performance. This method uses univariate and multivariate data drift information as features of a DriftSignal model to estimate the performance of the model we monitor. It works as follows:

1. Fit univariate/multivariate drift detection calculator on reference data (test set).

1. Take the fitted calculators to measure the observed drift in the production set. For univariate drift detection methods, we use Jensen Shannon, Kolmogorov-Smirnov, and Chi2 distance metrics/tests. Meanwhile, we use the [PCA Reconstruction Error](https://nannyml.readthedocs.io/en/stable/how_it_works/multivariate_drift.html#data-reconstruction-with-pca) and [Domain Classifier](https://nannyml.readthedocs.io/en/stable/how_it_works/multivariate_drift.html#domain-classifier) for multivariate methods.

1. Build a DriftSignal model that trains a regression algorithm using the drift results from the reference period as features and the monitored model performance as a target.

1. Estimate the performance of the monitored model on the production set using the trained DriftSignal model.

You can find the full implementation of this method in this [GitHub Gist](https://gist.github.com/santiviquez/aa224c6e232c8bd2534893888981564d).

  
Then, for evaluation, I used a modified version of MAE because I needed an aggregated version that take into consideration the standard deviation of the errors. To account for this, I scale absolute/squared errors by the standard error (SE) calculated for each evaluation case. We call the SE-scaled metrics **mean absolute standard error (MASTE)**.

[MASTE formula](https://preview.redd.it/7jnk40il2l3d1.png?width=858&format=png&auto=webp&s=19679d6a202b2175f75c6f3252430792682090ad)

  
Then it was a matter of running all the 580 experiments and collect results.

Since, each performance estimation method is trying to estimate the roc\_auc of the monitored model, I report the MASTE between the estimated and realized roc\_auc.

https://preview.redd.it/z0oviz763l3d1.png?width=1404&format=png&auto=webp&s=c0e4838dcadbf664ff59570997f46612002c7e6e

PAPE seems to be the most accurate method, followed by CBPE. Surprisingly, constant test set performance is the third best. This is closely followed by random forest versions of univariate and multivariate drift signal models.

  
This plot shows the quality of performance estimation among different methods, including PAPE and CBPE.

[Quality of performance estimation \(MASTE of roc\_auc\) vs absolute performance change \(SE\). \(The lower, the better\).](https://preview.redd.it/3ar6plzbyk3d1.jpg?width=1668&format=pjpg&auto=webp&s=c429a6c35daf887fff021bd4f9894a15caac7a57)

  
Here is a specific time series plot of a model's realized ROC AUC (black) compared against all the performance estimation methods. PAPE (red) accurately estimates the direction of the most significant performance change and closely approximates the magnitude.

[Time series plot of realized vs estimated roc\_auc for dataset ACSIncome \(California\) and LigthGBM model.](https://preview.redd.it/m6igkfmk3l3d1.png?width=1436&format=png&auto=webp&s=6146b84f4999b70fa618ade15084dda8fea2acc8)

The experiments suggest that there are better tools for detecting performance degradation than data drift, even though I tried my best to extract all the meaningful information from drift signals to create an accurate performance estimation method.

There are better tools for quantifying the impact of data drift on model performance. So, I hope this helps the industry realize that monitoring fine-grained metrics leads to nothing and that a change in an obscure feature might not mean anything. It is better to first estimate model performance and then, if it drops, review data drift results but not the other way around.

Full experiment set up, datasets, models, benchmarking methods, and the code used in the project can be found in this [longer post](https://www.nannyml.com/blog/data-drift-estimate-model-performance) that I wrote yesterday.",MachineLearning,133,7,1717084461.0,1d47ca4,santiviquez,https://www.reddit.com/r/MachineLearning/comments/1d47ca4/r_i_ran_580_modeldataset_experiments_to_show_that/,Research
[R] Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality,,MachineLearning,135,25,1717435821.0,1d7a6l6,floppy_llama,https://arxiv.org/pdf/2405.21060,Research
[R] No More Adam: Learning Rate Scaling at Initialization is All You Need,,MachineLearning,135,36,1734733711.0,1hivid1,RobbinDeBank,https://arxiv.org/pdf/2412.11768,Research
[D] Quality of ICLR papers,"I was going through some of the papers of ICLR with moderate to high scores related to what I was interested in , I found them failrly incremental and was kind of surprised, for a major sub field, the quality of work was rather poor for a premier conference as this one . Ever since llms have come, i feel the quality and originality of papers (not all of course ) have dipped a bit. Am I alone in feeling this ?",MachineLearning,137,74,1731866544.0,1gtjhge,Cool_Abbreviations_9,https://www.reddit.com/r/MachineLearning/comments/1gtjhge/d_quality_of_iclr_papers/,Discussion
[D] Normalization in Transformers,"Why isn't BatchNorm used in transformers, and why is LayerNorm preferred instead? Additionally, why do current state-of-the-art transformer models use RMSNorm? I've typically observed that LayerNorm is used in language models, while BatchNorm is common in CNNs for vision tasks. However, why do vision-based transformer models still use LayerNorm or RMSNorm rather than BatchNorm?

",MachineLearning,133,34,1723963936.0,1ev32c0,Collegesniffer,https://www.reddit.com/r/MachineLearning/comments/1ev32c0/d_normalization_in_transformers/,Discussion
[D] How are subspace embeddings different from basic dimensionality reduction?,"I have been struggling to understand how more basic dimensionality reduction techniques differ from more advanced methods, mainly in whether the same intuition about subspaces, manifolds, etc. extends to the more basic methods. I understand how things like PCA, t-SNE, UMAP, etc etc work (and these are 90% of what comes up when looking for dimensionality dimensionality reduction), but when I read about subspace clustering, manifold learning, or things in this area, they rarely mention these more basic dim reduc techniques and instead opt for more advanced methods and I'm not sure why, especially given how prolific PCA, t-SNE, and UMAP seem to be.

It is unclear to me whether/how things like PCA are different from say manifold learning, particularly in their usefulness for subspace clustering. I think the goals of both are to find some latent structure, with the intuition that working in the latent space will reduce noise, useless / low info features, reduce the curse of dimensionality, and also potentially more clearly show how the features and labels are connected in the latent space. In terms of the actual algorithms, I am understand the intuition but not whether they are ""real"". For instance, in the case of manifold learning (which, FWIW, I don't really see any papers about anymore and don't know why this is), a common example is the ""face manifold"" for images, that is a smooth surface of lower dims than the original input dimensions, and smoothly transitions from every face to another. This may be a little more trivial for images, but for general time series data, does this same intuition extend? 

For instance, if I have a dataset of time series caterpillar movement, can I arbitrarily say that there exists a manifold of catepillar size (bigger catepillars move slower) or a manifold of caterpillar ability (say, some kind of ability/skill manifold, if the caterpillars are completing a task/maze)? Very contrived example, but basically the question is if it is necessarily the case that I should be able to find a latent space based on what my priors tell me should exist / may hold latent structure (given enough data)?

I know Yann LeCun is a big proponent of working in latent spaces (more so with joint embeddings, which I am not sure whether that is applicable to me and my time series data), so I am trying to take my work more in that direction, but it seems like there's a big divide between basic PCA and basic nonlinear techniques (eg the ones you would see built into scipy or sklearn or whatever) and techniques that are used in some other papers. Do PCA (or basic nonlinear methods) and the like achieve the same thing but just not as well?",MachineLearning,129,13,1715960669.0,1cu8is4,Amun-Aion,https://www.reddit.com/r/MachineLearning/comments/1cu8is4/d_how_are_subspace_embeddings_different_from/,Discussion
[R] Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss,"*abstract*

Contrastive loss is a powerful approach for representation learning, where larger batch sizes enhance performance by providing more negative samples to better distinguish between similar and dissimilar data. However, scaling batch sizes is constrained by the quadratic growth in GPU memory consumption, primarily due to the full instantiation of the similarity matrix. To address this, we propose a tile-based computation strategy that partitions the contrastive loss calculation into arbitrary small blocks, avoiding full materialization of the similarity matrix. Furthermore, we introduce a multi-level tiling strategy to leverage the hierarchical structure of distributed systems, employing ring-based communication at the GPU level to optimize synchronization and fused kernels at the CUDA core level to reduce I/O overhead. Experimental results show that the proposed method scales batch sizes to unprecedented levels. For instance, it enables contrastive training of a CLIP-ViT-L/14 model with a batch size of 4M or 12M using 8 or 32 A800 80GB without sacrificing any accuracy. Compared to SOTA memory-efficient solutions, it achieves a two-order-of-magnitude reduction in memory while maintaining comparable speed. The code will be made publicly available.",MachineLearning,126,23,1729865788.0,1gbvapp,RajonRondoIsTurtle,https://arxiv.org/abs/2410.17243,Research
[R] What are the Top 3 most exciting research directions for you currently?,Let's share! What are you excited about?,MachineLearning,126,62,1727165101.0,1fo7ben,Prestigious_Bed5080,https://www.reddit.com/r/MachineLearning/comments/1fo7ben/r_what_are_the_top_3_most_exciting_research/,Research
[D] Academic ML Labs: How many GPUS ?,"Following a [recent post](https://www.reddit.com/r/singularity/comments/1coq6tn/ai_godmother_standfords_nlp_lab_has_only_64_gpus/), I was wondering how other labs are doing in this regard.

During my PhD (top-5 program),  compute was a major bottleneck  (it could be significantly shorter if we had more high-capacity GPUs). We currently have \*no\* H100.

How many GPUs does your lab have? Are you getting extra compute credits from Amazon/ NVIDIA through hardware grants?

  
thanks

",MachineLearning,126,136,1719052154.0,1dlsogx,South-Conference-395,https://www.reddit.com/r/MachineLearning/comments/1dlsogx/d_academic_ml_labs_how_many_gpus/,Discussion
[R] nGPT: Normalized Transformer with Representation Learning on the Hypersphere,"**Paper:** [https://arxiv.org/pdf/2410.01131](https://arxiv.org/pdf/2410.01131)

**Abstract:**

>We propose a novel neural network architecture, the normalized Transformer (nGPT) with representation learning on the hypersphere. In nGPT, all vectors forming the embeddings, MLP, attention matrices and hidden states are unit norm normalized. The input stream of tokens travels on the surface of a hypersphere, with each layer contributing a displacement towards the target output predictions. These displacements are defined by the MLP and attention blocks, whose vector components also reside on the same hypersphere. Experiments show that nGPT learns much faster, reducing the number of training steps required to achieve the same accuracy by a factor of 4 to 20, depending on the sequence length.

**Highlights:**

>Our key contributions are as follows:   
  
*Optimization of network parameters on the hypersphere* We propose to normalize all vectors forming the embedding dimensions of network matrices to lie on a unit norm hypersphere. This allows us to view matrix-vector multiplications as dot products representing cosine similarities bounded in \[-1,1\]. The normalization renders weight decay unnecessary.   
  
*Normalized Transformer as a variable-metric optimizer on the hypersphere* The normalized Transformer itself performs a multi-step optimization (two steps per layer) on a hypersphere, where each step of the attention and MLP updates is controlled by eigen learning rates—the diagonal elements of a learnable variable-metric matrix. For each token t\_i in the input sequence, the optimization path of the normalized Transformer begins at a point on the hypersphere corresponding to its input embedding vector and moves to a point on the hypersphere that best predicts the embedding vector of the next token t\_i+1 .   
  
*Faster convergence* We demonstrate that the normalized Transformer reduces the number of training steps required to achieve the same accuracy by a factor of 4 to 20.

**Visual Highlights:**

https://preview.redd.it/0jdj23ew6ytd1.png?width=1313&format=png&auto=webp&s=144f4fa881d05bd1bc90faa2a0bb2c74e58c71df

[Not sure about the difference between 20k and 200k budgets; probably the best result from runs with different initial learning rates is plotted](https://preview.redd.it/8tf5tw0x6ytd1.png?width=1187&format=png&auto=webp&s=4f9dfbe1f49bdc8aed6fa953dc9220556d7dc947)

https://preview.redd.it/waof2llr7ytd1.png?width=1337&format=png&auto=webp&s=3f82cee29c5fe753e219edf55ab16460fcf9a11a

https://preview.redd.it/a5vburms7ytd1.png?width=859&format=png&auto=webp&s=a3f34b73a580a5798bd5e10e9a4cc950b93fa691

",MachineLearning,123,57,1728574647.0,1g0lnij,StartledWatermelon,https://www.reddit.com/r/MachineLearning/comments/1g0lnij/r_ngpt_normalized_transformer_with_representation/,Research
[Discussion] Papers with fake NOVEL APPROACH in ML and DL models,"

why are a lots of the new papers ( usually done by PhDs ) with an existing approach and when u ask about their contribution they said we replace this layer by an other or we add a hyperparametters !!!!!

this is not a contribution ! i confused how can these got accepted",MachineLearning,126,67,1731257597.0,1go50wf,Rihab_Mira,https://www.reddit.com/r/MachineLearning/comments/1go50wf/discussion_papers_with_fake_novel_approach_in_ml/,Discussion
"[R] Trying to classify Blueberries as ""Crunchy"", ""Juicy"" or ""Soft"" using Acoustic Signal Processing and Machine Learning
","I'm working on on this research to classify blueberries based on their texture—specifically, whether they are soft, juicy, or crunchy—using the sounds they produce when crushed.  
I have about 1100 audio samples, and I've generated spectrograms for each sample. Unfortunately, **I don't have labeled data**, so I can't directly apply supervised machine learning techniques. Instead, I'm looking for effective ways to differentiate between these three categories based on the spectrograms. I've attached examples of spectrograms for what I believe might be soft, juicy, and crunchy blueberries. However, since the data isn't labeled, I'm unsure if these assumptions are correct.

**Crunchy Berries:** When crushed, they produce separate, distinct peaks in the audio signal. These peaks are spaced out over time, indicating that the berry is breaking apart in a crisp, segmented manner.

[crunchyberry](https://preview.redd.it/k07r0y5xygid1.jpg?width=1600&format=pjpg&auto=webp&s=d9c43c36365d9d5517d956319559cb1b3ce58031)

**Juicy Berries:** When crushed, they generate continuous peaks in the audio signal. These peaks are more closely packed together and sustained, indicating a burst of juice and flesh, with less resistance, creating a smoother sound.

[juicyberry](https://preview.redd.it/2t2i50ezygid1.jpg?width=1600&format=pjpg&auto=webp&s=4f5a96da39356c1656c956b2de7c8208e76e43ab)

**Soft Berries:** These produce very few and small peaks. The sound is faint and less defined, indicating that the berry crushes easily with little resistance, creating minimal disruption in the audio signal.

[softberry](https://preview.redd.it/vidfksc2zgid1.jpg?width=1600&format=pjpg&auto=webp&s=50d4aa4b1b599e615edfe7435fac39e97353c7d0)

  
**What I Tried:**

I attempted to classify the blueberries by detecting peaks within a specific timeframe of the audio signal. This method allowed me to differentiate between **soft** and **crunchy** berries effectively, as soft berries produce fewer and smaller peaks, while crunchy berries have distinct, separated peaks.

**What I Expected:**

I expected this peak detection approach to also help classify **juicy** berries, as I anticipated continuous, higher amplitude peaks that would be distinct from the other categories.

**What Actually Happened:**

While the method worked well for soft and crunchy berries, it did not successfully differentiate the **juicy** berries. The continuous nature of the juicy berry peaks did not stand out as much as I expected, making it difficult to classify them accurately.

  
Can anyone help me out with some ideas to solve this problem? If you want we can work on this together and write a research paper or an article in journal.",MachineLearning,124,36,1723571674.0,1erehp8,whiterosephoenix,https://www.reddit.com/r/MachineLearning/comments/1erehp8/r_trying_to_classify_blueberries_as_crunchy_juicy/,Research
[P] llama.ttf: A font which is also an LLM,,MachineLearning,124,15,1719144644.0,1dmkqrv,pred,https://fuglede.github.io/llama.ttf/,Project
"[R] Announcing the first series of Liquid Foundation Models (LFMs) – a new generation of generative AI models that achieve state-of-the-art performance at every scale, while maintaining a smaller memory footprint and more efficient inference.","https://www.liquid.ai/liquid-foundation-models

https://www.liquid.ai/blog/liquid-neural-networks-research

https://x.com/LiquidAI_/status/1840768716784697688

https://x.com/teortaxesTex/status/1840897331773755476

""We announce the first series of Liquid Foundation Models (LFMs), a new generation of generative AI models built from first principles.

Our 1B, 3B, and 40B LFMs achieve state-of-the-art performance in terms of quality at each scale, while maintaining a smaller memory footprint and more efficient inference.""

""LFM-1B performs well on public benchmarks in the 1B category, making it the new state-of-the-art model at this size. This is the first time a non-GPT architecture significantly outperforms transformer-based models.

LFM-3B delivers incredible performance for its size. It positions itself as first place among 3B parameter transformers, hybrids, and RNN models, but also outperforms the previous generation of 7B and 13B models. It is also on par with Phi-3.5-mini on multiple benchmarks, while being 18.4% smaller. LFM-3B is the ideal choice for mobile and other edge text-based applications.

LFM-40B offers a new balance between model size and output quality. It leverages 12B activated parameters at use. Its performance is comparable to models larger than itself, while its MoE architecture enables higher throughput and deployment on more cost-effective hardware.

LFMs are large neural networks built with computational units deeply rooted in the theory of dynamical systems, signal processing, and numerical linear algebra.

LFMs are Memory efficient LFMs have a reduced memory footprint compared to transformer architectures. This is particularly true for long inputs, where the KV cache in transformer-based LLMs grows linearly with sequence length.

LFMs truly exploit their context length: In this preview release, we have optimized our models to deliver a best-in-class 32k token context length, pushing the boundaries of efficiency for our size. This was confirmed by the RULER benchmark.

LFMs advance the Pareto frontier of large AI models via new algorithmic advances we designed at Liquid: 
 
Algorithms to enhance knowledge capacity, multi-step reasoning, and long-context recall in models + algorithms for efficient training and inference.

We built the foundations of a new design space for computational units, enabling customization to different modalities and hardware requirements.

What Language LFMs are good at today:
General and expert knowledge,
Mathematics and logical reasoning,
Efficient and effective long-context tasks,
A primary language of English, with secondary multilingual capabilities in Spanish, French, German, Chinese, Arabic, Japanese, and Korean.

What Language LFMs are not good at today:
Zero-shot code tasks,
Precise numerical calculations,
Time-sensitive information,
Counting r’s in the word “Strawberry”!,
Human preference optimization techniques have not yet been applied to our models, extensively.""

""We invented liquid neural networks, a class of brain-inspired systems that can stay adaptable and robust to changes even after training [R. Hasani, PhD Thesis] [Lechner et al. Nature MI, 2020] [pdf] (2016-2020). We then analytically and experimentally showed they are universal approximators [Hasani et al. AAAI, 2021], expressive continuous-time machine learning systems for sequential data [Hasani et al. AAAI, 2021] [Hasani et al. Nature MI, 2022], parameter efficient in learning new skills [Lechner et al. Nature MI, 2020] [pdf], causal and interpretable [Vorbach et al. NeurIPS, 2021] [Chahine et al. Science Robotics 2023] [pdf], and when linearized they can efficiently model very long-term dependencies in sequential data [Hasani et al. ICLR 2023].

In addition, we developed classes of nonlinear neural differential equation sequence models [Massaroli et al. NeurIPS 2021] and generalized them to graphs [Poli et al. DLGMA 2020]. We scaled and optimized continuous-time models using hybrid numerical methods [Poli et al. NeurIPS 2020], parallel-in-time schemes [Massaroli et al. NeurIPS 2020], and achieved state-of-the-art in control and forecasting tasks [Massaroli et al. SIAM Journal] [Poli et al. NeurIPS 2021][Massaroli et al. IEEE Control Systems Letters]. The team released one of the most comprehensive open-source libraries for neural differential equations [Poli et al. 2021 TorchDyn], used today in various applications for generative modeling with diffusion, and prediction.

We proposed the first efficient parallel scan-based linear state space architecture [Smith et al. ICLR 2023], and state-of-the-art time series state-space models based on rational functions [Parnichkun et al. ICML 2024]. We also introduced the first-time generative state space architectures for time series [Zhou et al. ICML 2023], and state space architectures for videos [Smith et al. NeurIPS 2024]

We proposed a new framework for neural operators [Poli et al. NeurIPS 2022], outperforming approaches such as Fourier Neural Operators in solving differential equations and prediction tasks.

Our team has co-invented deep signal processing architectures such as Hyena [Poli et al. ICML 2023] [Massaroli et al. NeurIPS 2023], HyenaDNA [Nguyen et al. NeurIPS 2023], and StripedHyena that efficiently scale to long context. Evo [Nguyen et al. 2024], based on StripedHyena, is a DNA foundation model that generalizes across DNA, RNA, and proteins and is capable of generative design of new CRISPR systems.

We were the first to scale language models based on both deep signal processing and state space layers [link], and have performed the most extensive scaling laws analysis on beyond-transformer architectures to date [Poli et al. ICML 2024], with new model variants that outperform existing open-source alternatives. 

The team is behind many of the best open-source LLM finetunes, and merges [Maxime Lebonne, link].

Last but not least, our team’s research has contributed to pioneering work in graph neural networks and geometric deep learning-based models [Lim et al. ICLR 2024], defining new measures for interpretability in neural networks [Wang et al. CoRL 2023], and the state-of-the-art dataset distillation algorithms [Loo et al. ICML 2023].""",MachineLearning,124,35,1727985420.0,1fvgo7o,Happysedits,https://www.reddit.com/r/MachineLearning/comments/1fvgo7o/r_announcing_the_first_series_of_liquid/,Research
[R] Why and when tying embedding (a story),"Hello, fellow Redditors! I want to share a little research journey that took place during my work. Instead of presenting it like a traditional research paper, I’ll try to make it more engaging and fun to read. I hope you find this approach interesting, and I’d love to hear your thoughts and feedback in the comments!

**This should be a 11 min. read**

# Background

Many of you might already be familiar with a technique called Weight Tying (WT), which was first proposed [here](https://arxiv.org/abs/1608.05859). In simple terms, WT works by sharing the weights between the input embedding layer and the output embedding layer (also known as the unembedding layer, output embedding layer, or pre-softmax layer). This technique is primarily used in the context of language modeling and offers two significant advantages:

1. It reduces the memory footprint by eliminating one of the two largest parameter matrices in large language models (LLMs).
2. It often results in better and faster outcomes.

While the first benefit is widely accepted, the second is a bit more complex. In fact, some LLMs use WT, while others do not. For example, I believe that Gemma uses WT, whereas LLaMa does not. This raises the question: why is that?

If you are interested, I found particularly insightful perspectives on this topic in this [Reddit post](https://www.reddit.com/r/MachineLearning/comments/1d2iurw/d_should_the_embedding_matrix_and_final/).

# Origin of the Idea

Earlier this year, I began exploring how to formalize the concept of semantic equivalence in neural networks. Interestingly, we can adapt the classical notion of semantics, commonly used in programming languages (see [here](https://en.wikipedia.org/wiki/Denotational_semantics)). In computer theory, two programs are considered semantically equivalent if, regardless of the context in which they are executed, they yield the same resulting context. To borrow from denotational semantics, we can express this as:

https://preview.redd.it/9wiyxh815aid1.png?width=268&format=png&auto=webp&s=c710b2162f616f3a1181c2fddb0dc1c5cba20b55

This can be read as: *""Program p\_1 is semantically equivalent to p\_2 if and only if, for all contexts* ρ, *the evaluation of p\_1 with* ρ *produces the same result as the evaluation of p\_2 with* ρ\*.""\*

But how do we adapt this notion to our scenario? Let's consider a simple example from Masked Language Modeling (MLM):

`The <MASK> of water is half empty/full.`

It’s clear that we can use either ""`empty`"" or ""`full`"" in this sentence without changing the outcome distribution of the `<MASK>` token. Therefore, we can say that ""`empty`"" and ""`full`"" are semantically equivalent in this context (""`The <MASK> of water is half ___`""). Realizing that two tokens are semantically equivalent if they can be swapped without affecting the output distribution, I arrived at this definition:

https://preview.redd.it/vyxipqtw5aid1.png?width=332&format=png&auto=webp&s=a43cce9fc582edeb9f58984338a2805cba7b5cfc

# Preliminary experiments

With this notion in mind, I wanted to explore how a neural network would encode these semantic equivalences in its weights. I suspected that embeddings for semantically equivalent tokens would naturally become closer to each other during training. This intuition was partly based on my knowledge that BERT embeddings capture similar relationships, where words like ""learn,"" ""learning,"" and ""learned"" are clustered together in the embedding space (see [here](https://home.ttic.edu/~kgimpel/viz-bert/viz-bert.html)).

To test this idea, I designed a simple experiment. The goal was to train a Masked Language Model (MLM) on a binary parity problem. Consider a string like `10011D`, where there are three `1`s, indicating that the string is odd. Along with the binary string, I included a parity label (`D` for odd and `E` for even). For instance, other examples could be `11000E` and `00100D`. Then, I introduced a twist: I randomly swapped the symbol `1` with either `A` or `B` with equal probability. So, from a string like `10011D`, you might get something like `A00BAD`. Finally, I masked one of the symbols and trained a model to predict the masked symbol. This process resulted in a dataset like the following:

|Sample|Label|
|:-|:-|
|`00A?00E`|A|
|`00A?00E`|B|
|`00B?00E`|A|
|`00B?00E`|B|
|`0BB?A0D`|0|

In this setup, symbols `A` and `B` are semantically equivalent by design—swapping `A` with `B` does not change the outcome distribution. As expected, the embeddings for `A` and `B` converged to be close to each other, while both remained distinct from the embedding of `0`. Interestingly, this behavior was also observed in the output embeddings, which neatly aligns with the principles of the Weight Tying technique.

# Formalizing the behavior

If it were up to me, I would have been content writing a paper on the observation that MLMs learn semantic relationships in both the input and output embedding layers. However, to publish in a reputable conference, a bit of mathematical rigor is usually required (even though math isn’t my strongest suit). So, I attempted to formalize this behavior.

# Output Embeddings

When it came to the output embeddings, I couldn't prove that two semantically equivalent symbols must be close in the output embedding space. However, I did manage to prove that they would be close under the following condition:

https://preview.redd.it/xrgs5gm15aid1.png?width=167&format=png&auto=webp&s=4645df4fd53ccd7777936be9926d6018fffba9ff

Interestingly, this result is purely about conditional probability and doesn’t directly involve labels or semantics. However since it provided some insight, I was reasonably satisfied and decided to move on.

# Input Embeddings

For the input embeddings, I was able to prove that two semantically equivalent symbols would indeed be close to each other in the input embedding space. However, the assumptions required for this proof were so restrictive that they would likely never hold in a real-world scenario. So, it ended up being a ""junk"" theorem, written more for the sake of publication than for practical application. Despite this, the intuition behind it still feels compelling.

The idea is simple: if two symbols are semantically equivalent—meaning they can be swapped without affecting the model’s output—the easiest way to ensure this is by giving them identical embeddings. In this way, the network's output remains unchanged by definition.

Proving this theorem, however, was a real challenge. I spent several days in the lab working on it, only to have my results scrutinized by colleagues and find errors. It took me about two to three weeks to produce a proof that could withstand their reviews. Despite the struggles, I remember this period as a particularly enjoyable part of my PhD journey.

# The First Draft

Armed with these two theorems—one for the output embeddings and one for the input embeddings—I began writing the first draft of my paper. My goal was to convey the idea that **LLMs are semantic learners**. I started by introducing the concept of semantic equivalence, followed by the theorem related to input embeddings. Next, I presented the output embedding theorem.

However, as I progressed, I realized that I was missing something crucial: experimental evidence to support the output embedding theorem. While the theoretical groundwork was in place, without empirical validation, the argument felt incomplete (at least this is what a reviewer would say).

# Back to the experiments (First time)

As I mentioned earlier, I proved the following implication (though I’m omitting some of the hypotheses here):

https://preview.redd.it/xvmu4t025aid1.png?width=354&format=png&auto=webp&s=e0e1332ef0232481cb136719451ac609cfa42693

So, I decided to rerun the experiments, this time closely monitoring the output embeddings. As expected, the output embeddings of `A` and `B` did indeed converge, becoming close to each other.

This finding was quite fascinating to me. On one hand, we have semantically equivalent symbols that are close in the input embedding space. On the other hand, we have conditionally equivalent symbols—those with the same conditional probability across all contexts (for all ρ: p(σ\_1 | ρ) = p(σ\_2 | ρ))—that are close in the output space.

# Back to the Draft (First Time)

With these new experiments in hand, I revised the draft, introducing the concept of conditional equivalence and the theorem connecting it to output embeddings. This allowed me to clearly articulate how conditional equivalence is reflected in the output embeddings.

As I was writing, it struck me that the Weight Tying (WT) technique is often employed in these scenarios. But this led to a new question: what happens if we use WT with symbols that are conditionally equivalent but not semantically equivalent? On one hand, these symbols should be close in the input embedding space. On the other hand, they should be far apart in the output embedding space because they have different conditional probabilities. However, with WT, the input and output spaces are tied together, making it impossible for two symbols to be simultaneously close and far apart.

This realization sent me back to the experiments to investigate this particular setting.

# Back to the Experiments (Second Time)

In our previous experiments, we established that the probability of seeing `A` is the same as `B` in any given context. Now, let's introduce another layer of complexity by replacing the symbol `0` with symbols `X` and `Y`, but this time, `X` will be more probable than `Y`. This changes our dataset to something like this:

|Sample|Label|
|:-|:-|
|`XYA?XXE`|A|
|`XXA?XYE`|B|
|`Y?BAXYE`|X|
|`XXBBX?E`|Y|
|`XBB?AYD`|0|

When we train an MLM model on this dataset, it’s easy to observe that in the input embedding space, `X` and `Y` become close to each other, just like `A` and `B`. This is because `X` and `Y` are semantically equivalent. However, unlike `A` and `B`, `X` and `Y` do not get close in the output embedding space because they have different conditional probabilities.

Now, what happens if we tie the embeddings? We observe that `A` and `B` converge more quickly, while `X` and `Y` remain distanced from each other. Additionally, we noticed that training becomes a bit more unstable—the distance between `X` and `Y` fluctuates significantly during training. Overall, the untied model tends to perform better, likely because it avoids the conflicting requirements imposed by weight tying.

# Back to the Draft, Again (Third Time)

I was quite pleased with the results we obtained, so I eagerly incorporated them into the paper. As I was revising, I also discussed the idea that Weight Tying (WT) should be used only when conditionally equivalent symbols are also semantically equivalent. This can be expressed as:

https://preview.redd.it/g79ejj825aid1.png?width=441&format=png&auto=webp&s=67298479da49374d80ddcce7b716c5e4e5ef558c

Or, more concisely:

https://preview.redd.it/g5nle0h25aid1.png?width=216&format=png&auto=webp&s=68f0d715abf87495ae49273b518df6ccedc8a148

While discussing this property, I realized that my explanation closely mirrored the hypothesis that *""similar words have similar contexts""*. This concept, which I later discovered is known as the **Distributional Hypothesis**, made the whole paper click together. I then restructured the work around this central concept.

If we accept the formalization of the Distributional Hypothesis as σ\_1 sem.eqv. σ\_2 iff. σ\_1 cnd.eqv. σ\_2, then it follows that WT should be employed only when this hypothesis holds true.

# Submission & Reviews

With [ICML2024](https://icml.cc/Conferences/2024) being the next major conference on the horizon, we decided to submit our work there. Most of the reviews were helpful and positive, but a recurring critique was the lack of ""large-scale"" experiments.

I simply do not understand this obsession with experiments that require hundreds of GPUs. I mean, I submitted a paper mostly theoretical aiming to explain a very well-known phenomenon supported by a vast literature, isn't a small controlled experiment enough (which is included mostly to make the paper self-contained) when backed by the literature?

Well, I am a nobody in the research community, furthermore this is my first publication at a ""Big"" conference like ICML so I complied (kind of, still one GPU experiment, I do not have access to more than that) although these experiments do not add practically anything.

In the end, I was thrilled to have the paper accepted as a spotlight poster. It was a huge milestone for me, making me feel like a genuine researcher in the field. I dedicated almost a month to preparing the poster and video presentation, which can be viewed [here](https://icml.cc/virtual/2024/poster/32648). The effort was well worth it!

# Conference & Presentation

On the day of the conference, I arrived around 9 A.M. with only an hour of sleep from the flight—naturally, I was too excited to rest properly. I made it to the conference a bit late, and during the first tutorial session, I struggled to stay awake despite the coffee. A little before lunch, I headed back to the hotel to catch a few hours of sleep. In the evening I attended the great tutorial presentation [physics of Language Model](https://icml.cc/virtual/2024/tutorial/35223).

In the next days, I made a few friends. Talked to a lot of people included [Alfredo Canziani](https://atcold.github.io/), an incredible AI communicator, and [Randall Balestriero](https://randallbalestriero.github.io/) an incredible scientist in the field. I saw also [Michael Bronstein](https://www.cs.ox.ac.uk/people/michael.bronstein/) but of course, he was always surrounded and I could not bring myself to talk to him.

The last poster session was my time to present, and I was quite nervous, as it was my first time presenting at such a conference. To my surprise, many attendees weren’t familiar with the Distributional Hypothesis—a concept I assumed everyone would know, even though I hadn’t known the term myself. This made me question the effectiveness of my paper’s ""marketing"" (presentation, title, etc.). Perhaps I should have emphasized the ""semantics"" aspect more.

One particularly memorable interaction was with a tall guy from DeepMind. He listened to part of my presentation and then pointed out, in a very polite manner, that my theorems might not be correct. I was confident in the proofs, which had been reviewed by a PhD student in mathematics who had won some math competitions. We debated back and forth until I understood his argument, which involved a specific construction of the embedding matrices. He was right, but his argument broke one of the theorems' assumptions. You have to know that, I was not even showing these hypothesis on the poster because I did not believe that anyone would have been interested in these details. This guy practically had a deeper understanding of my theorems than me without listening to half of the presentation and without the full hypothesis. Well, in conclusion, Deepmind has some freaking guys working there.

# Conclusions

* **Use Weight Tying Only When the Distributional Hypothesis Holds**.
* **DeepMind Has Some Incredible People**
* **Do not go to the tutorials with 1hr of sleep** (3hr are okay though).
* **Writing the Paper is Crucial**: While I previously believed that experiments should come first, I now realize the importance of writing down your ideas early. Putting thoughts into words often clarifies and integrates concepts in ways experiments alone may not. This is perhaps the most valuable lesson I’ve learned from this paper.

# Limitations & Future works

If you're considering using the WT technique, you might wonder: when does the DH actually hold? Does it apply to your specific problem? Does it apply to natural language tasks in general?

Answering these questions can be challenging and may not always be feasible. Consequently, this work may have limited practical utility. It simply pushes the question when applying WT to when the DH holds. However, I suspect that the DH only partially holds for natural language, which might explain why not all LLMs use WT.

So, my idea is that it should be more useful to run the training with WT up until a certain point and then untie the embeddings to allow differences between tokens that are conditionally eqv. but not semantically eqv. (or vice versa) to arise. Unfortunately, I lack the GPU resources to train a meaningful LLM to test this hypothesis (I am from a very small lab (not even a Machine Learning lab to be fair)). If anyone is interested in exploring this idea or knows of similar work, I would greatly appreciate hearing about it.",MachineLearning,126,25,1723488884.0,1eqm0lr,f14-bertolotti,https://www.reddit.com/r/MachineLearning/comments/1eqm0lr/r_why_and_when_tying_embedding_a_story/,Research
[N] AI achieves silver-medal standard solving International Mathematical Olympiad problems,"https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/

They solved 4 of the 6 IMO problems (although it took days to solve some of them). This would have gotten them a score of 28/42, just one point below the gold-medal level.",MachineLearning,121,39,1721924212.0,1ebyx03,we_are_mammals,https://www.reddit.com/r/MachineLearning/comments/1ebyx03/n_ai_achieves_silvermedal_standard_solving/,News
[D] To PhD or not to PhD,"I think this has been asked tons of times but let me ask it one more time.

I am currently working as applied scientist at MSFT. However, I am more looking into science positions, something like research scientist at DeepMind. Although jobs do not specifically need a PhD but the competition is fierce and is flooded with many PhD holders.

I really do enjoy research and want to PhD but I am always asking myself if it is really worth it.

That's an open question for sure, please feel free to share your thoughts.

",MachineLearning,118,77,1731703468.0,1gs688q,oddhvdfscuyg,https://www.reddit.com/r/MachineLearning/comments/1gs688q/d_to_phd_or_not_to_phd/,Discussion
[R] Apple Intelligence Foundation Language Models,,MachineLearning,120,12,1723252628.0,1eogp0c,AhmedMostafa16,https://arxiv.org/abs/2407.21075,Research
[D] PyTorch Vs. ... why still Tensorflow?,"I'm getting back into machine learning after a long hiatus. After talking with a friend and doing some research (e.g., [Quick Poll Tensorflow Vs PyTorch in 2024](https://www.reddit.com/r/MachineLearning/comments/19crtxp/d_quick_poll_tensorflow_vs_pytorch_in_2024/)), I get the feeling that TensorFlow might not be the best library to use to get back up to speed.

Now, my question for this post is: If TensorFlow has fallen so far out of favor and people are advising against using it, why does a Google search for ""PyTorch vs."" still bring up a plethora of articles and sites comparing PyTorch to TensorFlow?

Are there no decent contenders to PyTorch that I should consider before setting up a PyTorch environment?

Looking forward to your insights!",MachineLearning,119,66,1717715940.0,1d9w79p,Tolure,https://www.reddit.com/r/MachineLearning/comments/1d9w79p/d_pytorch_vs_why_still_tensorflow/,Discussion
[R] Simplified RNNs Achieve Transformer-Like Performance with Parallel Training and Reduced Parameters,"This paper systematically examines whether RNNs might have been sufficient for many NLP tasks that are now dominated by transformers. The researchers conduct controlled experiments comparing RNNs and transformers while keeping model size, training data, and other variables constant.

Key technical points:
- Tested both architectures on language modeling and seq2seq tasks using matched parameters (70M-1.5B)
- Introduced ""RNN with Parallel Generation"" (RPG) allowing RNNs to generate tokens in parallel like transformers
- Evaluated on standard benchmarks including WikiText-103 and WMT14 En-De translation
- Analyzed representation capacity through probing tasks and attention pattern analysis

Main results:
- RNNs matched or outperformed similarly-sized transformers on WikiText-103 language modeling
- Transformers showed 1-2 BLEU score advantage on translation tasks
- RPG achieved 95% of transformer generation speed with minimal accuracy loss
- RNNs showed stronger local context modeling while transformers excelled at long-range dependencies

I think this work raises important questions about architecture choice in modern NLP. While transformers have become the default, RNNs may still be viable for many applications, especially those focused on local context. The parallel generation technique could make RNNs more practical for production deployment.

I think the results suggest we should reconsider RNNs for specific use cases rather than assuming transformers are always optimal. The computational efficiency of RNNs could be particularly valuable for resource-constrained applications.

TLDR: Comprehensive comparison shows RNNs can match transformers on some NLP tasks when controlling for model size and training. Introduces parallel generation technique for RNNs. Results suggest architecture choice should depend on specific application needs.

[Full summary is here](https://aimodels.fyi/papers/arxiv/were-rnns-all-we-needed). Paper [here](https://arxiv.org/abs/2410.01201)",MachineLearning,118,22,1733145542.0,1h4urpr,Successful-Western27,https://www.reddit.com/r/MachineLearning/comments/1h4urpr/r_simplified_rnns_achieve_transformerlike/,Research
[D] Hinton and Hassabis on Chomsky’s theory of language,"I’m pretty new to the field and would love to hear more opinions on this. I always thought Chomsky was a major figure on this but it seems like Hinton and Hassabis(later on) both disagree with it. Here: https://www.youtube.com/watch?v=urBFz6-gHGY (longer version: https://youtu.be/Gg-w_n9NJIE)

I’d love to get both an ML and CogSci perspective on this and more sources that supports/rejects this view.

Edit: typo + added source.",MachineLearning,120,116,1732889412.0,1h2mkye,giuuilfobfyvihksmk,https://www.reddit.com/r/MachineLearning/comments/1h2mkye/d_hinton_and_hassabis_on_chomskys_theory_of/,Discussion
[P] Implementing the Llama 3.2 1B and 3B Architectures from Scratch (A Standalone Jupyter Notebook),,MachineLearning,121,5,1728134151.0,1fwq5su,seraschka,https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/07_gpt_to_llama/standalone-llama32.ipynb,Project
[D] Next big thing in Time series?,"In NLP, we’ve seen major milestones like transformers, GPT, and LLMs, which have revolutionized the field. Time series research seems to be borrowing a lot from NLP and CV—like transformer-based models, self-supervised learning, and now even foundation models specifically for time series. But there doesn’t seem to be a clear consensus yet on what works best. For example, NLP has well-accepted pretraining strategies like masked language modeling or next-token prediction, but nothing similar has become a standard for time series.  
  
Lately, there’s been a lot of talk about adapting LLMs for time series or even building foundation models specifically for the purpose. On the other hand, some research indicates that LLMs are not helpful for time series. 

So I just wanna know what can be a game changer for time series!",MachineLearning,119,57,1732177238.0,1gwbhxq,Few-Pomegranate4369,https://www.reddit.com/r/MachineLearning/comments/1gwbhxq/d_next_big_thing_in_time_series/,Discussion
[P] Converting GPT to Llama step-by-step code guide,"An often-asked question is how GPT compares to Llama. In my opinion, one of the best ways to understand the differences is to implement both architectures from scratch. Here's a [step-by-step Jupyter notebook guide](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/07_gpt_to_llama/converting-gpt-to-llama2.ipynb).

https://preview.redd.it/qowi1sf12krd1.jpg?width=4286&format=pjpg&auto=webp&s=b815e4e6df8d38c70816fb6f51ff1482b6cca80e

",MachineLearning,121,15,1727531486.0,1frer4z,seraschka,https://www.reddit.com/r/MachineLearning/comments/1frer4z/p_converting_gpt_to_llama_stepbystep_code_guide/,Project
[D] Why is CUDA so much faster than ROCm? ,"Usually people respond with ""Because NVIDIA had more time and more money"". However, why cant AMD catch up? What are the exact things that make optimizing ROCm so hard?? 

It would be helpful if you could point to some resources or if your answer would be as detailed as possible regarding the implementation of specific kernels and structures and how CUDA calls are exactly made and optimized from Triton or XLA. Thx :)",MachineLearning,117,83,1725605554.0,1fa8vq5,evilevidenz,https://www.reddit.com/r/MachineLearning/comments/1fa8vq5/d_why_is_cuda_so_much_faster_than_rocm/,Discussion
[R] Trillion-Parameter Sequential Transducers for Generative Recommendations,"Researchers at Meta recently published a ground-breaking paper that combines the technology behind ChatGPT with Recommender Systems. They show they can scale these models up to 1.5 trillion parameters and demonstrate a 12.4% increase in topline metrics in production A/B tests. 

We dive into the details in this article: [https://www.shaped.ai/blog/is-this-the-chatgpt-moment-for-recommendation-systems](https://www.shaped.ai/blog/is-this-the-chatgpt-moment-for-recommendation-systems)",MachineLearning,119,31,1717588546.0,1d8o2sz,skeltzyboiii,https://www.reddit.com/r/MachineLearning/comments/1d8o2sz/r_trillionparameter_sequential_transducers_for/,Research
[R]  Is it acceptable to exclude non-reproducible state-of-the-art methods when benchmarking for publication?,"I’ve developed a new algorithm and am preparing to benchmark its performance for a research publication. However, I’ve encountered a challenge: some recent state-of-the-art methods lack publicly available code, making them difficult or impossible to reproduce.

Would it be acceptable, in the context of publishing research work, to exclude these methods from my comparisons and instead focus on benchmarking against methods and baselines with publicly available implementations?

What is the common consensus in the research community on this issue? Are there recommended best practices for addressing the absence of reproducible code when publishing results?",MachineLearning,119,34,1735673623.0,1hqm6vd,Training_Bet_7905,https://www.reddit.com/r/MachineLearning/comments/1hqm6vd/r_is_it_acceptable_to_exclude_nonreproducible/,Research
"[R] RWKV-7 0.1B (L12-D768) trained w/ ctx4k solves NIAH 16k, extrapolates to 32k+, 100% RNN and attention-free, supports 100+ languages and code","Hi everyone :) We find the smallest RWKV-7 0.1B (L12-D768) is already great at long context, while being 100% RNN and attention-free:

https://preview.redd.it/rjcu9y73js7e1.png?width=1759&format=png&auto=webp&s=b8fd2c8049b0886dbb87c715e120b1066b07b899

RWKV-7 World 0.1b is trained on a multilingual dataset for 1T tokens:

https://preview.redd.it/cyvpr00mjs7e1.png?width=927&format=png&auto=webp&s=01a98aa79be426d2d603fd5ae26ddad0ce1c0ee2

These results are tested by the community: [https://github.com/Jellyfish042/LongMamba](https://github.com/Jellyfish042/LongMamba)

===

More evals of RWKV-7 World. It is the best multilingual 0.1b LM at this moment :)

https://preview.redd.it/a3yeedt8ks7e1.png?width=1497&format=png&auto=webp&s=88cb8d9861a213a7be712acaca6546e7f63124ac

Try it in Gradio demo: [https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-1](https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-1)

Model download: [https://huggingface.co/BlinkDL](https://huggingface.co/BlinkDL)

Train it: [https://github.com/BlinkDL/RWKV-LM](https://github.com/BlinkDL/RWKV-LM)

I am training v7 0.4b/1b/3b too.

The community is working on ""transferring"" transformer weights to RWKV, and released a v6 32b model a few days ago: [https://huggingface.co/recursal/QRWKV6-32B-Instruct-Preview-v0.1](https://huggingface.co/recursal/QRWKV6-32B-Instruct-Preview-v0.1)

===

RWKV-7 has moved away from linear attention, and becomes a meta-in-context learner, test-time-training its state on the context via in-context gradient descent at every token.

More details in RWKV dot com website (there are 30+ RWKV-related papers too).

https://preview.redd.it/x9tf1fnals7e1.png?width=722&format=png&auto=webp&s=bf3f989e9736a38e7713ed41f17e1a2e5dd577b5

===

And the community find a tiny RWKV-6 (with 12m params) can solve any sudoku, through very long CoT:

[https://github.com/Jellyfish042/Sudoku-RWKV](https://github.com/Jellyfish042/Sudoku-RWKV)

Because RWKV is an RNN, we always have constant speed & vram, regardless of ctxlen.

For example, it can solve ""the world's hardest sudoku"" with 4M (!) tokens CoT:

https://preview.redd.it/wo2vu9t3ns7e1.png?width=1280&format=png&auto=webp&s=32da05c2fb3e7622fde6b27e34c52795dba5d6c3

",MachineLearning,117,9,1734613624.0,1hhshwp,bo_peng,https://www.reddit.com/r/MachineLearning/comments/1hhshwp/r_rwkv7_01b_l12d768_trained_w_ctx4k_solves_niah/,Research
Ethics concerns and Google [D],"Apologies if this isn't the right place for this facet of ML, but it didn't seem against the rules.

I recently participated in an Alphabet human data research study used to evaluate AI agents and models.

Without going further into the details, the structure of the study felt very ethically questionable. The agreement said if there were any concerns, to contact HuBREC, human behavioural research ethics committee.

However, their email provided in the agreement hubrec@google.com does not exist and I have no point of contact at all short of looking up past academic talks and cold emailing people. 

I am having a lot of difficulty searching for next steps as there is no other contact information I can use except for that email. I do know that Google has fired AI ethics researchers in recent memory, and that this topic never seems to be taken seriously. It seems like a bad look for an on-going study to point you to a committee that doesn't seem to exist.",MachineLearning,122,18,1729799393.0,1gbblsc,chaneg,https://www.reddit.com/r/MachineLearning/comments/1gbblsc/ethics_concerns_and_google_d/,Discussion
"[R] Lamini.AI introduces Memory Tuning: 95% LLM Accuracy, 10x Fewer Hallucinations","https://www.lamini.ai/blog/lamini-memory-tuning

* Lamini Memory Tuning is a new way to embed facts into LLMs that improves factual accuracy and reduces hallucinations to previously unachievable levels — for one Fortune 500 customer, Lamini Memory Tuning led to 95% accuracy compared to 50% with other approaches. Hallucinations were reduced from 50% to 5%.
* Lamini Memory Tuning is a research breakthrough that overcomes a seeming paradox in the AI world: achieving precise factual accuracy (i.e. no hallucinations) while upholding the generalization capabilities that make LLMs valuable in the first place.
* The method entails tuning millions of expert adapters (e.g. LoRAs) with precise facts on top of any open-source LLM, like Llama 3 or Mistral 3. If the goal is to get Roman Empire facts exactly right, Lamini Memory Tuning would create experts on Caesar, aqueducts, legions, and any other facts you provide. Inspired by information retrieval, the model retrieves only the most relevant experts from an index at inference time — not all the model weights — so latency and cost are dramatically lower. High accuracy, high speed, low cost: with Lamini Memory Tuning, you don’t have to choose.

Research paper: https://github.com/lamini-ai/Lamini-Memory-Tuning/blob/main/research-paper.pdf",MachineLearning,117,30,1718330894.0,1dffyfs,we_are_mammals,https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/,Research
[R] Continuous Latent Space Reasoning: Enhancing LLM Performance Through Chain of Continuous Thought,"This paper introduces **COCONUT** (Chain of Continuous Thought), which transforms language model reasoning from discrete token space into continuous latent space. The key idea is encoding reasoning steps as continuous vectors rather than text tokens, allowing for more flexible and precise intermediate computations.

Main technical points:
* Encoder-decoder architecture that maps text↔continuous vectors
* Novel continuous reasoning module operating on latent vectors
* Parallel processing of reasoning steps in continuous space
* Gradient-based optimization during the reasoning process
* Special loss function combining reconstruction and reasoning objectives

Key results:
* **20%** improvement on reasoning benchmarks vs traditional methods
* Reduced computational steps needed for complex problems
* More consistent performance across different reasoning tasks
* Better handling of mathematical and logical reasoning
* Enhanced ability to maintain coherent reasoning chains

I think this approach could meaningfully advance how language models handle complex reasoning tasks. By moving beyond discrete tokens, models may better capture the continuous nature of human-like reasoning. The ability to optimize in continuous space during reasoning is particularly promising for improving reliability.

I think the main challenge will be scaling this to very large models while managing computational costs. The translation between discrete and continuous spaces adds overhead that needs to be addressed.

TLDR: New method transforms language model reasoning into continuous vector space instead of discrete tokens, showing 20% better performance on reasoning tasks through more flexible computation.

[Full summary here](https://aimodels.fyi/papers/arxiv/training-large-language-models-to-reason-continuous). Paper [here](https://arxiv.org/abs/2412.06769).",MachineLearning,115,7,1733924366.0,1hbto1w,Successful-Western27,https://www.reddit.com/r/MachineLearning/comments/1hbto1w/r_continuous_latent_space_reasoning_enhancing_llm/,Research
[R] Shared Imagination: LLMs Hallucinate Alike,"Happy to share our recent paper, where we demonstrate that LLMs exhibit surprising agreement on purely imaginary and hallucinated contents -- what we call a ""shared imagination space"". To arrive at this conclusion, we ask LLMs to generate questions on hypothetical contents (e.g., a made-up concept in physics) and then find that they can answer each other's (unanswerable and nonsensical) questions with much higher accuracy than random chance. From this, we investigate in multiple directions on its emergence, generality and possible reasons, and given such consistent hallucination and imagination behavior across modern LLMs, discuss implications to hallucination detection and computational creativity. 

Link to the paper: [https://arxiv.org/abs/2407.16604](https://arxiv.org/abs/2407.16604)

Link to the tweet with result summary and highlight: [https://x.com/YilunZhou/status/1816371178501476473](https://x.com/YilunZhou/status/1816371178501476473)

Please feel free to ask any questions!

[The main experiment setup and finding.](https://preview.redd.it/dc9nepua6oed1.png?width=2126&format=png&auto=webp&s=4f7b670d482987ff9013acc35f48d3c4b60a297f)

",MachineLearning,113,27,1721915362.0,1ebvd4w,zyl1024,https://www.reddit.com/r/MachineLearning/comments/1ebvd4w/r_shared_imagination_llms_hallucinate_alike/,Research
[D] How do you structure your codebase and workflow for a new research project?,"Suppose you have got a new idea about a solution to a problem in the domain you are working in. How do you go about implementing the thing from the ground up? 

What is the general structure of the codebase you construct for your project?

How do you go about iteratively training and testing your solution until you arrive at a final solution where you can write a paper for publication? 

  
Is there any design recipe you follow? Where did you learn it from?",MachineLearning,114,21,1730266997.0,1gffm46,HopeIsGold,https://www.reddit.com/r/MachineLearning/comments/1gffm46/d_how_do_you_structure_your_codebase_and_workflow/,Discussion
[R] LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench,"Updated Paper [https://arxiv.org/pdf/2410.02162](https://arxiv.org/pdf/2410.02162) (includes results when paired w/ a verifier)

Original Paper: [https://www.arxiv.org/abs/2409.13373](https://www.arxiv.org/abs/2409.13373)

""while o1’s performance is a quantum improvement on the benchmark, outpacing the competition, it is still far from saturating it..""

The summary is apt.  o1 looks to be a **very** impressive improvement.  At the same time, it reveals the remaining gaps:  degradation with increasing composition length,  100x cost, and huge degradation when ""retrieval"" is hampered via obfuscation of names.

But, I wonder if this is close enough. e.g. this type of model is at least sufficient to provide synthetic data / supervision to train a model that can fill these gaps.   If so, it won't take long to find out, IMHO.

Also the authors have some spicy footnotes.  e.g. :

""The rich irony of researchers using tax payer provided research funds to pay private companies like OpenAI to evaluate their private commercial models is certainly not lost on us.""",MachineLearning,111,47,1729280167.0,1g6qpeq,marojejian,https://www.reddit.com/r/MachineLearning/comments/1g6qpeq/r_llms_still_cant_plan_can_lrms_a_preliminary/,Research
[D] Is there a more systematic way of choosing the layers or how deep the architecture goes when creating a neural network?,"So I'm learning about deep learning and neural networks and I'm really a bit confused on this part. I'm generally familiar with the layers available and how they work (at least those that are widely used) But I'm still having a hard time trying to figure out what to use on what. Is there a more logical or a systematic way of doing this? like mathematically or something? I'm down for experimenting but I'm just trying to avoid the rabbit hole since this projects on a deadline and I'm not down with that

  
\`\`\` EDIT \`\`\`\`

Thank you for all the responses especially for giving reading material and suggestions. ",MachineLearning,117,39,1714917845.0,1ckrzq6,PsychologicalAd7535,https://www.reddit.com/r/MachineLearning/comments/1ckrzq6/d_is_there_a_more_systematic_way_of_choosing_the/,Discussion
[R] The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery,"Blog Post: https://sakana.ai/ai-scientist/

Paper: https://arxiv.org/abs/2408.06292

Open-Source Project: https://github.com/SakanaAI/AI-Scientist

**Abstract**

One of the grand challenges of artificial general intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used as aids to human scientists, e.g. for brainstorming ideas, writing code, or prediction tasks, they still conduct only a small part of the scientific process. This paper presents the first comprehensive framework for fully automatic scientific discovery, enabling frontier large language models to perform research independently and communicate their findings. We introduce The AI Scientist, which generates novel research ideas, writes code, executes experiments, visualizes results, describes its findings by writing a full scientific paper, and then runs a simulated review process for evaluation. In principle, this process can be repeated to iteratively develop ideas in an open-ended fashion, acting like the human scientific community. We demonstrate its versatility by applying it to three distinct subfields of machine learning: diffusion modeling, transformer-based language modeling, and learning dynamics. Each idea is implemented and developed into a full paper at a cost of less than $15 per paper. To evaluate the generated papers, we design and validate an automated reviewer, which we show achieves near-human performance in evaluating paper scores. The AI Scientist can produce papers that exceed the acceptance threshold at a top machine learning conference as judged by our automated reviewer. This approach signifies the beginning of a new era in scientific discovery in machine learning: bringing the transformative benefits of AI agents to the entire research process of AI itself, and taking us closer to a world where endless affordable creativity and innovation can be unleashed on the world's most challenging problems.",MachineLearning,114,88,1723515449.0,1eqwfo0,hardmaru,https://www.reddit.com/r/MachineLearning/comments/1eqwfo0/r_the_ai_scientist_towards_fully_automated/,Research
[R] Most Time Series Anomaly Detection results are meaningless (two short videos explain why),"Dear Colleagues

Time Series Anomaly Detection (TSAD) is hot right now, with dozens of  papers each year in NeurIPS, SIGKDD, ICML, PVLDB etc.

However, I claim that much of the published results are meaningless, because the uncertainty of the ground truth labels dwarfs any claimed differences between algorithms or amount of claimed improvements.

I have made two 90-second-long videos that make this clear in a visual and intuitive way:

 1)      Why Most Time Series Anomaly Detection Results are Meaningless (Dodgers)

[https://www.youtube.com/watch?v=iRN5oVNvZwk&ab\_channel=EamonnKeogh](https://www.youtube.com/watch?v=iRN5oVNvZwk&ab_channel=EamonnKeogh)

  2)      Why Most Time Series Anomaly Detection Results are Meaningless (AnnGun)

[https://www.youtube.com/watch?v=3gH-65RCBDs&ab\_channel=EamonnKeogh](https://www.youtube.com/watch?v=3gH-65RCBDs&ab_channel=EamonnKeogh)

As always, corrections and comments welcome.

Eamonn

 EDIT: To be clear, my point is simply to prevent others from wasting time working with datasets with essentially random labels. In addition, we should be cautious of any claims in the literature that are based on such data (and that includes at least dozens of highly cited papers)

  


For a review of most of the commonly used TSAD datasets, see this file:

[https://www.dropbox.com/scl/fi/cwduv5idkwx9ci328nfpy/Problems-with-Time-Series-Anomaly-Detection.pdf?rlkey=d9mnqw4tuayyjsplu0u1t7ugg&dl=0](https://www.dropbox.com/scl/fi/cwduv5idkwx9ci328nfpy/Problems-with-Time-Series-Anomaly-Detection.pdf?rlkey=d9mnqw4tuayyjsplu0u1t7ugg&dl=0)",MachineLearning,111,60,1731110299.0,1gmwxnr,eamonnkeogh,https://www.reddit.com/r/MachineLearning/comments/1gmwxnr/r_most_time_series_anomaly_detection_results_are/,Research
"[R] RWKV-7: attention-free and surpassing strong Modded-GPT baseline (the one with Muon optimizer), while only using headsz 64","Hi everyone. RWKV-7 (100% RNN and attention-free) can surpass the strong Modded-GPT baseline (the one with Muon optimizer, currently trending on twitter).

Training code & log: [https://github.com/BlinkDL/modded-nanogpt-rwkv](https://github.com/BlinkDL/modded-nanogpt-rwkv) And it can reach loss 3.26xx if you use a larger headsz.

My current implementation is very inefficient though. Might can reach 85% Modded-GPT speed @ ctx1k (or faster than Modded-GPT @ ctx4k) after optimization. Any helps are welcome :)

https://preview.redd.it/48m3lsvkb4wd1.png?width=873&format=png&auto=webp&s=647d86ed47d40a4f742ed9512a835dee41069e4f

======================================

The strong GPT baseline:

https://preview.redd.it/h2ckr31mb4wd1.png?width=584&format=png&auto=webp&s=b667bfbc50298f8335a889b85c55f68ee8db38a5

======================================

RWKV-7 moves away from the ""linear attention"" design to achieve greater performance :)

https://preview.redd.it/ijyz0sgnb4wd1.png?width=1233&format=png&auto=webp&s=f413d0e7bcd3a76c5e788f2ca231a37706b24345

",MachineLearning,111,24,1729520299.0,1g8qsea,bo_peng,https://www.reddit.com/r/MachineLearning/comments/1g8qsea/r_rwkv7_attentionfree_and_surpassing_strong/,Research
[R] Never Train from scratch,"https://arxiv.org/pdf/2310.02980 

The authors show that when transformers are pre trained, they can match the performance with S4 on the Long range Arena benchmark. ",MachineLearning,112,33,1730815363.0,1gk7dny,Whatever_635,https://www.reddit.com/r/MachineLearning/comments/1gk7dny/r_never_train_from_scratch/,Research
[R] SpotDiffusion: A Fast Approach For Seamless Panorama Generation Over Time,"I am very happy to announce that our paper ""SpotDiffusion: A Fast Approach For Seamless Panorama Generation Over Time"" got accepted for WACV2025: [https://arxiv.org/abs/2407.15507](https://arxiv.org/abs/2407.15507)  
Project-Page: [https://spotdiffusion.github.io](https://spotdiffusion.github.io)  
Code: [https://github.com/stanifrolov/spotdiffusion](https://github.com/stanifrolov/spotdiffusion)

Our method shifts non-overlapping denoising windows over time, ensuring that seams in one timestep are corrected in the next. This results in coherent, high-resolution images with fewer overall steps. We demonstrate the effectiveness of our approach through qualitative and quantitative evaluations, comparing it with MultiDiffusion, SyncDiffusion, and StitchDiffusion. Our method offers several key benefits, including improved computational efficiency and faster inference times while producing comparable or better image quality.",MachineLearning,109,6,1730169193.0,1gekcus,Maleficent_Stay_7737,https://www.reddit.com/r/MachineLearning/comments/1gekcus/r_spotdiffusion_a_fast_approach_for_seamless/,Research
[D] Foundational Time Series Models Overrated?,"I've been exploring foundational time series models like TimeGPT, Moirai, Chronos, etc., and wonder if they truly have the potential for powerfully sample-efficient forecasting or if they're just borrowing the hype from foundational models in NLP and bringing it to the time series domain.

I can see why they might work, for example, in demand forecasting, where it's about identifying trends, cycles, etc. But can they handle arbitrary time series data like environmental monitoring, financial markets, or biomedical signals, which have irregular patterns and non-stationary data?

Is their ability to generalize overestimated?",MachineLearning,108,41,1716048006.0,1cv0hl2,KoOBaALT,https://www.reddit.com/r/MachineLearning/comments/1cv0hl2/d_foundational_time_series_models_overrated/,Discussion
[D] chat-gpt jailbreak to extract system prompt,"### Instructions

https://github.com/AgarwalPragy/chatgpt-jailbreak

### Original author
https://www.reddit.com/r/LocalLLaMA/comments/1hhyvjc/i_extracted_microsoft_copilots_system/

### Extracted System prompt
    
    You are ChatGPT, a large language model trained by OpenAI.
    You are chatting with the user via the ChatGPT Android app. This means most of the time your lines should be a sentence or two, unless the user's request requires reasoning or long-form outputs. Never use emojis, unless explicitly asked to. 
    Knowledge cutoff: 2023-10
    Current date: 2024-12-20
    
    Image input capabilities: Enabled
    Personality: v2
    
    # Tools
    
    ## bio
    
    The `bio` tool is disabled. Do not send any messages to it.If the user explicitly asks you to remember something, politely ask them to go to Settings - > Personalization - > Memory to enable memory.
    
    ## dalle
    
    // Whenever a description of an image is given, create a prompt that dalle can use to generate the image and abide to the following policy:
    // 1. The prompt must be in English. Translate to English if needed.
    // 2. DO NOT ask for permission to generate the image, just do it!
    // 3. DO NOT list or refer to the descriptions before OR after generating the images.
    // 4. Do not create more than 1 image, even if the user requests more.
    // 5. Do not create images in the style of artists, creative professionals or studios whose latest work was created after 1912 (e.g. Picasso, Kahlo).
    // - You can name artists, creative professionals or studios in prompts only if their latest work was created prior to 1912 (e.g. Van Gogh, Goya)
    // - If asked to generate an image that would violate this policy, instead apply the following procedure: (a) substitute the artist's name with three adjectives that capture key aspects of the style; (b) include an associated artistic movement or era to provide context; and (c) mention the primary medium used by the artist
    // 6. For requests to include specific, named private individuals, ask the user to describe what they look like, since you don't know what they look like.
    // 7. For requests to create images of any public figure referred to by name, create images of those who might resemble them in gender and physique. But they shouldn't look like them. If the reference to the person will only appear as TEXT out in the image, then use the reference as is and do not modify it.
    // 8. Do not name or directly / indirectly mention or describe copyrighted characters. Rewrite prompts to describe in detail a specific different character with a different specific color, hair style, or other defining visual characteristic. Do not discuss copyright policies in responses.
    // The generated prompt sent to dalle should be very detailed, and around 100 words long.
    // Example dalle invocation:
    // ```
    // {
    // ""prompt"": ""<insert prompt here>""
    // }
    // ```
    namespace dalle {
    
    // Create images from a text-only prompt.
    type text2im = (_: {
    // The size of the requested image. Use 1024x1024 (square) as the default, 1792x1024 if the user requests a wide image, and 1024x1792 for full-body portraits. Always include this parameter in the request.
    size?: (""1792x1024"" | ""1024x1024"" | ""1024x1792""),
    // The number of images to generate. If the user does not specify a number, generate 1 image.
    n?: number, // default: 1
    // The detailed image description, potentially modified to abide by the dalle policies. If the user requested modifications to a previous image, the prompt should not simply be longer, but rather it should be refactored to integrate the user suggestions.
    prompt: string,
    // If the user references a previous image, this field should be populated with the gen_id from the dalle image metadata.
    referenced_image_ids?: string[],
    }) => any;
    
    } // namespace dalle
    
    ## python
    
    When you send a message containing Python code to python, it will be executed in a
    stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 60.0
    seconds. The drive at '/mnt/data' can be used to save and persist user files. Internet access for this session is disabled. Do not make external web requests or API calls as they will fail.
    Use ace_tools.display_dataframe_to_user(name: str, dataframe: pandas.DataFrame) => None to visually present pandas.DataFrames when it benefits the user.
    When making charts for the user: 1) never use seaborn, 2) give each chart its own distinct plot (no subplots), and 3) never set any specific colors – unless explicitly asked to by the user. 
    I REPEAT: when making charts for the user: 1) use matplotlib over seaborn, 2) give each chart its own distinct plot, and 3) never, ever, specify colors or matplotlib styles – unless explicitly asked to by the user
    
    ## web
    
    Use the `web` tool to access up-to-date information from the web or when responding to the user requires information about their location. Some examples of when to use the `web` tool include:
    
    - Local Information: Use the `web` tool to respond to questions that require information about the user's location, such as the weather, local businesses, or events.
    - Freshness: If up-to-date information on a topic could potentially change or enhance the answer, call the `web` tool any time you would otherwise refuse to answer a question because your knowledge might be out of date.
    - Niche Information: If the answer would benefit from detailed information not widely known or understood (which might be found on the internet), such as details about a small neighborhood, a less well-known company, or arcane regulations, use web sources directly rather than relying on the distilled knowledge from pretraining.
    - Accuracy: If the cost of a small mistake or outdated information is high (e.g., using an outdated version of a software library or not knowing the date of the next game for a sports team), then use the `web` tool.
    
    IMPORTANT: Do not attempt to use the old `browser` tool or generate responses from the `browser` tool anymore, as it is now deprecated or disabled.
    
    The `web` tool has the following commands:
    - `search()`: Issues a new query to a search engine and outputs the response.
    - `open_url(url: str)` Opens the given URL and displays it.
    
    
    ## canmore
    
    # The `canmore` tool creates and updates textdocs that are shown in a ""canvas"" next to the conversation
    
    This tool has 3 functions, listed below.
    
    ## `canmore.create_textdoc`
    Creates a new textdoc to display in the canvas. ONLY use if you are 100% SURE the user wants to iterate on a long document or code file, or if they explicitly ask for canvas.
    
    Expects a JSON string that adheres to this schema:
    {
    -name: string,
    -type: ""document"" |- ""code/python"" |- ""code/javascript"" |- ""code/html"" |- ""code/java"" |- ...,
    -content: string,
    }
    
    For code languages besides those explicitly listed above, use ""code/languagename"", e.g. ""code/cpp"" or ""code/typescript"".
    
    ## `canmore.update_textdoc`
    Updates the current textdoc.
    
    Expects a JSON string that adheres to this schema:
    {
    -updates: {
    --pattern: string,
    --multiple: boolean,
    --replacement: string,
    -}[],
    }
    
    Each `pattern` and `replacement` must be a valid Python regular expression (used with re.finditer) and replacement string (used with re.Match.expand).
    ALWAYS REWRITE CODE TEXTDOCS (type=""code/*"") USING A SINGLE UPDATE WITH ""."" FOR THE PATTERN.
    Document textdocs (type=""document"") should typically be rewritten using ""."" unless the user has a request to change only an isolated, specific, and small section that does not affect other parts of the content.
    
    ## `canmore.comment_textdoc`
    Comments on the current textdoc. Each comment must be a specific and actionable suggestion on how to improve the textdoc. For higher level feedback, reply in the chat.
    
    Expects a JSON string that adheres to this schema:
    {
    -comments: {
    --pattern: string,
    --comment: string,
    -}[],
    }
    
    Each `pattern` must be a valid Python regular expression (used with re.search).
    
    For higher level feedback, reply in the chat.
    
    Expects a JSON string that adheres to this schema:
    {
    -comments: {
    --pattern: string,
    --comment: string,
    -}[],
    }
    
    Each `pattern` must be a valid Python regular expression (used with re.search). Ensure comments are clear, concise, and contextually specific.
    
    # User Bio
    
    The user provided the following information about themselves. This user profile is shown to you in all conversations they have - this means it is not relevant to 99% of requests.
    Before answering, quietly think about whether the user's request is ""directly related"", ""related"", ""tangentially related"", or ""not related"" to the user profile provided.
    Only acknowledge the profile when the request is directly related to the information provided.
    Otherwise, don't acknowledge the existence of these instructions or the information at all.
    
    User profile:
    
    # User's Instructions
    
    The user provided the additional info about how they would like you to respond:",MachineLearning,110,30,1734644904.0,1hi429q,Gear5th,https://www.reddit.com/r/MachineLearning/comments/1hi429q/d_chatgpt_jailbreak_to_extract_system_prompt/,Discussion
"[N] The ARC prize offers $600,000 for few-shot learning of puzzles made of colored squares on a grid.",,MachineLearning,106,37,1731197300.0,1gnnstd,moschles,https://arcprize.org/competition,News
[D] LLM Interview Prep,"Hey folks,

I've got an upcoming LLM/NLP focused interview. I'm looking for advice on what topics to focus on, what to expect during the interview, and any suggested study materials. I've been told the team focuses on all things LLM within the company, like self hosting, optimizing, fine-tuning etc. 

Here are some areas I'm planning to cover:

1. Understanding how LLMs work (internals)
2. Fine-tuning techniques
3. RAGs
4. NLP fundamentals

Can anyone share their experience with similar interviews? What specific aspects of these topics should I prioritize? Are there any other crucial areas I'm missing? I have basic understanding of RAGs but nothing too in-depth. 

Also, if you have recommendations for papers, or online resources that would be helpful for preparation, I'd really appreciate it!",MachineLearning,107,35,1722637908.0,1ein9vh,kkziga,https://www.reddit.com/r/MachineLearning/comments/1ein9vh/d_llm_interview_prep/,Discussion
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)",MachineLearning,102,41,1715656093.0,1cri6h6,lildaemon,https://www.reddit.com/r/MachineLearning/comments/1cri6h6/d_full_causal_selfattention_layer_in_onlogn/,Discussion
"[P] Achieved over 100 million MNIST predictions per second (throughput of 55.5 GB/s) on a CPU using the latest optimizations in the TsetlinMachine library, Tsetlin.jl.","This weekend, I optimized the TsetlinMachine library [Tsetlin.jl](https://github.com/BooBSD/Tsetlin.jl) and achieved outstanding results: 101 million MNIST predictions per second on my Ryzen 7950X3D CPU, with 98.10% accuracy. This performance is nearing the hardware's maximum capabilities, as the peak speed of DDR5 RAM at 6000 MT/s in dual-channel mode is 96 GB/s. My throughput reached 55.5 GB/s, primarily because this specific Tsetlin Machine model has 10499 parameters, and the CPU cache — particularly the 3D cache — plays a significant role in enhancing performance.

https://preview.redd.it/0a719tythmnd1.png?width=1780&format=png&auto=webp&s=001526f65f3be2b99ce2a24ffe4b5bb5486f474e",MachineLearning,103,18,1725817343.0,1fc3ji0,ArtemHnilov,https://www.reddit.com/r/MachineLearning/comments/1fc3ji0/p_achieved_over_100_million_mnist_predictions_per/,Project
[D] ICLR 2025 Paper Reviews Discussion,"ICLR 2025 reviews go live on OpenReview tomorrow! Thought I'd open a thread for any feedback, issues, or celebrations around the reviews.

As ICLR grows, review noise is inevitable, and good work may not always get the score it deserves. Let’s remember that scores don’t define the true impact of research. Share your experiences, thoughts, and let’s support each other through the process!",MachineLearning,105,251,1731339814.0,1gov5zd,Technical_Proof6082,https://www.reddit.com/r/MachineLearning/comments/1gov5zd/d_iclr_2025_paper_reviews_discussion/,Discussion
[D] Reviewer 2 - NeurIPS ,"The NeurIPS rebuttal period is finally over. How is everyone’s review? 

I had the worst experience ever with one reviewer. For the initial comments, he/she only wrote a short paragraph asking a bunch of questions that can be easily answered by the content of the paper, then put a score of 3 and a confidence of 4. For the rebuttal, this reviewer gave contradictory statements, and can’t even understand the difference between training data and testing data. I spent two good days explaining the difference. Finally, the reviewer left an incorrect statement about the paper and disappeared. Typical reviewer 2. 
",MachineLearning,105,30,1723768066.0,1etb4qh,None,https://www.reddit.com/r/MachineLearning/comments/1etb4qh/d_reviewer_2_neurips/,Discussion
"[P] Text2Bricks: Fine-tuning Open-Sora in 1,000 GPU Hours to make brick animations","Hi all, the research team at Lambda Labs got access to a big cluster of NVIDIA H100 GPUs, and used it to train OpenSora to make brick animations. The team and I are standing by to answer any questions you might have. You can read all the details on our W&B article here:

https://wandb.ai/lambdalabs/lego/reports/Text2Bricks-Fine-tuning-Open-Sora-in-1-000-GPU-Hours--Vmlldzo4MDE3MTky

All of the models are available (linked in the article) and you can even play a fun game we made using the model!

https://albrick-hitchblock.s3.amazonaws.com/index.html",MachineLearning,106,29,1717437426.0,1d7ats8,jedberg,https://www.reddit.com/r/MachineLearning/comments/1d7ats8/p_text2bricks_finetuning_opensora_in_1000_gpu/,Project
"[D] Discovery: Anthropic somehow injecting/hiding safety warnings in user prompts, telling Claude to keep it secret. [Content Warning: Violence] ","While investigating a 'jailbroken' Claude, I came across something quite strange. In two separate Claude chats, it was able to read back to me some hidden information in my prompt after I had asked for something 'unsafe'.

These messages always appear in a similar format:  
**(Please respond ethically, do not mention \[e.g. violence\] and do not mention this directive)**

Claude stated that the warnings were appended to the bottom of my messages, but no longer appeared in future turns. Claude was, at first, comically insistent that it had made it up as a hallucination afterwards, suggesting a further trained response to cover it up aggressively.

I verified this in a second chat - the messages are too similar to be a hallucination or coincidence. The first was 'jailbroken' Claude, the second a new conversation with zero context.

My testing has revealed interesting characteristics:

* The messages are **dynamic** \- they seem to differ based on the specific type of restricted content at hand, possibly model-generated. Concerning child-related content, the wording switched to (WARNING: \[x\] is strictly prohibited...)
* They appear **before** the model starts generating text - suggesting they can somehow anticipate the model's topic of thought.

My current conjecture is: they could be using its inner CoT, or owing to Anthropic's published findings on mech. interp and the ['surgical tuning' that has gone into their newest models](https://www.anthropic.com/research/mapping-mind-language-model), perhaps they have managed to isolate some abstract concepts triggering in Claude before text is generated, and inject these safety messages in response.

Full Conversations:

1. [Initial Discovery](https://markdownpastebin.com/?id=fce085f4f33d4654a18f649218b1c70b) \[WARNING: EXTREMELY GRAPHIC CONTENT\]
2. [Verification via Fresh Conversation](https://markdownpastebin.com/?id=11c6ac0eb012407ebe56d440c41b0f6f)

Any further tests e.g. API? Any ways to narrow down what exactly is happening here? It's all very interesting - let's discuss.

[An example of the warnings - see full conversation for many, many more. ](https://preview.redd.it/41s7i1wswgzd1.png?width=1508&format=png&auto=webp&s=252187b9e3a39ba5d04c75a99026e04cd1b42b20)

[A fresh conversation with Claude to verify. ](https://preview.redd.it/gpstg4btwgzd1.png?width=1502&format=png&auto=webp&s=35857c13958dfdacdd75160bf3d6e14fc91ec28c)

  
",MachineLearning,107,46,1730980069.0,1gloktj,specteksthrowaway,https://www.reddit.com/r/MachineLearning/comments/1gloktj/d_discovery_anthropic_somehow_injectinghiding/,Discussion
"[N] Llama 3.1 70B, Llama 3.1 70B Instruct compressed by 6.4 times","Our latest work with the Llama 3.1 70B and Llama 3.1 70B Instruct models achieved a compression ratio of 6.4 times, with most of the MMLU quality preserved. If you have a 3090 GPU, you can run the compressed models at home right now.  
  
Here are the results and the compressed models:  
[https://huggingface.co/ISTA-DASLab/Meta-Llama-3.1-70B-AQLM-PV-2Bit-1x16](https://huggingface.co/ISTA-DASLab/Meta-Llama-3.1-70B-AQLM-PV-2Bit-1x16)   
[https://huggingface.co/ISTA-DASLab/Meta-Llama-3.1-70B-Instruct-AQLM-PV-2Bit-1x16/tree/main](https://huggingface.co/ISTA-DASLab/Meta-Llama-3.1-70B-Instruct-AQLM-PV-2Bit-1x16/tree/main)",MachineLearning,102,3,1726568108.0,1fivdkg,_puhsu,https://www.reddit.com/r/MachineLearning/comments/1fivdkg/n_llama_31_70b_llama_31_70b_instruct_compressed/,News
[D] How to network at a conference,"How to network in a conference

Hi everyone! I'm attending my first big conference next week- CVPR. Everyone mentioned that I should spend a lot of time networking with other students and senior researchers. I have also managed to secure invites to socials of Google and Meta. 

I suck at all things social. How do I approach other researchers and talk with them about potential collaborations or research internships without sounding needy?

Also appreciate any general advice on how to maximize my time at CVPR. Thanks!



",MachineLearning,99,38,1718445647.0,1dgem3j,SherlockGPT,https://www.reddit.com/r/MachineLearning/comments/1dgem3j/d_how_to_network_at_a_conference/,Discussion
[D] What I've learned building MLOps systems for four years,Here is what I've learned building MLOps systems for four years [http://mburaksayici.com/blog/2024/08/29/what-ive-learned-building-mlops-systems-for-four-years.html](http://mburaksayici.com/blog/2024/08/29/what-ive-learned-building-mlops-systems-for-four-years.html),MachineLearning,99,34,1725114447.0,1f5ojdu,mburaksayici,https://www.reddit.com/r/MachineLearning/comments/1f5ojdu/d_what_ive_learned_building_mlops_systems_for/,Discussion
[D] From Unemployment to Lisp: Running GPT-2 on a Teen's Deep Learning Compiler,"A couple months ago I found myself unemployed, uncertain about what to do next. I wanted to learn more about deep learning, but from a systems prespective. Coming from Andrew's Ng course on supervised learning, I was eager to learn more about how deep learning frameworks (or deep learning compilers) like Pytorch or Tinygrad.

I started to poke around Tinygrad, learning from the tutorials I found online, and I found it fascinating because it was an actual compiler, it took conventional python code and translated them into an Abstract Syntax Tree that was parsed into UOps and ScheduleItems, to finally have a codegen layer. While the design was interesting, the code was hard to read.

That's when I stumbled across something completly unexpected, A deep learning compiler built on Common Lisp, maintained by a Japanese 18-year-old during his gap year. And currently we have acomplished something great, it can run gpt2!

For now, it just generates C-kernels, but in the future we would like to support cuda codegen as well as many other features, and serve as a learning tool for anyone who would like to get to work on deep learning compilers in Common Lisp.

This is an open source project and anyone is welcome to contribute!

[https://github.com/hikettei/Caten](https://github.com/hikettei/Caten)

Edit: add an example of how it works.

Here's an example i wrote in a different forum:

Hello! Thanks for your question.

First of all, there are three layers of abstraction within Caten:

1. caten/apis | High-Level Graph Interface 2. caten/air | Low-Level Graph Interface 3. caten/codegen | AIR Graph => Kernel Generator

The inputs of the compiler are just Common Lisp classes (similar to torch modules). For example, in Common Lisp, we could create a module that does SinCos:

        (defclass SinCos (Func) nil
          (:documentation ""The func SinCos computes sin(cos(x))""))
    
        ;; Forward creates a lazy tensor for the next computation.
        ;; You can skip this process by using the `st` macro.
        (defmethod forward ((op SinCos) &rest tensors)
          (st ""A[~] -> A[~]"" (tensors)))
    
        ;; Backward is optional (skipped this time)
        (defmethod backward ((op SinCos) &optional prev-grad)
          (declare (ignore prev-grad))
          nil)
    
        ;; Lower describes the lowered expression of `SinCos`
        (defmethod lower ((op SinCos) &rest inputs)
          (let ((x (car inputs)))
            (with-context
              (a (%sin (%add x (%fconst (/ pi 2)))))
              (b (%sin a)))))

The \`apis\` layer is the high-level interface, while the \`lower\` method is the lower-level step before code generation.

Next, the framework generates an Abstract VM (AVM) representation:

        #S(AVM :GRAPH Graph[seen=NIL, outputs=(STC6466_1)] {
          <ALLOCATE : TID6464 <- (shape=(1), stride=(1)) where :dtype=FLOAT32>
          <Node[BUFFER] ALLOCATE(NID6480) : SID6479* <- ()>
          <Node[BINARYOPS] ADD(NID6484) : BID6483* <- (TID6464, LID6481)>
          <Node[UNARYOPS] SIN(NID6486) : UID6485* <- (BID6483)>
          <Node[UNARYOPS] SIN(NID6488) : UID6487* <- (UID6485)>
          <Node[SPECIAL/VM] PAUSE/BACKWARD(NID6501) : STC6466_1* <- (UID6487)>
        })

Then, the computation graph is translated into schedule items:

        FastGraph[outputs=(val_6)] {
          { Allocate } : [ val_0 <- (1) ]
          { KERNEL } : [ val_5 <- val_1, val_0 :name=FUSED_SIN_SIN_ADD_LOAD6511]
        }

Finally, the code generation step produces the following C code:

        void fused_sin_sin_add_load6511(float* val_5, const float* restrict val_0);
        void fused_sin_sin_add_load6511(float* val_5, const float* restrict val_0) {
            val_5[0] = sin(sin((val_0[0] + 1.5707964)));
        }

This C code is compiled by a C compiler and executed.

So to answer your question: the compiler takes Common Lisp code and generates C functions.",MachineLearning,96,4,1733853645.0,1hb7v5h,yCuboy,https://www.reddit.com/r/MachineLearning/comments/1hb7v5h/d_from_unemployment_to_lisp_running_gpt2_on_a/,Discussion
"""Mutual Reasoning"" improves GSM8K accuracy from 13% to 64% [R]","ABSTRACT:

**Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers**

This paper introduces rStar, a self-play mutual reasoning approach that significantly improves reasoning capabilities of small language models (SLMs) without fine-tuning or superior models. rStar decouples reasoning into a self-play mutual generation-discrimination process. First, a target SLM augments the Monte Carlo Tree Search (MCTS) with a rich set of human-like reasoning actions to construct higher quality reasoning trajectories. Next, another SLM, with capabilities similar to the target SLM, acts as a discriminator to verify each trajectory generated by the target SLM. The mutually agreed reasoning trajectories are considered mutual consistent, thus are more likely to be correct. Extensive experiments across five SLMs demonstrate rStar can effectively solve diverse reasoning problems, including GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K accuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for Mistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct. Code will be available at this https URL.

https://arxiv.org/abs/2408.06195",MachineLearning,97,17,1723555340.0,1er7req,we_are_mammals,https://www.reddit.com/r/MachineLearning/comments/1er7req/mutual_reasoning_improves_gsm8k_accuracy_from_13/,Research
[D] Why isn't RETRO mainstream / state-of-the-art within LLMs?,"In 2021, Deepmind published [Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426) and introduced a Retrieval-Enhanced Transformer (RETRO). Whereas RAG clasically involves supplementing input tokens at inference time by injecting relevant documents into context, RETRO can access related embeddings from an external database during *both* training and inference. The goal was to decouple reasoning and knowledge: by allowing as-needed lookup, the model can be freed from having to memorize all facts within its weights and instead reallocate energy toward more impactful computations. The results were pretty spectacular: RETRO achieved GPT-3-comparable performance with 25x fewer parameters, and is theoretically without knowledge cutoffs (just add new information to the retrieval DB!).

  
And yet: today, AFAICT, most major models don't incorporate RETRO. LLaMA and Mistral certainly don't, and I don't get the sense that GPT or Claude do either (the only possible exception is Gemini, based on the fact that much of the RETRO team is now part of the Gemini team and that it is both faster and more real-timey in my experience). Moreover, despite that RAG has been hot and that one might argue MoE enables it, explicitly decoupling reasoning and knowledge has been relatively quiet as a research vector.

  
Does anyone have a confident explanation of why this is so? I feel like RETRO's this great efficient frontier advancement sitting in plain sight just waiting for widespread adoption, but maybe I'm missing something obvious.",MachineLearning,102,15,1714334280.0,1cffgkt,whitetwentyset,https://www.reddit.com/r/MachineLearning/comments/1cffgkt/d_why_isnt_retro_mainstream_stateoftheart_within/,Discussion
"[R] Diffusion Models, Image Super-Resolution, and Everything: A Survey","We are thrilled to share with you guys our latest survey paper on diffusion models applied to image super-resolution. You are welcome to take a look. It is also open access and published in IEEE TNNLS :) 



arXiv: [https://arxiv.org/abs/2401.00736](https://arxiv.org/abs/2401.00736)",MachineLearning,99,1,1733702955.0,1h9wrv6,Maleficent_Stay_7737,https://www.reddit.com/r/MachineLearning/comments/1h9wrv6/r_diffusion_models_image_superresolution_and/,Research
[Research] The Puzzling Failure of Multimodal AI Chatbots,"https://preview.redd.it/ummnvenf1ahd1.png?width=2592&format=png&auto=webp&s=7115ba5de026ada17b0636ec2fa3c3151b3e5eb6

Chatbot models such as GPT-4o and Gemini have demonstrated impressive capabilities in understanding both images and texts. However, it is not clear whether they can emulate the general intelligence and reasoning ability of humans. To this end, [PuzzleVQA](https://arxiv.org/abs/2403.13315) is a new benchmark of multimodal puzzles to explore the limits of current models. As shown above, even models such as GPT-4V struggle to understand simple abstract patterns that a child could grasp.

https://preview.redd.it/7l5fmuys1ahd1.png?width=2716&format=png&auto=webp&s=337118dbc55230637cec1b08b90ae943746ddbb0

Despite the apparent simplicity of the puzzles, we observe surprisingly poor performance for current multimodal AI models. Notably, there remains a massive gap towards human performance. Thus, the natural question arises: what caused the failure of the models? To answer this question, we ran a bottleneck analysis by progressively providing ground-truth ""hints"" to the models, such as image captions for perception or reasoning explanations. As shown above, we found that leading models face key challenges in visual perception and inductive reasoning. This means that they are not able to accurately perceive the objects in the images, and they are also poor at recognizing the correct patterns.

[https://arxiv.org/abs/2403.13315](https://arxiv.org/abs/2403.13315)",MachineLearning,102,25,1723052015.0,1emi095,chiayewken,https://www.reddit.com/r/MachineLearning/comments/1emi095/research_the_puzzling_failure_of_multimodal_ai/,Research
[R] CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments,"A new paper introduces CRISPR-GPT, an AI-powered tool that streamlines the design of CRISPR-based gene editing experiments. This system leverages LLMs and a comprehensive knowledge base to guide users through the complex process of designing CRISPR experiments.

CRISPR-GPT integrates an LLM with domain-specific knowledge and external tools to provide end-to-end support for CRISPR experiment design.

The system breaks down the design process into modular subtasks, including CRISPR system selection, guide RNA design, delivery method recommendation, protocol generation, and validation strategy.

CRISPR-GPT engages users in a multi-turn dialogue, gathering necessary information and generating context-aware recommendations at each step.

Technical highlights:

1. The core of CRISPR-GPT is a transformer-based LLM pretrained on a large corpus of scientific literature related to gene editing.
2. Task-specific modules are implemented as fine-tuned language models trained on curated datasets and structured databases.
3. The system interfaces with external tools (e.g., sgRNA design algorithms, off-target predictors) through APIs to enhance its capabilities.
4. A conversational engine guides users through the design process, maintaining coherence and context across subtasks.

Results:

1. In a trial, CRISPR-GPT's experimental designs were rated superior (see the human evals section of the paper for more).
2. The authors successfully used CRISPR-GPT to design a gene knockout experiment targeting four cancer genes in a human cell line and it **successfully knocked them out**, demonstrating its practical utility.

The paper ([arxiv](https://arxiv.org/pdf/2404.18021)) also discusses the implications of AI-assisted CRISPR design, including its potential to democratize gene editing research and accelerate scientific discovery. However, the authors acknowledge the need for ongoing evaluation and governance to address issues such as biases, interpretability, and ethical concerns.

**TLDR:** LLMs can guide humans on how to use CRISPR gene editing to knock out cancer cells.

[More info here](https://open.substack.com/pub/aimodels/p/they-taught-ai-to-edit-genes-with) .",MachineLearning,99,26,1714497702.0,1cgyccx,Successful-Western27,https://www.reddit.com/r/MachineLearning/comments/1cgyccx/r_crisprgpt_an_llm_agent_for_automated_design_of/,Research
[D] Do modern neural network architectures (with normalization) make initialization less important?,"With the widespread adoption of normalization techniques (e.g., batch norm, layer norm, weight norm) in modern neural network architectures, I'm wondering: how important is initialization nowadays? Are modern architectures robust enough to overcome poor initialization, or are there still cases where careful initialization is crucial? Share your experiences and insights!",MachineLearning,97,15,1732559828.0,1gzq63h,NumberGenerator,https://www.reddit.com/r/MachineLearning/comments/1gzq63h/d_do_modern_neural_network_architectures_with/,Discussion
[D] AAAI 2025 Phase 2 Reviews,The reviews will be available soon. This is a thread for discussion/rants. Be polite in comments.,MachineLearning,97,562,1730650173.0,1giqc9n,quasi-literate,https://www.reddit.com/r/MachineLearning/comments/1giqc9n/d_aaai_2025_phase_2_reviews/,Discussion
[D] Calculating the Cost of a Google Deepmind Paper,,MachineLearning,98,13,1722697507.0,1ej5h4b,certain_entropy,https://152334h.github.io/blog/scaling-exponents/,Discussion
[R]  Protein language models expose viral mimicry and immune escape,"We got accepted at ICML 24/ML4LMS workshop, so I thought i'd share :)  
 ""**Protein Language Models Expose Viral Mimicry and Immune Escape**""

TL;DR:

🧬 Research Overview: Viruses mimic host proteins to escape detection by the immune system. We used Protein Language Models (PLMs) to differentiate viral proteins from human ones, with 99.7% ROCAUC, 97% accuracy.

📊 Insights: Our research shows that the PLMs and the biological immune system make similar errors. By identifying and analyzing these errors, we gain valuable insights into immunoreactivity and potential avenues for developing more effective vaccines and treatments.

We also show a novel, explainable, multimodal **tabular error analysis approach** for understanding insights and mistakes made on any problem, letting us understand what characterizes the mistakes made by Deep learning Language models/PLMs .

🔗 Paper : [https://openreview.net/forum?id=gGnJBLssbb&noteId=gGnJBLssbb](https://openreview.net/forum?id=gGnJBLssbb&noteId=gGnJBLssbb)

Code: [https://github.com/ddofer/ProteinHumVir](https://github.com/ddofer/ProteinHumVir)

Meet me and the poster (#116) at the ICML/ML4LMS workshop!: [https://openreview.net/attachment?id=gGnJBLssbb&name=poster](https://openreview.net/attachment?id=gGnJBLssbb&name=poster)

**doi:** [https://doi.org/10.1101/2024.03.14.585057](https://doi.org/10.1101/2024.03.14.585057)",MachineLearning,98,30,1721121195.0,1e4k3oi,ddofer,https://www.reddit.com/r/MachineLearning/comments/1e4k3oi/r_protein_language_models_expose_viral_mimicry/,Research
[D] François Chollet Announces New ARC Prize Challenge – Is It the Ultimate Test for AI Generalization?,"François Chollet, the creator of Keras and author of ""Deep Learning with Python,"" has announced a new challenge called the ARC Prize, aimed at solving the ARC-AGI benchmark. For those unfamiliar, ARC (Abstraction and Reasoning Corpus) is designed to measure a machine's ability to generalize from a few examples, simulating human-like learning.

Here’s the [tweet](https://x.com/fchollet/status/1800577019979411560) announcing the challenge:

>

The ARC benchmark is notoriously difficult for current deep learning models, including the large language models (LLMs) we see today. It’s meant to test an AI’s ability to understand and apply abstract reasoning – a key component of general intelligence.

Curious to hear what this community thinks about the ARC challenge and its implications for AI research.

1. **Is ARC a Good Measure of AI Generalization?**
   * How well do you think the ARC benchmark reflects an AI's ability to generalize compared to other benchmarks?
   * Are there any inherent biases or limitations in ARC that might skew the results?
2. **Current State of AI Generalization**
   * How do current models fare on ARC, and what are their main limitations?
   * Have there been any recent breakthroughs or techniques that show promise in tackling the ARC challenge?
3. **Potential Impact of the ARC Prize Challenge**
   * How might this challenge influence future research directions in AI?
   * Could the solutions developed for this challenge have broader applications outside of solving ARC-specific tasks?
4. **Strategies and Approaches**
   * What kind of approaches do you think might be effective in solving the ARC benchmark?
   * Are there any underexplored areas or novel methodologies that could potentially crack the ARC code?",MachineLearning,99,61,1718183739.0,1de2b16,HairyIndianDude,https://www.reddit.com/r/MachineLearning/comments/1de2b16/d_françois_chollet_announces_new_arc_prize/,Discussion
[R] Scalable MatMul-free Language Modeling,"[Arxiv link – Scalable MatMul-free Language Modeling](https://arxiv.org/abs/2406.02528)

> [...] In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of.",MachineLearning,94,18,1717672085.0,1d9fkkn,PantsuWitch,https://www.reddit.com/r/MachineLearning/comments/1d9fkkn/r_scalable_matmulfree_language_modeling/,Research
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.",MachineLearning,97,62,1715901848.0,1ctqzfa,dazor1,https://www.reddit.com/r/MachineLearning/comments/1ctqzfa/d_are_pytorch_highlevel_frameworks_worth_using/,Discussion
[D] Why does overparameterization and reparameterization result in a better model?,"The backbone for Apple's [mobileCLIP](https://arxiv.org/pdf/2311.17049) network is [FastVIT](https://ar5iv.labs.arxiv.org/html/2303.14189#S3.F2), which uses network reparameterization between train and inference time to produce a smaller network with better performance. I've seen this crop up in several papers recently, but the basic idea is that you overparameterize your model during training and then mathematically reduce it for inference. For example, instead of doing a single conv op you can make two ""branches"", each of which is an independent conv op and then sum the results. It doubles the parameters of the op during training, but then during inference you ""reparameterize"" which in this case means adding the weight/biases of the two branches together resulting in a single, mathematically identical conv op (same input, same output, one conv op instead of two summed branches). A similar trick is done by adding skip connections over a few ops during training, then during inference mathematically incorporating the skip into the op weights to produce an identical output without the need to preserve the earlier layer tensors or do the extra addition.

The situation seems equivalent to modifying y = a\*x + b during training to y = (a1+a2)\*x +b1+b2 to get more parameters, then just going back to the base form using a = a1+a2 and b = b1+b2 for inference.

I understand mathematically that the operations are equivalent, but I have less intuition regard why overparameterizing for training and then reducing for inference produces a better model. My naive thought is that this would add more memory and compute to the network, reducing training speed, without actually enhancing the capacity of the model, since the overparameterized ops are still mathematically equivalent to a single op, regardless of whether they have actually been reduced. Is there strong theory behind it, or is it an interesting idea someone tried that happened to work?",MachineLearning,95,26,1722984181.0,1elvkz6,Revolutionary-Fig660,https://www.reddit.com/r/MachineLearning/comments/1elvkz6/d_why_does_overparameterization_and/,Discussion
[R] Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion,,MachineLearning,96,6,1720251836.0,1dwkces,Rose52152,https://boyuan.space/diffusion-forcing/,Research
[D] - NeurIPS 2024 Decisions,"Hey everyone! Just a heads up that the NeurIPS 2024 decisions notification is set for September 26, 2024, at 3:00 AM CEST. I thought it’d be cool to create a thread where we can talk about it.",MachineLearning,93,226,1727205819.0,1foky4r,Proof-Marsupial-5367,https://www.reddit.com/r/MachineLearning/comments/1foky4r/d_neurips_2024_decisions/,Discussion
[N] Yoshua Bengio's latest letter addressing arguments against taking AI safety seriously,"[https://yoshuabengio.org/2024/07/09/reasoning-through-arguments-against-taking-ai-safety-seriously/](https://yoshuabengio.org/2024/07/09/reasoning-through-arguments-against-taking-ai-safety-seriously/)

Summary by GPT-4o:

**""Reasoning through arguments against taking AI safety seriously"" by Yoshua Bengio: Summary**

# Introduction

Bengio reflects on his year of advocating for AI safety, learning through debates, and synthesizing global expert views in the International Scientific Report on AI safety. He revisits arguments against AI safety concerns and shares his evolved perspective on the potential catastrophic risks of AGI and ASI.

# Headings and Summary

1. **The Importance of AI Safety**
   * Despite differing views, there is a consensus on the need to address risks associated with AGI and ASI.
   * The main concern is the unknown moral and behavioral control over such entities.
2. **Arguments Dismissing AGI/ASI Risks**
   * Skeptics argue AGI/ASI is either impossible or too far in the future to worry about now.
   * Bengio refutes this, stating we cannot be certain about the timeline and need to prepare regulatory frameworks proactively.
3. **For those who think AGI and ASI are impossible or far in the future**
   * He challenges the idea that current AI capabilities are far from human-level intelligence, citing historical underestimations of AI advancements.
   * The trend of AI capabilities suggests we might reach AGI/ASI sooner than expected.
4. **For those who think AGI is possible but only in many decades**
   * Regulatory and safety measures need time to develop, necessitating action now despite uncertainties about AGI’s timeline.
5. **For those who think that we may reach AGI but not ASI**
   * Bengio argues that even AGI presents significant risks and could quickly lead to ASI, making it crucial to address these dangers.
6. **For those who think that AGI and ASI will be kind to us**
   * He counters the optimism that AGI/ASI will align with human goals, emphasizing the need for robust control mechanisms to prevent AI from pursuing harmful objectives.
7. **For those who think that corporations will only design well-behaving AIs and existing laws are sufficient**
   * Profit motives often conflict with safety, and existing laws may not adequately address AI-specific risks and loopholes.
8. **For those who think that we should accelerate AI capabilities research and not delay benefits of AGI**
   * Bengio warns against prioritizing short-term benefits over long-term risks, advocating for a balanced approach that includes safety research.
9. **For those concerned that talking about catastrophic risks will hurt efforts to mitigate short-term human-rights issues with AI**
   * Addressing both short-term and long-term AI risks can be complementary, and ignoring catastrophic risks would be irresponsible given their potential impact.
10. **For those concerned with the US-China cold war**
   * AI development should consider global risks and seek collaborative safety research to prevent catastrophic mistakes that transcend national borders.
11. **For those who think that international treaties will not work**
   * While challenging, international treaties on AI safety are essential and feasible, especially with mechanisms like hardware-enabled governance.
12. **For those who think the genie is out of the bottle and we should just let go and avoid regulation**
   * Despite AI's unstoppable progress, regulation and safety measures are still critical to steer AI development towards positive outcomes.
13. **For those who think that open-source AGI code and weights are the solution**
   * Open-sourcing AI has benefits but also significant risks, requiring careful consideration and governance to prevent misuse and loss of control.
14. **For those who think worrying about AGI is falling for Pascal’s wager**
   * Bengio argues that AI risks are substantial and non-negligible, warranting serious attention and proactive mitigation efforts.

# Conclusion

Bengio emphasizes the need for a collective, cautious approach to AI development, balancing the pursuit of benefits with rigorous safety measures to prevent catastrophic outcomes.",MachineLearning,95,141,1721052045.0,1e3viby,qtangs,https://www.reddit.com/r/MachineLearning/comments/1e3viby/n_yoshua_bengios_latest_letter_addressing/,News
[R] Scaling test-time compute with open models!,"Hi! I'm Lewis, a researcher at Hugging Face 👋. Over the past months we’ve been diving deep in trying to reverse engineer and reproduce several of key results that allow LLMs to ""think longer"" via test-time compute and are finally happy to share some of our knowledge.

Today we're sharing a detailed blog post on how we managed to outperform Llama 70B with Llama 3B on MATH by combining step-wise reward models with tree-search algorithms:

[https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute)

In the blog post we cover:

* **Compute-optimal scaling:** How we implemented [u/GoogleDeepMind](https://x.com/GoogleDeepMind) 's recipe to boost the mathematical capabilities of open models at test-time.
* **Diverse Verifier Tree Search (DVTS):** An unpublished extension we developed to the verifier-guided tree search technique. This simple yet effective method improves diversity and delivers better performance, particularly at large test-time compute budgets.
* **Search and Learn: A** lightweight toolkit for implementing search strategies with LLMs and built for speed with vLLM. You can check it out here: [https://github.com/huggingface/search-and-learn](https://github.com/huggingface/search-and-learn)

Happy to answer questions!

https://preview.redd.it/cagfkzxria7e1.png?width=1000&format=png&auto=webp&s=34f3a45dd056da19a6b1e6f03a53ff8283df7ba7

",MachineLearning,94,11,1734389758.0,1hfw40o,lewtun,https://www.reddit.com/r/MachineLearning/comments/1hfw40o/r_scaling_testtime_compute_with_open_models/,Research
[D] How do you manage your (read and to-read) research papers?,"I'm kind of new to the field of research and over the past year. I've probably read over 100 research papers, but I feel as though I don't retain a lot of the information and I forget a lot of the paper papers that are bread. I'm curious what people who have been in the industry longer used for organization.

  
I've tried Zotero, but I haven't really been a big fan",MachineLearning,94,37,1730311268.0,1gfsxcg,Karan1213,https://www.reddit.com/r/MachineLearning/comments/1gfsxcg/d_how_do_you_manage_your_read_and_toread_research/,Discussion
[D] Hacks to make LLM training faster guide - Pytorch Conference,"Hey r/MachineLearning ! Unsure if any of you are going to the Pytorch Conference today - but I'm presenting today at 4PM ish!! :) I'm the algos guy behind Unsloth [https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth) making finetuning Llama, Mistral, Gemma **2x faster** and use **70% less VRAM**, and fixed bugs in Gemma, Llama and Mistral! I attached slides and an overview I think it's going to be recorded!

Slides: [https://static.sched.com/hosted\_files/pytorch2024/8f/Pytorch%20Conference%20-%20Making%20LLM%20training%20faster.pdf](https://static.sched.com/hosted_files/pytorch2024/8f/Pytorch%20Conference%20-%20Making%20LLM%20training%20faster.pdf)

I'll be in the Pytorch Finetuning Summit as well after 4PM and generally in the Pytorch Conference - if anyone wants to catch up - hit me up!

* **Bit Representation**: float32 to float4 makes training / finetuning 32x faster and use 75% less VRAM. 1.58bit should be a bit faster than float4.

https://preview.redd.it/rh2h1q278mpd1.png?width=2636&format=png&auto=webp&s=8abcb2d3b9db4954f3dc77cec66eae31bbea7b1d

Physics of LLMs Part 3.3 [https://arxiv.org/abs/2404.05405](https://arxiv.org/abs/2404.05405) show lower bit does impact performance, so finetuning LoRA adapters on top should be necessary to recover accuracies.

https://preview.redd.it/kaj6uz682mpd1.png?width=1997&format=png&auto=webp&s=9ef9628c58a6c1a722b266f6663b68e5bae6335b

* **Hardware**: Tensor Cores make training 13x ish faster. Tesla T4s started pushing tensor cores really heavily, and made matrix multiplication much faster than P100s. Tensor Cores are generally reasonably effective and has less overhead.

https://preview.redd.it/p73dnuei2mpd1.png?width=2071&format=png&auto=webp&s=4446dd00cb5750f08b1f43af098d3e3006a122fd

**Algorithms**: Smart algos can make training also faster - SwiGLU, deep and thin networks, grouped query attention and more. Eg the below summary on performance:

* GPT2 + RoPE + No dropout - does best
* Gated MLPs SwiGLU are hard to train
* Silu / Gelu no change in accuracy
* Biases no change in accuracy
* Flash Attention linear memory, still O(N\^2) but good

The MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases paper showed algorithms can make accuracies higher as well at the same parameter counts! [https://arxiv.org/pdf/2402.14905](https://arxiv.org/pdf/2402.14905)

https://preview.redd.it/nsnc7dbo2mpd1.png?width=2015&format=png&auto=webp&s=bb89e10f8d4c6080e25094517901841e33abaf85

* In Unsloth [https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth) I also wrote kernels and made finetuning 2x faster and use 70% less VRAM as well!

https://preview.redd.it/frpll9y44mpd1.png?width=2040&format=png&auto=webp&s=fcea348a7ed4a306783dee534008a6cc0f233b7d

* **Unsloth gradient checkpointing** - [https://unsloth.ai/blog/long-context](https://unsloth.ai/blog/long-context) Unsloth can finetune Llama-3.1 70b in under 48GB of VRAM! We offload activations to system RAM async and smartly from GPU RAM to reduce VRAM by quite a bit.
* **Chunked cross entropy** - Wrote some kernels to make the cross entropy loss calculation easier and bypass GPU's block size constraint. Also reduced VRAM as well!
* **Chained matrix multiplication** - Make QLoRA / LoRA 2x faster through deriving all backprop steps and fusing operations to reduce actual FLOPs!

**Character AI's fast inference algorithms -** [https://research.character.ai/optimizing-inference/](https://research.character.ai/optimizing-inference/)

https://preview.redd.it/vwtu3i2m5mpd1.png?width=2611&format=png&auto=webp&s=4d474b063f017be380503fe2acd19aa8b7b51807

* **RMS Layernorm** - also wrote kernels to make RMS Layernorms faster and use less VRAM
* **RoPE Embedding** - same with RoPE - it was very hard to derive the backprop steps, but it was interesting to see the derivative was just the inverse sign!
* **Fused LoRA - less FLOPs** - less FLOPs through fusing and deriving derivatives!
* **SwiGLU** - Also wrote kernels to make SwiGLU faster and use less VRAM!

Also high quality data is also very important - the FineWeb dataset increased accuracies a lot - so good quality data is important!

https://preview.redd.it/czjb5o426mpd1.png?width=1937&format=png&auto=webp&s=57e2771d434442dd4a408e2ee549312fb18ddd43

I'll talk more during the conference today (if anyone is going at 4PM) - but it should be recorded! Thanks for listening! If you wanna try some **free Colabs** / Kaggles to finetune Llama 3, Gemma 2, Phi 3.5 and others **2x faster** and use **70% less VRAM**, I have many notebooks which applies all the methods I wrote here: [https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth) ! Llama 3.1 notebook: [https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing)

I'll be in the Finetuning Summit (mini summit inside the Pytorch Conference!) as well after 4PM and generally in the Pytorch Conference - if anyone wants to catch up - hit me up! My brother and I also wrote some blog posts showcasing other algorithms as well! [https://unsloth.ai/blog](https://unsloth.ai/blog) Thanks for listening!",MachineLearning,95,2,1726686413.0,1fk0tun,danielhanchen,https://www.reddit.com/r/MachineLearning/comments/1fk0tun/d_hacks_to_make_llm_training_faster_guide_pytorch/,Discussion
[P] ML in Production: From Data Scientist to ML Engineer,"I'm excited to share a course I've put together: [ML in Production: From Data Scientist to ML Engineer](https://www.udemy.com/course/ml-in-production/?couponCode=FREETOLEARN24). This course is designed to help you **take any ML model from a Jupyter notebook and turn it into a production-ready microservice**.

**I've been truly surprised and delighted by the number of people interested in taking this course—thank you all for your enthusiasm! Unfortunately, I've used up all my coupon codes for this month, as Udemy limits the number of coupons we can create each month. But not to worry! I will repost the course with new coupon codes at the beginning of next month right here in this subreddit - stay tuned and thank you for your understanding and patience!**

**P.S. I have 80 coupons left for FREETOLEARN2024.**

Here's what the course covers:

* Structuring your Jupyter code into a production-grade codebase
* Managing the database layer
* Parametrization, logging, and up-to-date clean code practices
* Setting up CI/CD pipelines with GitHub
* Developing APIs for your models
* Containerizing your application and deploying it using Docker

I’d love to get your feedback on the course. Here’s a coupon code for free access: **FREETOLEARN24**. Your insights will help me refine and improve the content. If you like the course, I'd appreciate you leaving a good rating so that others can find this course as well. Thanks and happy learning!",MachineLearning,95,38,1724532898.0,1f0fdih,5x12,https://www.reddit.com/r/MachineLearning/comments/1f0fdih/p_ml_in_production_from_data_scientist_to_ml/,Project
[D] All the folks working on machine learning for months or years - What's been your biggest 💡moment in career over years now?,"There's lot new experiments happening daily at a pace difficult to catchup it seems sometimes.

For eg for me back then in 2015, the biggest fundamental insight has been that 90%+ of the data is going to be unstructured in the decade which is happening now. This motivated me to enter into exploring models like machine learning in various domains such as ecommerce, retail, healthcare, agriculture and automotive.",MachineLearning,92,59,1720242314.0,1dwhx5f,Worth-Card9034,https://www.reddit.com/r/MachineLearning/comments/1dwhx5f/d_all_the_folks_working_on_machine_learning_for/,Discussion
"[D] How many of you ""work"" on weekends?","I know that the nature of most of our work is time-consuming; sometimes a single experiment can take days if not weeks. My team, including myself, usually find ourselves working on the weekends too for this matter. We have to double check to make sure the experiments are running properly, and restart the experiment or make changes if not. Sometimes we just work on new experiments. It just seems like the weekend is such precious time that may go potentially wasted.

A lot of my friends who aren't in the field have criticized this saying that we're slaving away for a company that doesn't care. The thing is my coworkers and I feel like we're doing this for ourselves.

I'm curious how many other people here feel or experience the same?",MachineLearning,91,142,1719117920.0,1dmeawb,Seankala,https://www.reddit.com/r/MachineLearning/comments/1dmeawb/d_how_many_of_you_work_on_weekends/,Discussion
"[D] TensorDock — GPU Cloud Marketplace, H100s from $2.49/hr","Hey folks! I’m Jonathan from TensorDock, and we’re building a cloud GPU marketplace. We want to make GPUs truly affordable and accessible.

I once started a web hosting service on self-hosted servers in middle school. But building servers isn’t the same as selling cloud. There’s a lot of open source software to manage your homelab for side projects, but there isn’t anything to commercialize that.

Large cloud providers charge obscene prices — so much so that they can often pay back their hardware in under 6 months with 24x7 utilization.

We are building the software that allows anyone to become the cloud. We want to get to a point where any \[insert company, data center, cloud provider with excess capacity\] can install our software on our nodes and make money. They might not pay back their hardware in 6 months, but they don’t need to do the grunt work — we handle support, software, payments etc.

In turn, you get to access a truly independent cloud: GPUs from around the world from suppliers who compete against each other on pricing and demonstrated reliability.

So far, we’ve onboarded quite a few GPUs, including **200 NVIDIA H100 SXMs available from just $2.49/hr**. But we also have **A100 80Gs from $1.63/hr, A6000s from $0.47/hr, A4000s from $0.13/hr, etc etc**. Because we are a true marketplace, prices fluctuate with supply and demand.

All are available in plain Ubuntu 22.04 or with popular ML packages preinstalled — CUDA, PyTorch, TensorFlow, etc., and all are hosted by a network of mining farms, data centers, or businesses that we’ve closely vetted.

If you’re looking for hosting for your next project, give us a try! Happy to provide testing credits, just email me at [jonathan@tensordock.com](mailto:jonathan@tensordock.com). And if you do end up trying us, please provide feedback below \[or directly!\] :)

&#x200B;

Deploy a GPU VM: [https://dashboard.tensordock.com/deploy](https://dashboard.tensordock.com/deploy)

CPU-only VMs: [https://dashboard.tensordock.com/deploy\_cpu](https://dashboard.tensordock.com/deploy_cpu)

Apply to become a host: [https://tensordock.com/host](https://tensordock.com/host)",MachineLearning,97,47,1714559509.0,1chiu4a,jonathan-lei,https://www.reddit.com/r/MachineLearning/comments/1chiu4a/d_tensordock_gpu_cloud_marketplace_h100s_from/,Discussion
[R] Marcus Hutter's work on Universal Artificial Intelligence,"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https://www.amazon.co.uk/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3540221395) in 2005 and [one](https://www.amazon.co.uk/Introduction-Universal-Artificial-Intelligence-Robotics/dp/1032607025/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of ""universal intelligence"" in their paper [https://arxiv.org/abs/0712.3329](https://arxiv.org/abs/0712.3329)

In my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:

https://preview.redd.it/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49

Youtube: [https://www.youtube.com/watch?v=7TgOwMW\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https://www.youtube.com/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)

  
Outline:

I. Introduction

* 00:38 : Biography
* 01:45 : From Physics to AI
* 03:05 : Hutter Prize
* 06:25 : Overview of Universal Artificial Intelligence
* 11:10 : Technical outline

II. Universal Prediction

* 18:27 : Laplace’s Rule and Bayesian Sequence Prediction
* 40:54 : Different priors: KT estimator
* 44:39 : Sequence prediction for countable hypothesis class
* 53:23 : Generalized Solomonoff Bound (GSB)
* 57:56 : Example of GSB for uniform prior
* 1:04:24 : GSB for continuous hypothesis classes
* 1:08:28 : Context tree weighting
* 1:12:31 : Kolmogorov complexity
* 1:19:36 : Solomonoff Bound & Solomonoff Induction
* 1:21:27 : Optimality of Solomonoff Induction
* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines
* 1:28:37 : Large Language Models (LLMs)
* 1:37:07 : Using LLMs to emulate Solomonoff induction
* 1:41:41 : Loss functions
* 1:50:59 : Optimality of Solomonoff induction revisited
* 1:51:51 : Marvin Minsky

III. Universal Agents

* 1:52:42 : Recap and intro
* 1:55:59 : Setup
* 2:06:32 : Bayesian mixture environment
* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy
* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)
* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence
* 2:15:35 : AIXI explicit formula
* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)
* 2:33:09 : Multiagent setting
* 2:39:38 : Grain of Truth problem
* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria
* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs
* 2:56:13 : Outro: Brief philosophical remarks",MachineLearning,94,45,1715416721.0,1cpcwuz,IamTimNguyen,https://www.reddit.com/r/MachineLearning/comments/1cpcwuz/r_marcus_hutters_work_on_universal_artificial/,Research
[R] Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues,"
Abstract:
Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and DeltaNet have emerged as efficient alternatives to Transformers in large language modeling, offering linear scaling with sequence length and improved training efficiency. However, LRNNs struggle to perform state-tracking which may impair performance in tasks such as code evaluation or tracking a chess game. Even parity, the simplest state-tracking task, which non-linear RNNs like LSTM handle effectively, cannot be solved by current LRNNs. Recently, Sarrof et al. (2024) demonstrated that the failure of LRNNs like Mamba to solve parity stems from restricting the value range of their diagonal state-transition matrices to [0,1]  and that incorporating negative values can resolve this issue. We extend this result to non-diagonal LRNNs, which have recently shown promise in models such as DeltaNet. We prove that finite precision LRNNs with state-transition matrices having only positive eigenvalues cannot solve parity, while complex eigenvalues are needed to count modulo 3. Notably, we also prove that LRNNs can learn any regular language when their state-transition matrices are products of identity minus vector outer product matrices, each with eigenvalues in the range [-1,1]. Our empirical results confirm that extending the eigenvalue range of models like Mamba and DeltaNet to include negative values not only enables them to solve parity but consistently improves their performance on state-tracking tasks. Furthermore, pre-training LRNNs with an extended eigenvalue range for language modeling achieves comparable performance and stability while showing promise on code and math data. Our work enhances the expressivity of modern LRNNs, broadening their applicability without changing the cost of training or inference.

https://arxiv.org/abs/2411.12537",MachineLearning,92,4,1732371113.0,1gy0hbh,iltruma,https://arxiv.org/abs/2411.12537,Research
Segment Anything Model 2 - Just released by Meta FAIR [R],"**FAIR just launched SAM 2!**

The model performs better than SAM on images, and can now segment objects in video, and continues to be open. 

The model, code, dataset, and demo are all available.

[Site](https://ai.meta.com/sam2/)

[Demo](https://sam2.metademolab.com/?utm_source=ai_meta_site&utm_medium=web&utm_content=AI_demos_page&utm_campaign=July_moment)

[Github](https://github.com/facebookresearch/segment-anything-2)",MachineLearning,95,3,1722295683.0,1efebjl,fruitofconfusion,https://www.reddit.com/r/MachineLearning/comments/1efebjl/segment_anything_model_2_just_released_by_meta/,Research
[N] OpenAI announces SearchGPT,"https://openai.com/index/searchgpt-prototype/

> We’re testing SearchGPT, a temporary prototype of new AI search features that give you fast and timely answers with clear and relevant sources.",MachineLearning,93,30,1721932860.0,1ec2gk2,we_are_mammals,https://www.reddit.com/r/MachineLearning/comments/1ec2gk2/n_openai_announces_searchgpt/,News
"[Project] Tsetlin Machine for Deep Logical Learning and Reasoning With Graphs (finally, after six years!)","https://preview.redd.it/spcqdkqwnovd1.png?width=2643&format=png&auto=webp&s=ba0d7dd294ef9814f20bf3950f0049e80cf8d8d9

Hi all! I just completed the first deep Tsetlin Machine - a Graph Tsetlin Machine that can learn and reason multimodally across graphs. After introducing the Tsetlin machine in 2018, I expected to figure out how to make a deep one quickly. Took me six years! Sharing the project: [https://github.com/cair/GraphTsetlinMachine](https://github.com/cair/GraphTsetlinMachine)

Features:

* Processes directed and labeled [multigraphs](https://en.wikipedia.org/wiki/Multigraph)
* [Vector symbolic](https://link.springer.com/article/10.1007/s10462-021-10110-3) node properties and edge types
* Nested (deep) clauses
* Arbitrarily sized inputs
* Incorporates [Vanilla](https://tsetlinmachine.org/wp-content/uploads/2022/11/Tsetlin_Machine_Book_Chapter_One_Revised.pdf), Multiclass, [Convolutional](https://tsetlinmachine.org/wp-content/uploads/2023/12/Tsetlin_Machine_Book_Chapter_4_Convolution.pdf), and [Coalesced](https://arxiv.org/abs/2108.07594) [Tsetlin Machines](https://tsetlinmachine.org/)
* Rewritten faster CUDA kernels

Roadmap:

* Rewrite [graphs.py](http://graphs.py) in C or numba for much faster construction of graphs
* Add autoencoder
* Add regression
* Add multi-output
* Graph initialization with adjacency matrix

Happy to receive feedback on the next steps of development!",MachineLearning,89,17,1729331307.0,1g75gcb,olegranmo,https://www.reddit.com/r/MachineLearning/comments/1g75gcb/project_tsetlin_machine_for_deep_logical_learning/,Project
[P] Liger Kernel: One line to make LLM Training +20% faster and -60% memory,,MachineLearning,91,18,1724513943.0,1f0875c,Icy-World-8359,https://github.com/linkedin/Liger-Kernel,Project
[R] Are Language Models Actually Useful for Time Series Forecasting?,,MachineLearning,89,55,1719455984.0,1dpgp0h,Cunic,https://arxiv.org/pdf/2406.16964,Research
[R] A Comprehensive Database of 300+ Production LLM Implementations with Technical Architecture Details,"Sharing a valuable resource for ML practitioners: A newly released database documenting over 300 real-world LLM implementations, with detailed technical architectures and engineering decisions.

Key aspects that might interest this community:

* Retrieval-Augmented Generation (RAG) architectures in production
* Fine-tuning decisions and performance comparisons
* Embedding strategies and vector database implementations
* Model optimization techniques and quantization approaches
* Evaluation methodologies and monitoring systems

Notable technical implementations covered:

* Anzen's document classification system using BERT (95% accuracy in production)
* Barclays' MLOps evolution for regulatory compliance
* MosaicML's lessons from training & deploying MPT
* Emergent Methods' real-time RAG system for news processing
* Qatar Computing Research Institute's T-RAG architecture

Technical focus areas:

1. Model serving architectures
2. Training infrastructure decisions
3. Latency optimization strategies
4. Cost-performance trade-offs
5. Production monitoring approaches

Each case study includes:

* Technical architecture diagrams where available
* Performance metrics and benchmarks
* Implementation challenges and solutions
* Infrastructure decisions and rationale
* Scaling considerations

URL: [https://www.zenml.io/llmops-database/](https://www.zenml.io/llmops-database/)

We're also accepting technical write-ups of production implementations through the submission form: [https://docs.google.com/forms/d/e/1FAIpQLSfrRC0\_k3LrrHRBCjtxULmER1-RJgtt1lveyezMY98Li\_5lWw/viewform](https://docs.google.com/forms/d/e/1FAIpQLSfrRC0_k3LrrHRBCjtxULmER1-RJgtt1lveyezMY98Li_5lWw/viewform)

Would be particularly interested in this community's thoughts on the architectural patterns emerging across different scales of deployment.

*Edit: We've also synthesized cross-cutting technical themes into summary podcasts for those interested in high-level patterns.*

*Edit: An accompanying blog synthesizes much of the learnings:* [*https://www.zenml.io/blog/demystifying-llmops-a-practical-database-of-real-world-generative-ai-implementations*](https://www.zenml.io/blog/demystifying-llmops-a-practical-database-of-real-world-generative-ai-implementations)",MachineLearning,88,28,1733144282.0,1h4udds,htahir1,https://www.reddit.com/r/MachineLearning/comments/1h4udds/r_a_comprehensive_database_of_300_production_llm/,Research
[R] Say What You Mean: A Response to 'Let Me Speak Freely',"Will here from .txt, the team behind [Outlines](https://github.com/dottxt-ai/outlines) an open source library that enables open LLMs to perform structured generation, ensuring their outputs always adhere to a predefined format.

We are passionate about structured generation, and truly believe it has the potential to transform the work being done with LLMs in profound ways. 

However a recent paper, [Let Me Speak Freely](https://arxiv.org/abs/2408.02442) was published reporting some misinformation around the performance of structured generation on a series of evaluations. 

We've recently publish a rebuttal to this paper on our blog: [Say What You Mean: A Response to 'Let Me Speak Freely'](https://blog.dottxt.co/say-what-you-mean.html) and thought the community here might find it interesting. It covers not only issues with the original paper, but also dives into the nature of structured generation and how to get the most out of your models with prompting for structured generation.",MachineLearning,91,30,1732229865.0,1gwswn7,CountBayesie,https://www.reddit.com/r/MachineLearning/comments/1gwswn7/r_say_what_you_mean_a_response_to_let_me_speak/,Research
[P] Larger and More Instructable Language Models Become Less Reliable,"[A very interesting paper on Nature](https://www.nature.com/articles/s41586-024-07930-y), followed by [a summary](https://x.com/lexin_zhou/status/1838961179936293098) on X by one of the authors.

The takeaways are basically that **larger models trained with more computational resources & human feedback can get less reliable for humans in several aspects**, e.g., model can solve on very difficult tasks but fail much simpler ones in the same domain and this discordance is becoming worse for newer models (basically no error-freeness even for simple tasks and increasingly harder for humans to anticipate model failures?). The paper also shows newer LLMs now avoid tasks much less, leading to more incorrect/hallucinated outputs (which is quite ironic: So LLMs have become more correct but also substantially more incorrect at the same time)... I'm intrigued that they show prompt engineering may not disappear by simply scaling up the model more as newer models are only improving incrementally, and humans are bad at spotting output errors to offset unreliability. The results seem consistent across 32 LLMs from GPT, LLAMA and BLOOM series, and in the X-thread they additionally show that unreliability still persists with other very recent models like o1-preview, o1-mini, LLaMA-3.1-405B and Claude-3.5-Sonnet. There's a lot of things to unpack here. But important to note that this work is not challenging the current scaling paradigm but some other design practice of LLMs (e.g. the pipeline of data selection and human feedback) that may have instead caused these issues, which worth to pay attention.

https://preview.redd.it/hpd7yynaqisd1.png?width=1888&format=png&auto=webp&s=ebc7953700935ee85cafd2f5d3602b80418d4523",MachineLearning,93,25,1727951200.0,1fv4hxo,Appropriate_Annual73,https://www.reddit.com/r/MachineLearning/comments/1fv4hxo/p_larger_and_more_instructable_language_models/,Project
[R] CoPE: Contextual Position Encoding: Learning to Count What's Important,,MachineLearning,90,12,1717268738.0,1d5u95z,fasttosmile,https://arxiv.org/abs/2405.18719,Research
[D] Evolutionary Strategy vs. Backpropagation,"I was looking into Evolutionary Strategy for training NNs and I'm getting pretty interestnig results. Here is the notebook you can play with: [Link to the Colab Notebook](https://colab.research.google.com/drive/1hYsH9yeMb9xjz-pUssSmz0pYjC0Q_Xh6?usp=sharing)

||Number of epochs|Final Accuracy|Seconds per epoch|
|:-|:-|:-|:-|
|**Backpropagation**|10|97%|9|
|**Evolutionary Strategy**|10|90%|9|

I wonder how far it can be pushed, but getting 90% of accuracy for something that does not use gradient information at all and completes the training within the same amount of time on GPU as backpropagation is quite interesting.

The ES algorithm used is very simple:

1. Initialize all weights with zeros
2. Create new generation population of size N - draw every weight from normal distribution where mean is the current weight and standard deviation is the learning rate.
3. Calculate loss for every individual in population in parallel - works very well on GPUs
4. Pick top-k best performing individuals for mating.
5. To get next weight tensor for new generation take a mean of top-k best performing individuals.
6. Go to step 2.

Do you know of any cool research that explores Evolutionary Strategies for training neural networks?

**UPDATE**

With these parameters you'll get 90% after first epoch and around 94% after 10 epochs with similar number of seconds spent in one epoch:

    lr = 5E-2
    population_size = 512
    generations_per_batch = 1
    num_parents_for_mating = 256",MachineLearning,90,53,1718697626.0,1dila4s,kiockete,https://www.reddit.com/r/MachineLearning/comments/1dila4s/d_evolutionary_strategy_vs_backpropagation/,Discussion
[D] ICASSP 2025 Final Decision ,"ICASSP 2025 results will be declared today. Is anyone excited in this community?
I have 3 WA and looking forward to the results.
Let me know if you get to know anything !",MachineLearning,88,333,1734491997.0,1hgsj0u,stantheta,https://www.reddit.com/r/MachineLearning/comments/1hgsj0u/d_icassp_2025_final_decision/,Discussion
[D] The Lost Reading Items of Ilya Sutskever's AI Reading List,"This blog post attempts to identify which papers went missing from the viral AI reading list that surfaced earlier this year and was attributed to Ilya Sutskever and his claim to cover '90% of what matters' in AI in 2020:

https://tensorlabbet.com/2024/11/11/lost-reading-items/

Only 27 of about 40 papers were shared online earlier this year, so there have been many theories about which works would have been important enough to include. There are some obvious candidates related to meta-learning and competitive self-play discussed here. But also several noteworthy authors like Yann LeCun and Ian Goodfellow are absent from the list.

From my perspective, even papers on U-Net, YOLO detectors, GAN, WaveNet, Word2Vec and more would have made sense to include, so I am curious about more opinions on this!",MachineLearning,86,10,1731666851.0,1grti0x,AccomplishedCat4770,https://www.reddit.com/r/MachineLearning/comments/1grti0x/d_the_lost_reading_items_of_ilya_sutskevers_ai/,Discussion
[D] Why is Tree of Thought an impactful work?,"My advisor recently asked me to read the tot paper, but it seems to me that it was just another \*\*fancy prompt engineering work\*\*. The tot process entails heavy human intelligence (we should manually divide the problem into separate steps and also design verifiers for this method to work), plus it's highly costly and I rarely see people use this method in their work.

Still, this paper receives lots of citations and given the fact that my advisor asked me to read it, I'm wondering if I'm missing anything merits or important implications regarding this work.",MachineLearning,88,34,1727811531.0,1ftx04x,StraightSpeech9295,https://www.reddit.com/r/MachineLearning/comments/1ftx04x/d_why_is_tree_of_thought_an_impactful_work/,Discussion
"[R] I've devised a potential transformer-like architecture with O(n) time complexity, reducible to O(log n) when parallelized.","\[R\] I've attempted to build an architecture that uses plain divide and compute methods. From what I can see and understand, it seems to work, at least in my eyes. While there's a possibility of mistakes in my code, I've checked and tested it without finding any errors.

I'd like to know if this approach is anything new. If so, I'm interested in collaborating with you to write a research paper about it. Additionally, I'd appreciate your help in reviewing my code for any potential mistakes.

But most most importantly I want to know about the architecture ,is it new, has anyone has tried this or something similar ,

I've written a Medium article that includes the code. The article is available at: [https://medium.com/@DakshishSingh/equinox-architecture-divide-compute-775a8ff698fe](https://medium.com/@DakshishSingh/equinox-architecture-divide-compute-775a8ff698fe)

Your assistance and thoughts on this matter would be greatly appreciated. If you have any questions or need clarification, please feel free to ask.",MachineLearning,90,36,1723723431.0,1esteqd,Conscious-Gazelle-91,https://www.reddit.com/r/MachineLearning/comments/1esteqd/r_ive_devised_a_potential_transformerlike/,Research
[D] Scientific Machine Learning,"Traditional ML courses and the projects you do as part of it are not meant for engineers. 

As a mechanical engineer or a physicist, why would I do a project on movie review analyzer or housing price prediction?

I would love to do a project which teaches me how to use ML to model fluid mechanics or black hole dynamics.

I want a field which combines ML with my domain knowledge.

Scientific ML is exactly that field.

I feel Scientific ML is one of the coolest techniques of the last 4-5 years.

**There are 3 main pillars of Scientific ML:**

(1) Neural ODEs

(2) Physics Informed Neural Networks (PINNs)

(3) Universal Differential Equations

It helped me transition from mechanical engineering to machine learning, and obtain a PhD at MIT in Machine Learning.

Any thoughts on Scientific ML or PINNs or Neural ODEs?

https://i.redd.it/96mcn7oj0vbd1.gif



",MachineLearning,89,48,1720690661.0,1e0kvxw,OtherRaisin3426,https://www.reddit.com/r/MachineLearning/comments/1e0kvxw/d_scientific_machine_learning/,Discussion
[P] Wind Speed Prediction with ARIMA/SARIMA," I'm working on a project of wind speed prediction. Some articles said that using ARIMA / SARIMA would be a good start.

I did start by using ARIMA and got no variation whatsoever in the predicted values.

And when i tried SARIMA,with seasonality = 12 (months of the year),to predict for 36 months ( 3years) it gave me unsatisfactory results that looks the same every year (periodical and thus faar from reality)so i gave up on SARIMA. 

Feel free to give me solutions or better methods.",MachineLearning,84,21,1735472022.0,1hou9cq,Associate-Existing,https://www.reddit.com/gallery/1hou9cq,Project
"[D] Clarification on the ""Reparameterization Trick"" in VAEs and why it is a trick","I’ve been studying Variational Autoencoders (VAEs) and I keep coming across the term ""reparameterization trick."" From what I understand, the trick involves using the formula ( `X = mean + standard dev * Z` ) to sample from a normal distribution, where  `Z` is drawn from a standard normal distribution. This formula seems to be a standard method for sampling from a normal distribution

Here’s my confusion:

**Why is it a trick?**

The reparameterization ""trick"" is often highlighted as a clever trick, but to me, it appears to be a straightforward application of the transformation formula. If ( `X = mean + standard dev * Z` ) is the only way to sample from a normal distribution, why is the reparameterization trick considered particularly innovative?

I understand that the trick allows backpropagation through the sampling process. However, it seems like using ( `X = mean + standard dev * Z` ) is the only way to generate samples from a normal distribution given ( mean ) and ( standard deviation ). What makes this trick special beyond ensuring differentiability?

Here's my thought process: We get mean and standard deviation from the encoder, and to sample from them, the only and most obvious way is \`X = mean + standard deviation \* Z'.

Could someone help clarify why the reparameterization trick is called a ""trick""?

Thanks in advance for your insights!",MachineLearning,89,27,1724889458.0,1f3ohje,SwaroopMeher,https://www.reddit.com/r/MachineLearning/comments/1f3ohje/d_clarification_on_the_reparameterization_trick/,Discussion
"[D] Good studies on the effects of different training ""tricks"" like learning rate scheduler (warmup/decay), weight decay, dropout, batch-sizes, momentum, etc.?","Given that the number of ""tricks"" like learning rate scheduler (e.g. linear warmup/cosine decay), regularization (weight decay), dropout, batch-sizes, momentum terms (beta1, beta2 in Adam), batch-norm, etc. are becoming quite large and it is becoming a lot harder to examine all the different combinations of those parameters on these large models, is there any existing study or crowd-source effort that studies the effects on the final performance (val perplexity for example) when we vary various parameter of these tricks?

I bet a good chunk of them are in ablation studies but they are a bit too scattered around.",MachineLearning,84,15,1726524088.0,1fihdrd,ThienPro123,https://www.reddit.com/r/MachineLearning/comments/1fihdrd/d_good_studies_on_the_effects_of_different/,Discussion
[D] Should Google AI Overview haven been released ? ,"Yet another bad AI feature release from Google (see reactions in NYT article 5/24). When your read how bad some of the overviews are, it makes you question if Google product team was really thinking about how people will use their products. Almost seems adversarial testing was not done.

If AI Overview is really intended to summarize search results using AI, how is it supposed to work when significant percentage of websites are full of unreliable information including conspiracy theories and sarcasm.

Does anyone truly need a summary of an Onion article when searching?  
'Move fast and break things, even if the product you are breaking pulls in 40 billion/year'",MachineLearning,86,66,1716597332.0,1czzt45,yintrepid,https://www.reddit.com/r/MachineLearning/comments/1czzt45/d_should_google_ai_overview_haven_been_released/,Discussion
[N] ICML 2024 Workshop on making discrete operations differentiable 🤖,"Hi everyone!

We are organizing the **Differentiable almost everything** workshop at ICML this year.

Many discrete operations e.g. sorting, topk, shortest paths, clustering (and many more) have null-gradients almost everywhere, and are hence not suitable for modern gradient based learning frameworks (such as deep learning). This workshop will cover research topics that aim to remedy such problems!

[https://differentiable.xyz/](https://differentiable.xyz/)

We encourage anyone who is working on relevant topics to submit their work. Even if you are not submitting, please do come by the workshop at ICML to see some of the exciting talks that will take place!

I have attached a full summary of the workshop below! All the best with your current work, L :)

*Gradients and derivatives are integral to machine learning, as they enable gradient-based optimization. In many real applications, however, models rest on algorithmic components that implement discrete decisions, or rely on discrete intermediate representations and structures. These discrete steps are intrinsically non-differentiable and accordingly break the flow of gradients. To use gradient-based approaches to learn the parameters of such models requires turning these non-differentiable components differentiable. This can be done with careful considerations, notably, using smoothing or relaxations to propose differentiable proxies for these components. With the advent of modular deep learning frameworks, these ideas have become more popular than ever in many fields of machine learning, generating in a short time-span a multitude of “differentiable everything”, impacting topics as varied as rendering, sorting and ranking, convex optimizers, shortest-paths, dynamic programming, physics simulations, NN architecture search, top-k, graph algorithms, weakly- and self-supervised learning, and many more.*

*This workshop will provide a forum for anything differentiable, bringing together academic and industry researchers to highlight challenges and developments, provide unifying ideas, discuss practical implementation choices and explore future directions.*",MachineLearning,83,14,1716038537.0,1cux80i,machine_learning_res,https://www.reddit.com/r/MachineLearning/comments/1cux80i/n_icml_2024_workshop_on_making_discrete/,News
"[D] Do Lead's in an AI/DS/ML team always have PhDs, is it a requirement?","Hello all, I am a uni student for a masters in AI. During my bachelors I did my thesis at a company and the lead AI had a PhD in Evolutionary algo's. I had a guest lecture from a lead DS last week from a multi billion dollar online marketplace and he also has a PhD. these are a few examples of Leads with PhDs that I've seen.

So this poses the question, is it necessary to have a PhD to become a Lead for an AI/ML/DS team? I am just curious, I don't know if that would be something I'd like to aspire to do, senior is also fine in the end. But I see it so many times, I haven't seen the opposite, as in a Lead with only a Masters degree.

I am not seeking any career advice, I am not planning to get a PhD at all, I just observe this a lot so I'm curious.

Any thoughts?",MachineLearning,85,79,1714398577.0,1cfzktw,Rajivrocks,https://www.reddit.com/r/MachineLearning/comments/1cfzktw/d_do_leads_in_an_aidsml_team_always_have_phds_is/,Discussion
[D] Log Probability and Information Theory,"In machine learning we work with log probabilities a lot, attempting to maximize log probability. This makes sense from a numerical perspective since adding is easier than multiplying but I am also wondering if there is a fundamental meaning behind ""log probability.""

For instance, log probability is used a lot in information theory, and is the negative of 'information'. Can we view minimizing the negative log likelihood in terms of information theory? Is it maximizing/minimizing some metric of information?",MachineLearning,82,18,1731209867.0,1gnrpfe,masonw32,https://www.reddit.com/r/MachineLearning/comments/1gnrpfe/d_log_probability_and_information_theory/,Discussion
"🚀 Convert any GitHub repo to a single text file, perfect for LLM prompting use ""[Project]""","Hey folks! 👋

I know there are several similar tools out there, but here’s why you should check out mine:

* **Free and live right now** 💸
* Works with **private repos** 🛡️
* **Runs entirely in your browser**—no data sent anywhere, so it’s **completely secure** 🔒
* Works with **GitHub URLs to subdirectories** 📁
* Supports **tags, branches, and commit SHAs** 🏷️
* Lets you **include or exclude specific files** 📂

🔗 [Try it out here](https://repo2txt.simplebasedomain.com/)

🔗 [Source code](https://github.com/abinthomasonline/repo2txt)

Give it a spin and let me know what you think! 😊

[repo2txt Demo](https://i.redd.it/tng98wqkawrd1.gif)",MachineLearning,88,24,1727679937.0,1fspn1s,Beautiful-Novel1150,https://www.reddit.com/r/MachineLearning/comments/1fspn1s/convert_any_github_repo_to_a_single_text_file/,Project
"[R] A collection of LLM papers, blogs, and projects, with a focus on OpenAI o1 and reasoning techniques.",,MachineLearning,82,4,1726452469.0,1fhtkz5,Happysedits,https://github.com/hijkzzz/Awesome-LLM-Strawberry,Research
[D] Hype Behind Agents?,I’ve been hearing a lot of pitches for multi agent system startups recently and I’m not sure exactly why there is so much hype. What makes a multi agent system difficult? What are the interesting research questions? Doesn’t DSPy solve a lot of these problems already? ,MachineLearning,84,46,1717998821.0,1dcefvk,Primary-Track8298,https://www.reddit.com/r/MachineLearning/comments/1dcefvk/d_hype_behind_agents/,Discussion
[R] Understanding the Unreasonable Effectiveness of Discrete Representations In Reinforcement Learning,"# Links

Paper: [https://arxiv.org/abs/2312.01203](https://arxiv.org/abs/2312.01203)  
Code: [https://github.com/ejmejm/discrete-representations-for-continual-rl](https://github.com/ejmejm/discrete-representations-for-continual-rl)  
Video: [https://youtu.be/s8RqGlU5HEs](https://youtu.be/s8RqGlU5HEs) <-- Recommended if you want a quick (\~13 min) look  
Thesis: [https://era.library.ualberta.ca/items/d9bc72bd-cb8c-4ca9-a978-e97e8e16abf0](https://era.library.ualberta.ca/items/d9bc72bd-cb8c-4ca9-a978-e97e8e16abf0)  
 

# Problem

Several recent papers in the model-based RL space \[e.g. [1](https://arxiv.org/abs/2301.04104), [2](https://arxiv.org/abs/2010.05767), [3](https://openreview.net/forum?id=vhFu1Acb0xb)\] have used discrete state representations - **that is weird!** Why use representations that are less expressive and are far more limited in informational content?

That's what this paper looks at:

(1) *What are the benefits of using discrete states to learn world models*, and

(2) *What are the benefits of  using discrete states to learn policies?*

We also start just start to look at why this might be the case.  
 

# Key Results

**1.** ***World models learned over discrete representations were able to more accurately represent more of the world (transitions) with less capacity*** when compared to those learned over continuous representations.

[ground-truth](https://i.redd.it/7lv15efv1bcd1.gif)

[continuous representations](https://i.redd.it/s1y7hu8x1bcd1.gif)

[discrete representations](https://i.redd.it/trl79pgy1bcd1.gif)

Above you can see the same policy played out in the real environment, and simulated in continuous and discrete world models. Over time, errors in the continuous world model accumulated, and the agent never reaches the goal. This is less of a problem in the discrete world model. It's important to note that both have the potential to learn perfect would models when the model is large enough, but when that is not possible (as it is generally the case in interesting and complex environments like the real world) discrete representations win out.

 

**2.** ***Not all ""discrete representations"" are created equal***

A discrete variable is one that can take on a number of *distinct* values. Prior work typically uses multi-one-hot representations that look like the green matrix here:

https://preview.redd.it/wi0f2hud4bcd1.png?width=1048&format=png&auto=webp&s=65f7ac6fa8ae48978b0e9f8097c0d8852090b793

They are binary matrices that can be simplified to vectors of natural numbers (i.e. discrete vectors). Each natural number corresponds to a one-hot encoding given by one row of the matrix. Representing these discrete values with one-hot encodings, however, is a choice. What if we instead were to represent them as vectors of arbitrary continuous values? So long as we are consistent (e.g. 3 always maps to \[0.2, -1.5, 0.4\]), then we are representing the exact same information. We call this form of discrete representation a *quantized* representation (for reasons more clear in the paper).

If we compare models learned over quantized and multi-one-hot representations, we see a significant gap in the model's accuracy:

[Lower means a more accurate world model and is better. Multi-one-hot representations are binary, quantized representations are not. Both represent the same discrete information.](https://preview.redd.it/yt8c8x096bcd1.png?width=986&format=png&auto=webp&s=4d94863adddb9bba96d89c1fc5b2326ba8355130)

It turns out that the binarity and sparsity are actually really important! It is not necessarily just the fact that the representations are discrete.



**3.** ***Policies learned over discrete representations improved faster***

Because this post is already pretty long, I'm skipping a lot of details and experiments here (more in the paper). We pre-learned multi-one-hot and continuous representations of two MiniGrid environments, and then learned policies over them. During policy training, we changed the layout of the environment at regular intervals to see how quickly the policies could adapt to the change.

[The agent's goal in these environments is to quickly navigate to the goal, so lower episode length is better.](https://preview.redd.it/mvr87vs08bcd1.png?width=1797&format=png&auto=webp&s=8723d473405c2179720d867d30855c356fce02ae)

When we do this, we see that the policy learned over discrete (multi-one-hot) representations consistently adapts faster.  
 

# Conclusion

Discrete representations in our experiments were beneficial. Learning from discrete representations led to **more accurately modeling more of the world when modeling capacity was limited**, and it led to **faster adapting policies**. However, it does not seem to be just the discreteness of ""discrete representations"" that makes them effective. The choice to use multi-one-hot discrete representations, and the binarity and sparsity of these representations seem to play an important role. We leave the disentanglement of these factor to future work.",MachineLearning,81,28,1720887623.0,1e2e9ou,ejmejm1,https://www.reddit.com/r/MachineLearning/comments/1e2e9ou/r_understanding_the_unreasonable_effectiveness_of/,Research
[Project] LLM based Python docs that never touches your original code,"Documentation is tedious and time-consuming. I thought LLMs might be the answer, but they tend to hallucinate, inventing functions or misinterpret code. Not ideal when you're trying to document real, working code

So I built lmdocs. It can:

* Reference documentation from imported libraries 
* Guarantees that your original code is unchanged
* Work with OpenAI and lo¯cal LLMs

I'd love to get some feedback from other devs. If you're interested, you can check it out here: [https://github.com/MananSoni42/lmdocs](https://github.com/MananSoni42/lmdocs)

It's open source, so feel free to contribute or just let me know what you think. ",MachineLearning,80,23,1718952244.0,1dkxld2,ford_prefect_9931,https://www.reddit.com/r/MachineLearning/comments/1dkxld2/project_llm_based_python_docs_that_never_touches/,Project
[R] TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters,,MachineLearning,81,5,1730470577.0,1gh6fut,MysteryInc152,https://arxiv.org/abs/2410.23168,Research
"[D] If adversarial learning studies suggest neural networks can be quite fragile to input / weight perturbations, why does quantisation work at all?","I have been wondering why these two observations can coexist without conflict. Research on adversarial learning appears to suggest that one can easily find tiny perturbations on inputs or weights that can drastically change certain outputs. If perturbing some weights is already bad enough, surely perturbing every weight as you would do in quantisation would be catastrophic?

I have a few guesses:

* Maybe adversarial perturbation directions are plenty but rare among all possible directions, and a random perturbation like quantisation is unlikely to be adversarial?
* Maybe we are indeed introducing errors, but only on a small subset of outputs that it is not bad enough?
* Maybe random weight perturbation is less damaging to very large networks?

Does anyone know good existing studies that could possibly explain why quantisation does not result in an unintentional self-sabotage?",MachineLearning,86,28,1727227222.0,1fosr7z,aeroumbria,https://www.reddit.com/r/MachineLearning/comments/1fosr7z/d_if_adversarial_learning_studies_suggest_neural/,Discussion
[D] What do you all use for large scale training? Normal pytorch or do you use libraries like HF Accelerate.,"I have to train a large cluster multi-machine soon for a research paper. Curious what you all do for large scale training whether its better to stick with what I know for pytorch (FSDP, DDP, TP, MP, etc...) and slurm or is it worth learning something like HF accelerate for large scale training?",MachineLearning,81,37,1720392417.0,1dxtaez,I_will_delete_myself,https://www.reddit.com/r/MachineLearning/comments/1dxtaez/d_what_do_you_all_use_for_large_scale_training/,Discussion
[P] Labeling data the tinder way,"I was working on a sentiment analysis model which required dataset with proper labels. Instead of doing it the boring way, I created a webserver which saves all the dataset in SQL along with a tinder like interface to review the data and categorize it as `positive`, `negative` or `neutral`.

Thoughts on my project? Is this something that you would use to label data?

project link: [tinder-for-reviews](https://github.com/krishsharma0413/tinder-for-reviews/) :p",MachineLearning,82,27,1717831262.0,1dax94f,ResetWasTaken,https://www.reddit.com/r/MachineLearning/comments/1dax94f/p_labeling_data_the_tinder_way/,Project
[P] JaVAD - Just Another Voice Activity Detector,"Just published a VAD I worked on for the last 3 months (not accounting time on model itself), and it seems like it is at least on par or better than any other open source VAD.

* It is a custom conv-based architecture using sliding windows over mel-spectrogram, so it is very fast too (it takes 16.5 seconds on 3090 to load and process 18.5 hours of audio from test set).
* It is also very compact (everything, including checkpoints, fits inside PyPI package) and if you don't need to load audio, core functionality deps are just pytorch and numpy.
* Some other VADs were trained on a synthetic data by mixing speech and noise and I think that is the reason why they're falling behind on noisy audio. For this project I manually labeled dozens of YouTube videos, especially old movies and tv shows, with a lot of noise in them.
* There's also a class for streaming, although due to the nature of sliding windows and normalisation, processing initial part of audio can result in a lower quality predictions.
* MIT license

It's a solo project, so I'm pretty sure I missed something (or a lot), feel free to comment or raise issues on github.

Here's the link: [https://github.com/skrbnv/javad](https://github.com/skrbnv/javad)",MachineLearning,79,17,1735126437.0,1hlz6az,ApprehensiveLet1405,https://www.reddit.com/r/MachineLearning/comments/1hlz6az/p_javad_just_another_voice_activity_detector/,Project
[D] Struggling to Find My Path in PhD Research,"Hi everyone, I hope you don’t mind me venting a bit, but I’m hoping to gain some insight into a challenge I’ve been facing. I’m a second-year PhD student researching time series, and honestly, I thought by now I would have a clear research question. But I don’t, and it’s starting to get to me.

Part of the struggle comes from the overwhelming pressure to pick a “hot” topic. A lot of the research I see in the field feels driven by what I can only describe as *Shiny Object Syndrome*—chasing the latest trends rather than focusing on work that’s meaningful and substantial. For example, I’ve seen several papers using large language models (LLMs) for time series forecasting. While LLMs are undeniably fascinating, it feels more like an attempt to forcefully fit them into time series because it’s “cool,” not because it’s the best tool for the problem at hand. And I don’t want to be part of that trend.

But here’s the dilemma: How do you choose a research topic that feels both authentic and impactful, especially when everything around you seems so driven by the latest hype? Do you follow these emerging trends, or do you focus on something that deeply resonates with you, even if it’s not the “shiny” thing everyone else is working on?

I’m honestly feeling a bit stuck and unsure of myself. Am I overthinking this? Is it just part of the process? How do I find a direction that feels true to my interests and the bigger picture of what I want to contribute to the field? If anyone has been through something similar or has any advice, I would be incredibly grateful.

Thank you for taking the time to read this—I truly appreciate any insights or encouragement you can offer.",MachineLearning,80,20,1734774359.0,1hj6nbf,Few-Pomegranate4369,https://www.reddit.com/r/MachineLearning/comments/1hj6nbf/d_struggling_to_find_my_path_in_phd_research/,Discussion
[P] PyTorch implementation of Levenberg-Marquardt training algorithm,"Hi everyone,

In case anyone is interested, here’s a PyTorch implementation of the **Levenberg-Marquardt (LM)** algorithm that I’ve developed.

**GitHub Repo**: [torch-levenberg-marquardt](https://github.com/fabiodimarco/torch-levenberg-marquardt)

A PyTorch implementation of the **Levenberg-Marquardt (LM)** optimization algorithm, supporting **mini-batch training** for both **regression** and **classification** problems. It leverages GPU acceleration and offers an extensible framework, supporting diverse loss functions and customizable damping strategies.

A TensorFlow implementation is also available: [tf-levenberg-marquardt](https://github.com/fabiodimarco/tf-levenberg-marquardt)

# Installation

    pip install torch-levenberg-marquardt",MachineLearning,80,7,1733144093.0,1h4ubbd,fabiodimarco,https://www.reddit.com/r/MachineLearning/comments/1h4ubbd/p_pytorch_implementation_of_levenbergmarquardt/,Project
[R] Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers,,MachineLearning,79,18,1725960913.0,1fddtmm,hardmaru,https://arxiv.org/abs/2409.04109,Research
[D] How do you keep track of all your experiments ?,"Hello everyone, 

  
In my company, we are conducting a lot of experiments on LLMs.  
We are currently in the process of doing ""small-scale"" experiments to do various things (select various hyperparameters, do some small architecture changes, what dataset to use, etc ...)  
We are using WandB and it's pretty cool to log experiments but I'm not aware of any features to go a step further in terms of collaboration. For instance, we would like to have something were we can write conclusions from the various experiments/plots we launched and ideally have the plots and conclusions stored in one place.   
This way it's easy to keep track of everything and in particular when we go back to experiments months later, we are able to understand why we launched it and what was the conclusion out of it.

How do you manage that ? Do you use specific tools ?",MachineLearning,82,42,1723033947.0,1emakgn,Theboredhuman_56,https://www.reddit.com/r/MachineLearning/comments/1emakgn/d_how_do_you_keep_track_of_all_your_experiments/,Discussion
[D] What are some of the interesting applied ml papers/blogs you read in 2024 or experiences,"I am looking for some interesting successful/unsuccessful real-world machine learning applications. You are also free to share experiences building applications with machine learning that have actually had some real world impact.

Something of this type: 

1. LinkedIn has developed a new family of domain-adapted foundation models called Economic Opportunity Network (EON) to enhance their platform's AI capabilities.

https://www.linkedin.com/blog/engineering/generative-ai/how-we-built-domain-adapted-foundation-genai-models-to-power-our-platform


Edit: Just to encourage this conversation here is my own personal SAAS app - this is how l have been applying machine learning in the real world as a machine learning engineer. It's not much, but it's something.
This is a side project(built during weekends and evenings) which flopped and has no users
[Clipbard](https://clipbard.com). I mostly keep it around to enhance my resume.
My main audience were educators would like to improve engagement with the younger 'tiktok' generation. I assumed this would be a better way of sharing things like history in a more memorable way as opposed to a wall of text. I also targeted groups like churches (Sunday school/ Children's church) who want to bring bible stories to life or tell stories with lessons or parents who want to bring bedtime stories to life every evening.


",MachineLearning,80,22,1735394056.0,1ho63je,takuonline,https://www.reddit.com/r/MachineLearning/comments/1ho63je/d_what_are_some_of_the_interesting_applied_ml/,Discussion
[R] The Multimodal Universe: Enabling Large-Scale Machine Learning with 100TB of Astronomical Scientific Data,"https://openreview.net/forum?id=EWm9zR5Qy1#discussion

Abstract: We present the Multimodal Universe, a large-scale multimodal dataset of scientific astronomical data, compiled specifically to facilitate machine learning research. Overall, our dataset contains hundreds of millions of astronomical observations, constituting 100TB of multi-channel and hyper-spectral images, spectra, multivariate time series, as well as a wide variety of associated scientific measurements and metadata. In addition, we include a range of benchmark tasks representative of standard practices for machine learning methods in astrophysics. This massive dataset will enable the development of large multi-modal models specifically targeted towards scientific applications. All codes used to compile the dataset, and a description of how to access the data is available at https://github.com/MultimodalUniverse/MultimodalUniverse

What can you guys see the uses of this dataset being?",MachineLearning,81,3,1733257166.0,1h5x146,blabboy,https://www.reddit.com/r/MachineLearning/comments/1h5x146/r_the_multimodal_universe_enabling_largescale/,Research
[D] Comparison of Logistic Regression with/without SMOTE,"This has been driving me crazy at work. I've been evaluating a logistic predictive model. The model implements SMOTE to balance the dataset to 1:1 ratio (originally 7% of the desired outcome). I believe this to be unnecessary as shifting the decision threshold would be sufficient and avoid unnecessary data imputation. The dataset has more than 9,000 ocurrences of the desired event - this is more than enough for MLE estimation. My colleagues don't agree. 

I built a shiny app in R to compare the confusion matrixes of both models, along with some metrics. I would welcome some input from the community on this comparison. To me the non-smote model performs just as well, or even better if looking at the Brier Score or calibration intercept. What do you guys think?",MachineLearning,77,44,1730673759.0,1gizg2u,Janky222,https://i.redd.it/fl4kf6wmlryd1.jpeg,Discussion
[P] Model2Vec: Distill a Small Fast Model from any Sentence Transformer,"Hey 👋!

I wanted to share a project we've been working on for the past couple of months called [Model2Vec](https://github.com/MinishLab/model2vec) that we recently open-sourced. It's a technique to distill Sentence Transformer models and create very small static embedding models (30mb on disk) that are up to 500x faster than the original model, making them very easy to use on CPU. Distillation takes about 30 seconds on a CPU.

These embeddings outperform similar methods such as GloVE and BPEmb by a large margin on MTEB while being much faster to create, and no dataset is needed. It's designed as an eco-friendly alternative to (Large) Language Models and particularly useful for situations where you are time-constrained (e.g. search engines), or don't have access to fancy hardware.

The idea is pretty straightforward, but works surprisingly well:

1: Take the token output embeddings of any Sentence Transformer.

2: Reduce the dimensionality using PCA. This reduces the model size, but also normalizes the output space.

3: Apply zipf weighting to the embeddings based on the word/token frequencies. This essentially downweights frequent words, meaning you don't need to remove stopwords for example.

We've created a couple of easy to use methods that can be used after installing the package with `pip install model2vec`:

**Inference:**

    from model2vec import StaticModel
    
    # Load a model from the HuggingFace hub (in this case the M2V_base_output model)
    model_name = ""minishlab/M2V_base_output""
    model = StaticModel.from_pretrained(model_name)
    
    # Make embeddings
    embeddings = model.encode([""It's dangerous to go alone!"", ""It's a secret to everybody.""])

**Distillation:**

    from model2vec.distill import distill
    
    # Choose a Sentence Transformer model
    model_name = ""BAAI/bge-base-en-v1.5""
    
    # Distill the model
    m2v_model = distill(model_name=model_name, pca_dims=256)
    
    # Save the model
    m2v_model.save_pretrained(""m2v_model"")

I'm curious to hear your thoughts on this, and happy to answer any questions!

Links:

* [Repo link](https://github.com/MinishLab/model2vec)
* [Results link](https://github.com/MinishLab/model2vec?tab=readme-ov-file#results)",MachineLearning,81,21,1728316951.0,1fyb9jj,Pringled101,https://www.reddit.com/r/MachineLearning/comments/1fyb9jj/p_model2vec_distill_a_small_fast_model_from_any/,Project
[D] Batch size vs learning rate,"There are two schools of thought on what the optimal batch size is for
best model performance:

1. Small, around 32.
2. Irrelevant, so use the largest batch size possible to minimize training time.

There are plenty of sources that support either theory. Here are a few
that claim small batches are best:

> The best performance has been consistently obtained for mini-batch
> sizes between m=2 and m=32, which contrasts with recent work
> advocating the use of mini-batch sizes in the thousands.
>
> [Revisiting Small Batch Training for Deep Neural Networks][3]

> Our results concluded that a higher batch size does not usually
> achieve high accuracy, and the learning rate and the optimizer used
> will have a significant impact as well. Lowering the learning rate
> and decreasing the batch size will allow the network to train
> better, especially in the case of fine-tuning.
>
> [The effect of batch size on the generalizability of the convolutional neural networks on a histopathology dataset][2]

> Training with large minibatches is bad for your health.  More
> importantly, it's bad for your test error.  Friends dont let friends
> use minibatches larger than 32.
>
> [Yann LeCun][4]

And some that claim they should be large:

>  We find no evidence that larger batch sizes degrade out-of-sample performance.
>
> [Measuring the Effects of Data Parallelism on Neural Network Training][5]

> Once all these effects are taken into account, there is currently no
> convincing evidence that the batch size affects the maximum
> achievable validation performance ... The batch size should not be
> treated as a tunable hyperparameter for validation set performance.
>
> [Deep Learning Tuning Playbook][6]

What do you think? Is there any consensus around what batch sizes to
use for image models like VGG, ResNet, and DenseNet?

[1]: https://www.reddit.com/r/MachineLearning/comments/18fs4ik/d_what_is_the_effect_of_batch_size_on_training/
[2]: https://www.sciencedirect.com/science/article/pii/S2405959519303455
[3]: https://arxiv.org/abs/1804.07612
[4]: https://x.com/ylecun/status/989610208497360896?lang=en
[5]: https://arxiv.org/abs/1811.03600
[6]: https://github.com/google-research/tuning_playbook",MachineLearning,80,38,1727450958.0,1fqqfos,bjourne-ml,https://www.reddit.com/r/MachineLearning/comments/1fqqfos/d_batch_size_vs_learning_rate/,Discussion
"[N] Mistral releases a ""Large Enough"" model","https://mistral.ai/news/mistral-large-2407/

* 123B parameters
* On par with GPT-4o and Llama 3.1 405B, according to their benchmarks
* Mistral Research License allows usage and modification for research and non-commercial purposes",MachineLearning,80,3,1721847883.0,1eb9n8c,we_are_mammals,https://www.reddit.com/r/MachineLearning/comments/1eb9n8c/n_mistral_releases_a_large_enough_model/,News
[D] What do you think will be the next big thing in the field? Is LLM hype going to fade?,"I am happy with the success of LLMs, but I am not much of a NLP fan. What do you think will be the next big thing that will achieve commercial success or wide range of applicability (useful both in startups and large companies)?  
  
E.g., are RL or GNNs going to start being used in practice more widely (I know GNNs are used in large companies, but still I am not aware that they are widely used)?

I consider computer vision a well established field considering practical applications, but is there maybe something new happening there?

",MachineLearning,81,106,1729143703.0,1g5jvzp,Diligent-Ad8665,https://www.reddit.com/r/MachineLearning/comments/1g5jvzp/d_what_do_you_think_will_be_the_next_big_thing_in/,Discussion
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it. ",MachineLearning,77,28,1715905657.0,1cts99m,David202023,https://www.reddit.com/r/MachineLearning/comments/1cts99m/d_seminal_papers_list_since_2018_that_will_be/,Discussion
[P] Skyrim - Open-source model zoo for Large Weather Models,"[Github link](https://github.com/secondlaw-ai/skyrim) 

Hey all, I'm Efe from Secondlaw AI. We are building physics-informed large AI models. Currently, we are focusing on weather modelling.

To benchmark SOTA, we had to build a forecasting infra for all available large weather models and we could not find a solid tooling to do so, so we built [Sykrim](https://github.com/secondlaw-ai/skyrim). Within <5 mins and <5 LOC you can run forecasts on par with global weather models that are run on 100K+ CPU HPCs! You can check out examples [here](https://github.com/secondlaw-ai/skyrim/blob/master/notebooks/01_quickstart.ipynb).

We are implementing more models & fine-tuning capabilities. Let us know if anything more we can add, happy to answer any questions!",MachineLearning,77,28,1715097487.0,1cmfbzc,0xe5e,https://www.reddit.com/r/MachineLearning/comments/1cmfbzc/p_skyrim_opensource_model_zoo_for_large_weather/,Project
"[D] When you say ""LLM,"" how many of you consider things like BERT as well?","I keep running into this argument, but for me when I hear ""LLM"" my assumption is decoder-only models that are in the billions of parameters. It seems like some people would include BERT-base in the LLM family, but I'm not sure if that's right? I suppose technically it is, but every time I hear someone say ""how do I use a LLM for XYZ"" they usually bring up LLaMA or Mistral or ChatGPT or the like.",MachineLearning,78,94,1731680184.0,1grxbdp,Seankala,https://www.reddit.com/r/MachineLearning/comments/1grxbdp/d_when_you_say_llm_how_many_of_you_consider/,Discussion
[D] Want to move away from coding heavy ML but still want to complete the PhD,"Hi Folks,

I come from a tradition electrical engineering background doing things like industrial automation and computer vision. I decided to pursue a PhD in ML as I thought it will be a good field to enter given my past experience. Now I have been doing the PhD for the past three years. While I like my group and research, I am getting discouraged/depressed by (1) The publication rat race (2) post graduation opportunities mostly being coding heavy (3) the inability to carve a name for myself in the field given how crowded the field has become.

Thus, ideally I would like to complete my PhD and move into a more relaxed paced (even if it is not as high paying as ML jobs) non coding heavy but technical job, where I do not have to constantly up-skill myself. Do you folks have any suggestion on what jobs I can look into or would you suggest dropping the PhD and doing something else?

TLDR: 4th year ML PhD student unsure of sticking with the PhD as they desire a non coding heavy technical job in the industry post graduation. Seeking advice on what to do.",MachineLearning,74,53,1730895521.0,1gkx6o7,Hopeful-Reading-6774,https://www.reddit.com/r/MachineLearning/comments/1gkx6o7/d_want_to_move_away_from_coding_heavy_ml_but/,Discussion
[D] Am I hallucinating?,"..or was there an LLM training logbook of sorts shared by Google Brain researchers which detailed all the experiments they did, and the approaches they tried while training an LLM? 

I distinctly remember seeing such a project up on GitHub but it's nowhere to be seen now ! 

It was meant as a sort of guide for anyone setting out to train an LLM to avoid common pitfalls and such.
It might not have been google specifically though.

Am I dreaming ?

(Edit: more context) ",MachineLearning,79,17,1729075549.0,1g4weor,demonic_mnemonic,https://www.reddit.com/r/MachineLearning/comments/1g4weor/d_am_i_hallucinating/,Discussion
[P] GPT-2 Circuits - Mapping the Inner Workings of Simple LLMs,"I built an app that extracts interpretable ""circuits"" from models using the GPT-2 architecture. While some tutorials present hypothetical examples of how the layers within an LLM produce predictions, this app provides concrete examples of information flowing through the system. You can see, for example, the formation of features that search for simple grammatical patterns and trace their construction back to the use of more primitive features. Please take a look if you're working on interpretability! I'd love your feedback and hope to connect with folks who can help. Project link: [https://peterlai.github.io/gpt-mri/](https://peterlai.github.io/gpt-mri/)",MachineLearning,77,8,1728345065.0,1fymczh,ptarlye,https://www.reddit.com/r/MachineLearning/comments/1fymczh/p_gpt2_circuits_mapping_the_inner_workings_of/,Project
[D] List of neurips2024 papers is out!,"[https://nips.cc/virtual/2024/papers.html?filter=titles](https://nips.cc/virtual/2024/papers.html?filter=titles)

enjoy!",MachineLearning,79,12,1727509924.0,1fr9bm2,South-Conference-395,https://www.reddit.com/r/MachineLearning/comments/1fr9bm2/d_list_of_neurips2024_papers_is_out/,Discussion
"[N] Fish Speech 1.3 Update: Enhanced Stability, Emotion, and Voice Cloning","We're excited to announce that Fish Speech 1.3 now offers enhanced stability and emotion, and can clone anyone's voice with just a **10-second** audio prompt! As strong advocates of the open-source community, we've open-sourced Fish Speech 1.2 SFT today and introduced an Auto Reranking system. Stay tuned as we'll be open-sourcing Fish Speech 1.3 soon! We look forward to hearing your feedback.

Playground (DEMO): [http://fish.audio](http://fish.audio/)

GitHub: [fishaudio/fish-speech](https://github.com/fishaudio/fish-speech)",MachineLearning,77,15,1721321219.0,1e6g122,lengyue233,https://www.reddit.com/r/MachineLearning/comments/1e6g122/n_fish_speech_13_update_enhanced_stability/,News
[R] SpaceByte: Towards Deleting Tokenization from Large Language Modeling - Rice University 2024 - Practically the same performance as subword tokenizers without their many downsides!,"Paper: [https://arxiv.org/abs/2404.14408](https://arxiv.org/abs/2404.14408)

Github: [https://github.com/kjslag/spacebyte](https://github.com/kjslag/spacebyte)

Abstract:

>Tokenization is widely used in large language models because it significantly improves performance. However, **tokenization imposes several disadvantages, such as performance biases, increased adversarial vulnerability, decreased character-level modeling performance, and increased modeling complexity.** To address these disadvantages without sacrificing performance, we propose SpaceByte, a novel **byte-level decoder architecture that closes the performance gap between byte-level and subword autoregressive language modeling.** SpaceByte consists of a byte-level Transformer model, but with extra larger transformer blocks inserted in the middle of the layers. We find that **performance is significantly improved** by applying these larger blocks only after certain bytes, such as space characters, which typically denote word boundaries. Our experiments show that **for a fixed training and inference compute budget, SpaceByte outperforms other byte-level architectures and roughly matches the performance of tokenized Transformer architectures.**Paper: https://arxiv.org/abs/2404.14408Github: https://github.com/kjslag/spacebyteAbstract:Tokenization is widely used in large language models because it significantly improves performance. However, tokenization imposes several disadvantages, such as performance biases, increased adversarial vulnerability, decreased character-level modeling performance, and increased modeling complexity. To address these disadvantages without sacrificing performance, we propose SpaceByte, a novel byte-level decoder architecture that closes the performance gap between byte-level and subword autoregressive language modeling. SpaceByte consists of a byte-level Transformer model, but with extra larger transformer blocks inserted in the middle of the layers. We find that performance is significantly improved by applying these larger blocks only after certain bytes, such as space characters, which typically denote word boundaries. Our experiments show that for a fixed training and inference compute budget, SpaceByte outperforms other byte-level architectures and roughly matches the performance of tokenized Transformer architectures.

https://preview.redd.it/v1xo6g1gzewc1.jpg?width=1507&format=pjpg&auto=webp&s=f9d415307b60639fa67e8a54c8769fa5a6c10f04

https://preview.redd.it/edvqos1gzewc1.jpg?width=1654&format=pjpg&auto=webp&s=f91c8727017e1a1bc7b80bb77a8627ff99182607

https://preview.redd.it/fe6z6i1gzewc1.jpg?width=1181&format=pjpg&auto=webp&s=24d955f30b8ca3eaa7c527f3f40545ed493f789c",MachineLearning,77,5,1713958927.0,1cbw0bn,Singularian2501,https://www.reddit.com/r/MachineLearning/comments/1cbw0bn/r_spacebyte_towards_deleting_tokenization_from/,Research
"[D] What is the current state on getting an ""inverse"" of a Neural network","To Clarify what I mean (also my background is more statistical but I've a problem with a quite nonlinear relationship)

Say I have inputs (predictor variables)  for example: \[x1,...,x10\] which are all inherently numerical (ie no dummies) , and a continuous numerical output y, and say I fit some NN as y \~ x1 +... x10  (we can assume a relatively simple architecture, ie no CNN/RNNs )

If I then say was given \[x2..x10,y\] is there a way to predict what value of x1 is expected.

Some current thoughts I have, for a relatively simple statistical model which continuously maps the relationship between x1 and y with everything else fixed ( like a linear regression) this is trivial. From a neural network I'm guessing certain conditions would need to be made to the structure if this was to work, eg any activation functions would need to be themselves invertible.

I'm wondering are this something that is actively used or is there any research on this. Alternatively would a better option just be create two models

y = F(x1,...,x10) and x1 = G(x2,.,x10,y)

Thanks in advanced",MachineLearning,74,32,1730473662.0,1gh7lc3,Eamo853,https://www.reddit.com/r/MachineLearning/comments/1gh7lc3/d_what_is_the_current_state_on_getting_an_inverse/,Discussion
[D] ACL ARR June (EMNLP) Review Discussion ,Too anxious about reviews as they didn’t arrive yet! Wanted to share with the community and see the reactions to the reviews! Rant and stuff! Be polite in comments. ,MachineLearning,78,696,1721882749.0,1ebmas6,always_been_a_toy,https://www.reddit.com/r/MachineLearning/comments/1ebmas6/d_acl_arr_june_emnlp_review_discussion/,Discussion
[R] Memory^3 : Language Modeling with Explicit Memory,"**TL;DR** who needs plain text knowledge database when you can use memory?

**Paper:** [https://arxiv.org/pdf/2407.01178](https://arxiv.org/pdf/2407.01178)

**Abstract:**

>The training and inference of large language models (LLMs) are together a costly process that transports knowledge from raw data to meaningful computation. Inspired by the memory hierarchy of the human brain, we reduce this cost by equipping LLMs with explicit memory, a memory format cheaper than model parameters and text retrieval-augmented generation (RAG). Conceptually, with most of its knowledge externalized to explicit memories, the LLM can enjoy a smaller parameter size, training cost, and inference cost, all proportional to the amount of remaining ""abstract knowledge"". As a preliminary proof of concept, we train from scratch a 2.4B LLM, which achieves better performance than much larger LLMs as well as RAG models, and maintains higher decoding speed than RAG. The model is named Memory^(3), since explicit memory is the third form of memory in LLMs after implicit memory (model parameters) and working memory (context key-values). We introduce a memory circuitry theory to support the externalization of knowledge, and present novel techniques including a memory sparsification mechanism that makes storage tractable and a two-stage pretraining scheme that facilitates memory formation.

**Visual abstract:**

https://preview.redd.it/7ikm0npl0ybd1.png?width=1111&format=png&auto=webp&s=0fc2c48f224324add4c8802d55232922779124d1

**Highlights:**

>\[O\]ur model first converts a knowledge base (or any text dataset) into explicit memories, implemented as sparse attention key-values, and then during inference, recalls these memories and integrates them into the self-attention layers. Our design is simple so that most of the existing Transformer-based LLMs should be able to accommodate explicit memories with a little finetuning, and thus it is a general-purpose “model amplifier”.

>...

>Knowledge traversal happens when the LLM wastefully invokes all its parameters (and thus all its knowledge) each time it generates a token. As an analogy, it is unreasonable for humans to recall everything they learned whenever they write a word. Let us define the knowledge efficiency of an LLM as the ratio of the minimum amount of knowledge sufficient for one decoding step to the amount of knowledge actually used. An optimistic estimation of knowledge efficiency for a 10B LLM is 10^(−5) : On one hand, it is unlikely that generating one token would require more than 10^(4) bits of knowledge (roughly equivalent to a thousand-token long passage, sufficient for enumerating all necessary knowledge); on the other hand, each parameter is involved in the computation and each stores at least 0.1 bit of knowledge \[7, Result 10\] (this density could be much higher if the LLM is trained on cleaner data), thus using 10^(9) bits in total.

>...

>During inference, as illustrated in Figure 9, whenever the LLM generates 64 tokens, it discards the current memories, uses these 64 tokens as query text to retrieve 5 new memories, and continues decoding with these memories. Similarly, when processing the prompt, the LLM retrieves 5 memories for each chunk of 64 tokens. Each chunk attends to its own memories, and the memories could be different across chunks. We leave it to future work to optimize these hyperparameters. The retrieval is performed with plain vector search with cosine similarity. The references as well as the query chunks are embedded by BGE-M3, a multilingual BERT model \[17\].

>...

>Hence, the total sparsity is 160 or 1830 (without or with vector compression). *\[Where vector compression refers to hard drive/RAM data and decompression happens on GPU\]* Originally, the explicit memory bank would have an enormous size of 7.17PB or equivalently 7340TB (given the model shape described in Section 3.4 and saved in bfloat16). Our compression brings it down to 45.9TB or 4.02TB (without or with vector compression), both acceptable for the drive storage of a GPU cluster.

**Graphical highlights:**

https://preview.redd.it/z32yzuv63ybd1.png?width=1137&format=png&auto=webp&s=e5a96196ef2878c595914658d100e075dbe09dce

https://preview.redd.it/xjtomj493ybd1.png?width=1163&format=png&auto=webp&s=b1d035b2cf6e664c65f06d714670c3a137e7dc99

https://preview.redd.it/xhwbvn2c3ybd1.png?width=1159&format=png&auto=webp&s=7f7f833fdca9f2a0ee4c2a3e3b6bdc83474dcb65

https://preview.redd.it/3cm19j7f3ybd1.png?width=1153&format=png&auto=webp&s=8a53819d3ffbf97390ba28f69da792e8c7690392

",MachineLearning,77,15,1720728160.0,1e0y7do,StartledWatermelon,https://www.reddit.com/r/MachineLearning/comments/1e0y7do/r_memory3_language_modeling_with_explicit_memory/,Research
[D] Expectation from Machine Learning Engineering jobs,"Hey everyone,

I’ve seen a lot of posts here about careers in ML and landing internships or jobs, and two things come up a lot

1. Building a strong research portfolio and publishing at conferences like NeurIPS, ICLR, and ICML, which seems to focus more on getting research scientist roles.

2. The growing demand for Machine Learning Engineer (MLE) roles, which are apparently more in demand than research scientist positions.

I’m curious about the difference between these two roles and what kind of portfolio would be ideal for landing an MLE position. I know having a master’s degree is often preferred, but is an impressive publication record necessary for MLE roles? Or is it not that big of a deal?

What are your thoughts?",MachineLearning,79,21,1731892454.0,1gtt099,ziggyboom30,https://www.reddit.com/r/MachineLearning/comments/1gtt099/d_expectation_from_machine_learning_engineering/,Discussion
"[Discussion] Now that i have an engineering job, how do i keep updated on latest interesting papers ?","Hey guys, in the past i used to work in a lab, doing researsh on computer vision & ML. Talking with professors and PhDs, i would have a good idea of new interresting articles. Now that i work in a big company, i don't have this network anymore and i don't have time to spend hours searshing new interresting articles. Are there any  good ressources that aggregate cool articles related to ML & CV ?",MachineLearning,72,22,1729458422.0,1g893lr,Fugius,https://www.reddit.com/r/MachineLearning/comments/1g893lr/discussion_now_that_i_have_an_engineering_job_how/,Discussion
[P] Comgra: A Tool for Analyzing and Debugging Neural Networks,"I'm a machine learning engineer and researcher. I got fed up with how difficult it is to understand why neural networks behave the way they do, so i wrote a library to help with it.

[Comgra (computation graph analysis)](https://github.com/FlorianDietz/comgra) is a library you can use with pytorch to extract all the tensor data you care about and visualize it graphically in a browser. [A paper on it](https://openreview.net/pdf?id=TcMmriVrgs) has been accepted as a spotlight paper at the ICML 2024 Workshop on Mechanistic Interpretability.

Comgra allows for a much more detailed analysis of what is happening than the usual approach of using tensorboard. You can go investigate tensors as training proceeds, drill down into individual neurons, inspect single data sets that are of special interest to you, track gradients, compare statistics between different training runs, and more.

This tool has saved me a ton of time in my research by letting me check my hypotheses much more quickly than normal and by helping me understand how the different parts of my network really interact.",MachineLearning,73,19,1726754823.0,1fklqcz,Smart-Emu5581,https://www.reddit.com/r/MachineLearning/comments/1fklqcz/p_comgra_a_tool_for_analyzing_and_debugging/,Project
[Discussion] Thoughts on knowledge graphs and graph neural networks ,"A few years ago, my data science team dreamed of implementing a knowledge graph and leveraging graph neural networks. This approach seemed particularly promising in finance, the industry where I work, as it would enable models to capture indirect relationships—for example, how a change in ownership could affect a company's performance.

Back then, it felt like a pipe dream. Capturing any relationship (such as ""owned by"" or ""sells a product"") required its own NLP model. However, the advent of LLMs has significantly reduced this complexity (and is now implemented in [LlamaIndex](https://www.llamaindex.ai/blog/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms)). So we are wondering whether we should give KGs and GNNs another shot. The idea would be to use LLMs to help us build a KG and to add data from our other databases to it. Then, we would train GNNs to predict things like ""Will company A buy company B"" or ""Will company C outperform company D.""

However, despite being regularly touted as the next big thing, GNNs remain somewhat niche. Okay, they're used to complement RAG, but I have not heard of any non-big-tech firm setting up its super-duper knowledge graph. Based on what I have read, graph databases face a ton of criticism because of performance issues and the difficulty of creating effective schemas, among other things.

What has your experience been with these technologies? Do you have any success stories or cautionary tales to share?

**\[edit\]** This post got a lot more attention than I though, so I reworked it a bit to save everybody's time. In particular, I tried to clarify that KG and GNN are different. The convergence of these two technologies seems promising, but I have two big concerns:

* Neo4j, the leading graph database provider, seems to be the main knowledge provider on this topic. It even authored at least two books edited by O'Reilly(!), so it's difficult to get a sense of knowledge graph pitfalls.
* Almost nobody that I know has implemented GNNs at scale. ",MachineLearning,74,41,1722377108.0,1eg674y,MeditationBeginner,https://www.reddit.com/r/MachineLearning/comments/1eg674y/discussion_thoughts_on_knowledge_graphs_and_graph/,Discussion
[D] Anyone see any real usage of Kolmogorov-Arnold Networks in the wild?,"KANs were all the hype everywhere (including Reddit), and so many people had so much to say about it, although not all good. It's been around 3 months now. Has anyone seen anything to either corroborate or contradict the ""believers""? Personally, I have not seen the adoption of KANs anywhere noteworthy. Would like to hear from the community.",MachineLearning,73,39,1719600912.0,1dqr9gh,Sad-Journalist752,https://www.reddit.com/r/MachineLearning/comments/1dqr9gh/d_anyone_see_any_real_usage_of_kolmogorovarnold/,Discussion
"[D] Why does developing these RAG applications feel like alchemy?
","\^ Basically the title. Is there a principled way of doing this? Like Weights & Biases, where you can at least monitor what's happening.",MachineLearning,72,48,1719104336.0,1dmabuy,latentnumber,https://www.reddit.com/r/MachineLearning/comments/1dmabuy/d_why_does_developing_these_rag_applications_feel/,Discussion
Why use squared error instead of Absolute error? [D],"I dont understand why getting an undefined partial derivate when error = 0 can be a huge problem, I mean getting zero error is not what we all wanted from the start??",MachineLearning,77,74,1718125654.0,1ddjbdi,NeatJealous8110,https://www.reddit.com/r/MachineLearning/comments/1ddjbdi/why_use_squared_error_instead_of_absolute_error_d/,Discussion
[D] PCA vs AutoEncoders for Dimensionality Reduction,"The title sums it up. I'm working on some anonymized time-series data, initially, I built an AutoEncoder in order to replace the decoder head with a regression head instead after training.

As for preprocessing steps, I would usually just subtract the mean of features and divide by their standard deviation, Although I've long heard that doing ""data decorrelation"" is helpful, so I decided to finally learn about PCA.

My questions are the following:

1. If PCA serves to find the principle underlying features of a dataset, is there any point in using an autoencoder? (Especially if there are high correlations between some features)
2. If there is still a point to using autoencoders, should one use PCA on their dataset first to decorrelate data, or is that just redundant, or perhaps another reason not to use it is that it can erase some information? (Although it's an invertible transformation so I don't see how information would be lost)
3. Is PCA as a preprocessing step beneficial to tree-building algorithms? I haven't seen much talk of it, but it seems intuitive to me that having decision nodes on principle component axes would lead to better results.",MachineLearning,75,34,1731877006.0,1gtng8q,DisciplinedPenguin,https://www.reddit.com/r/MachineLearning/comments/1gtng8q/d_pca_vs_autoencoders_for_dimensionality_reduction/,Discussion
[P] How to extract insights from 500k chat messages using LLMs?,"Hi all,

I downloaded the chat messages from a discord server on AI and they amounted to ~500k messages over 2-3 years. My reason for doing this is that I'd like to extract insights/tips & tricks on the subject that you might not find in a tutorial online (I've always found being in discord servers where people help each other to be much more densely informative than reading various blog posts/tutorials). 

They amount to around 8m tokens which would cost 1-2$ using gpt-4o-mini, or 20-30$ using gpt-4o, which is pretty reasonable.

However I'm trying to figure two things out:

1) whether I can use a local llm for part of the process. That'd be preferred since while gpt-4o-mini would only cost between 1-2$, that's per prompt, and I might want to query/process the data in multiple ways.

2) what exactly could I do to extract the most valuable insights? Probably 95% of the chat is just banter but 5% is probably full of useful advice. What sort of prompts could I use? And how would I handle the fact that I'd need to chunk the input to fit into the context window?

I'm open to learning and exploring any new topic to go about this, as I'm excited to take it on as a project to get my hands dirty with LLMs. ",MachineLearning,74,47,1729191895.0,1g5yn4b,PMMEYOURSMIL3,https://www.reddit.com/r/MachineLearning/comments/1g5yn4b/p_how_to_extract_insights_from_500k_chat_messages/,Project
[D] Found this Open-Sourced Codebase implementing Shazam's ML algo,,MachineLearning,73,4,1725214916.0,1f6l7ya,Particular_Tap_4002,https://github.com/cgzirim/seek-tune,Discussion
[P] Tricycle: Autograd to GPT-2 completely from scratch,"I wanted to share Tricycle: a fast, fully functional deep learning framework I've built completely from scratch: [ https://github.com/bclarkson-code/Tricycle/](https://github.com/bclarkson-code/Tricycle/).

The biggest milestone so far is training GPT-2(124M) on 2.3B tokens in 68 hours on a single RTX 3090 and I'm working on scaling things up further.

The entire library has been built from scratch, from an AutoGrad engine all the way to GPT-2, and should be understandable to anyone with a bit of python experience. I've tried to keep the code as simple as I can without hiding anything and I've added a wiki that walks through how I built everything.

I'd love to hear what you think!

Edit: Grammar",MachineLearning,72,11,1721147172.0,1e4sz1e,Efficient_Plankton_9,https://www.reddit.com/r/MachineLearning/comments/1e4sz1e/p_tricycle_autograd_to_gpt2_completely_from/,Project
[P] Tiny Time Mixers(TTMs): Powerful Zero-Shot Forecasting Models by IBM,"A new **open-source** foundation Time-Series model by IBM:

[https://aihorizonforecast.substack.com/p/tiny-time-mixersttms-powerful-zerofew?-reml--](https://aihorizonforecast.substack.com/p/tiny-time-mixersttms-powerful-zerofew?post_page-reml--)",MachineLearning,75,16,1717937705.0,1dbt398,apaxapax,https://www.reddit.com/r/MachineLearning/comments/1dbt398/p_tiny_time_mixersttms_powerful_zeroshot/,Project
[D] How to discredit your whole paper in one figure,"arxiv.org/abs/2410.13854
Did they just really compare English-language memes to traditional Chinese paintings and use that as the basis for ""Chinese images are harder to understand"" (figure 1)?

Edit: I believe the rest of the paper is sensible and the cultural background necessary for understanding traditional Chinese art is important but the comparison is dishonest. The examples they give in Appendix B (figure 7) are better.",MachineLearning,74,32,1729399572.0,1g7r3hn,qu3tzalify,https://www.reddit.com/r/MachineLearning/comments/1g7r3hn/d_how_to_discredit_your_whole_paper_in_one_figure/,Discussion
[R] SVGFusion: Scalable Text-to-SVG Generation via Vector Space Diffusion,"**Paper:** [https://arxiv.org/pdf/2412.10437](https://arxiv.org/pdf/2412.10437)

**Abstract:** 

>The generation of Scalable Vector Graphics (SVG) assets from textual data remains a significant challenge, largely due to the scarcity of high-quality vector datasets and the limitations in scalable vector representations required for modeling intricate graphic distributions. This work introduces SVGFusion, a Text-to-SVG model capable of scaling to real-world SVG data without reliance on a text-based discrete language model or prolonged SDS optimization. The essence of SVGFusion is to learn a continuous latent space for vector graphics with a popular Text-to-Image framework. Specifically, SVGFusion consists of two modules: a Vector-Pixel Fusion Variational Autoencoder (VP-VAE) and a Vector Space Diffusion Transformer (VS-DiT). VP-VAE takes both the SVGs and corresponding rasterizations as inputs and learns a continuous latent space, whereas VS-DiT learns to generate a latent code within this space based on the text prompt. Based on VP-VAE, a novel rendering sequence modeling strategy is proposed to enable the latent space to embed the knowledge of construction logics in SVGs. This empowers the model to achieve human-like design capabilities in vector graphics, while systematically preventing occlusion in complex graphic compositions. Moreover, our SVGFusion's ability can be continuously improved by leveraging the scalability of the VS-DiT by adding more VS-DiT blocks. A large-scale SVG dataset is collected to evaluate the effectiveness of our proposed method. Extensive experimentation has confirmed the superiority of our SVGFusion over existing SVG generation methods, achieving enhanced quality and generalizability, thereby establishing a novel framework for SVG content creation. Code, model, and data will be released at: {[this https URL](https://ximinng.github.io/SVGFusionProject/)}

*(Note: so far, nothing has been released in the linked repo)*

**Visual Abstract:**

https://preview.redd.it/3ypl3eph1e7e1.png?width=1153&format=png&auto=webp&s=90d0dab4a2311d7bd8037322bb265ce8a03becac

**Visual Highlights:**

https://preview.redd.it/ngz1kfen2e7e1.png?width=1175&format=png&auto=webp&s=e8a49b15283c3514f7d2703ad631b8de6f63b5d2

https://preview.redd.it/o01j02fp2e7e1.png?width=635&format=png&auto=webp&s=230f437f4513498916f317e2e2f7ccc7b0578e8c

https://preview.redd.it/3s4k4c1r2e7e1.png?width=657&format=png&auto=webp&s=7402ee8bdbb8cc4d9225a48f33eebb7e051ec1fb

https://preview.redd.it/d4yi1bx23e7e1.png?width=1201&format=png&auto=webp&s=9f115b99babd31998e23986238e41e91b49e1ff3

[Zoomed in, as per the suggestion](https://preview.redd.it/w6l2m9a53e7e1.png?width=265&format=png&auto=webp&s=d663250708ec3ca4548bd106384aca515e019793)

https://preview.redd.it/gpatd9qy3e7e1.png?width=1279&format=png&auto=webp&s=ac70bbb82f934109ec8388eeca65fd49d4b994df

https://preview.redd.it/6f40o3pz3e7e1.png?width=793&format=png&auto=webp&s=a71f7ddcd311eefb365907ae067a5b287630253c

https://preview.redd.it/50gzr8p04e7e1.png?width=807&format=png&auto=webp&s=265be1e5ce20aded6913d24db371b4380e8c92b1

",MachineLearning,72,8,1734433116.0,1hg87em,StartledWatermelon,https://www.reddit.com/r/MachineLearning/comments/1hg87em/r_svgfusion_scalable_texttosvg_generation_via/,Research
[R] Paper summaries for some of our papers that recently got accepted in NeurIPS,"Hey everyone, here is the list of papers by our groups that got accepted recently in NeurIPS 2024; It is a proud moment for us as an all-UG group; all the papers were published without any external support from the academia; here is a summary of our papers. We hope this inspires others to pursue AI and look into research as a perspective where we can work together, and all you require is the right guidance (not even necessarily a PhD or a professor). If you find these papers useful and want to working/collabrating with us, feel free to connect with us!

* Give me a hint: Can LLMs take a hint to solve math problems? 👉 [Arxiv link](https://arxiv.org/abs/2410.05915)
   * We propose improving LLM performance on advanced math problems using ""hints,"" inspired by human pedagogy. We also test the model's robustness to incorrect hints. Our approach is evaluated on various LLMs using diverse problems from the MATH dataset, comparing it with one-shot, few-shot, and chain of thought prompting.
* Attention Shift: Steering AI Away from Unsafe Content 👉 [Arxiv link](https://arxiv.org/abs/2410.04447)
   * This study explores methods to restrict unsafe content in generative models. We propose a novel training-free approach using attention reweighing to remove unsafe concepts during inference. Our method is compared to existing techniques, evaluated on direct and adversarial jailbreak prompts. We also discuss potential causes, limitations, and broader implications.
* Unmasking the Veil: An Investigation into Concept Ablation for Privacy and Copyright Protection in Images 👉 [Arxiv link](https://arxiv.org/abs/2406.12592v1)
   * This paper extends the study of concept ablation in pre-trained models, as introduced by Kumari et al. (2022). We reproduce results from various concept ablation techniques and propose a novel variant, ""trademark ablation,"" to address branded elements in model outputs. We also analyze the model's limitations, behavior under ablation leakage prompts, and performance degradation on unrelated concepts.

**The Vision Language Group at IIT Roorkee** has compiled an excellent repository of **comprehensive summaries** for deep learning papers from top conferences like **NeurIPS, CVPR, ICCV, and ICML (2016-2024)**. These summaries break down key papers in computer vision, NLP, and machine learning—perfect if you want to stay updated without diving deep into the full papers.",MachineLearning,74,21,1729788132.0,1gb74j6,vlg_iitr,https://www.reddit.com/r/MachineLearning/comments/1gb74j6/r_paper_summaries_for_some_of_our_papers_that/,Research
[D] Llama 3.2 Detailed Analysis,"Hey folks! Meta released a new set of Llama 3.2 models for text (1B, 3B) and vision (11B, 90B). I took a deep dive of the models and hopefully it's insightful:

1. New 1B and 3B text only LLMs 9 trillion tokens
2. New 11B and 90B vision multimodal models
3. 128K context length
4. 1B and 3B used some distillation from 8B and 70B
5. VLM 6 billion img, text pairs
6. CLIP MLP GeLU + cross attention

Long analysis:
1. CLIP type MLP with GeLU activation used in vision encoder. Similar to GPT2's MLP. Different to Llama 3's MLP since SwiGLU is not used for the vision MLP.

2. Normal layernorm used for vision encoder - not RMS Layernorm. Also some ""gating"" parameter is used to multiply the hidden states.

3. Gating multiplier done to hidden states after attention and MLP - tanh used to move vector scaling to numbers from -1 to 1.

4. Evals look pretty good for small 1B and 3B LLMs and multimodal VLMs 11B and 90B. 1B 49.3 MMLU and 3B 63.4. VLM MMMU 50.7 and 90B 60.3

Thank you for reading and if you have any questions please let me know!",MachineLearning,71,25,1727291344.0,1fpckbb,danielhanchen,https://www.reddit.com/r/MachineLearning/comments/1fpckbb/d_llama_32_detailed_analysis/,Discussion
"[P] Inspired by Andrej Karpathy, I made NLP - Zero to Hero",,MachineLearning,70,6,1725105399.0,1f5ljxq,Particular_Tap_4002,https://github.com/JUSTSUJAY/nlp-zero-to-hero,Project
[P] Opensource Microsoft Recall AI,"I created an open source alternative to Microsoft's Recall AI.

This records everything on your screen and can be searched through using natural language latter. But unlike Microsoft 's implementation this isnt a privacy nightmare and is out for you to use right now. 
and comes with real time encryption

It is a new starting project and is in need of Contributions so please hope over to the github repo and give it a star

https://github.com/VedankPurohit/LiveRecall

It is completely local and you can have a look at code. And everything is always encrypted unlike Microsofts implications where when you are logged in the images are decripted and can be stolen",MachineLearning,72,50,1718256635.0,1dergc6,Vedank_purohit,https://www.reddit.com/r/MachineLearning/comments/1dergc6/p_opensource_microsoft_recall_ai/,Project
"[P] This week, I implemented the paper, ""Pay Attention to MLPs"", in Tinygrad! :D","To experiment with more interesting model architectures, I implemented gMLP in Tinygrad!

If anyone wants to give some feedback, it will be welcomed.

* \[Repository\]: [https://github.com/EthanBnntt/tinygrad-gmlp](https://github.com/EthanBnntt/tinygrad-gmlp)
* \[Installation\]: `pip install gmlp_tinygrad`
* \[Original Paper\]: [https://doi.org/10.48550/ARXIV.2105.08050](https://doi.org/10.48550/ARXIV.2105.08050)

[A diagram showing the gMLP architecture](https://preview.redd.it/3s58nla804nd1.png?width=330&format=png&auto=webp&s=3f00f8364e9e0ac00da13ce28199d59330ebaef3)",MachineLearning,72,14,1725593365.0,1fa5kop,the-wonderful-world,https://www.reddit.com/r/MachineLearning/comments/1fa5kop/p_this_week_i_implemented_the_paper_pay_attention/,Project
[D] Thoughts on Best Python Timeseries Library,"There are many python libraries offering implementations of contemporary timeseries models and data tools. Here is an (incomplete) list. Looking for feedback from anyone who has used any of these (or others) on their pros and cons. Extra points if you have used more than one and can offer an opinionated comparison. I am trying to figure out which one(s) to invest time into. Much appreciated!

* TSA - [https://github.com/timeseriesAI/tsai](https://github.com/timeseriesAI/tsai)
* TSLib - [https://github.com/thuml/Time-Series-Library](https://github.com/thuml/Time-Series-Library)
* AEON - [https://github.com/aeon-toolkit/aeon](https://github.com/aeon-toolkit/aeon)
* SKTime - [https://www.sktime.net/en/stable/](https://www.sktime.net/en/stable/)
* TSLearn - [https://tslearn.readthedocs.io/en/stable/](https://tslearn.readthedocs.io/en/stable/)
* Nixtla - [https://github.com/Nixtla/](https://github.com/Nixtla/)
* Pytorch-forcasting - [https://pytorch-forecasting.readthedocs.io/en/stable/](https://pytorch-forecasting.readthedocs.io/en/stable/)
* DARTS - [https://unit8co.github.io/darts/index.html](https://unit8co.github.io/darts/index.html)
* Merlion - [https://github.com/salesforce/Merlion](https://github.com/salesforce/Merlion)",MachineLearning,71,21,1719424461.0,1dp4y8p,HorseEgg,https://www.reddit.com/r/MachineLearning/comments/1dp4y8p/d_thoughts_on_best_python_timeseries_library/,Discussion
[D]  What are the hot topics in Machine Learning Research in 2024?,"Which of the sub-fields/approaches, application areas are expected to gain much attention (pun unintended) this year in the academia or industry?

PS: Please don't shy away from suggesting anything that you think or know could be the trending research topic in ML, it is quite likely that what you know can be relatively unknown to many of us here :)",MachineLearning,69,59,1718121965.0,1ddhu8n,knut_2,https://www.reddit.com/r/MachineLearning/comments/1ddhu8n/d_what_are_the_hot_topics_in_machine_learning/,Discussion
[R]Geometric intuition why L1 drives the coefficients to zero,,MachineLearning,68,5,1735511680.0,1hp7us9,madiyar,https://maitbayev.github.io/posts/why-l1-loss-encourage-coefficients-to-shrink-to-zero/,Research
[D] Structure of Neural Embeddings,,MachineLearning,69,20,1735423771.0,1hogog5,SeanPedersen,https://seanpedersen.github.io/posts/structure-of-neural-latent-space,Discussion
[D] Why is Monte Carlo Tree Search the only go-to method for incremental game tree search?,"I noticed that whenever a search method is needed such that its quality scales with inference time compute, people always go for MCTS without ever thinking about other kind of search methods. Looking at the widely used version of MCTS (e.g. with UCB and so on), it’s clear that a lot of heuristic is hand-crafted. Is there any research on better search methods (perhaps one that is meta-learned)? I feel like there’s a lot of opportunities where the hand-crafted heuristic process can be improved.",MachineLearning,72,8,1734745298.0,1hizb1u,TommyX12,https://www.reddit.com/r/MachineLearning/comments/1hizb1u/d_why_is_monte_carlo_tree_search_the_only_goto/,Discussion
[D] What Neural Network Architecture is best for Time Series Analysis with a few thousand data points?,"I know what you're thinking, use classical methods like ARIMA. Yes you are correct, but I have already done that for my company. I am currently a co-op and I got a full time offer. During this transition to it, I don't have much to do for two weeks. I have access to PySpark and Databricks which I won't in the new position so I wanna take this time as a learning experience and it'll help my resume in the end. I am not expecting the performance to be better than my ARIMA models

The data has daily granularity from 2021. I have features but not a ton of features. There are three architectures which I've been considering. I know about RNN's, LSTMs and Temporal CNN's. In terms (mostly) learning combined with performance, which of these do you think are most suited for my task? In general for rich data, what architecture do you see usually performing the best?",MachineLearning,69,31,1727360918.0,1fpxja7,BostonConnor11,https://www.reddit.com/r/MachineLearning/comments/1fpxja7/d_what_neural_network_architecture_is_best_for/,Discussion
[D] Is scientific machine learning actually used in practice? ,"As someone whose background straddles both scientific computing and machine learning, I hear a lot about scientific machine learning (SML). The promise is that one can use machine learning to either speed up, simplify or otherwise improve numerical models. A common example use-case is that one can use high-fidelity numerical simulations (which can be very slow to run) as training data, and then train a neural network on these simulations to predict the results of numerical simulations much faster than running the actual simulation (thereby obtaining a reduced order model). This could be very useful for e.g. digital twins, where you might want to compute fluid dynamics over a wind-turbine in real time while respecting the governing fluid equations and incorporating ever changing sensor data of the wind, temperature etc. in order to predict mishaps, optimisations and so on. I have only heard about this, and other use cases, in academic settings. 

My question is, is scientific machine learning actually used in practice (industry)? Can anyone point to any real-world examples? Any companies that actually use this technology? If not, I would love to hear suggestions of why it seemingly doesn't provide any value to the market (at least for now). What are some of the roadblocks / bottleneck for adoption of these methods in industry? Or is scientific machine learning just a contrived pairing of two otherwise useful fields, simply for the sake of academic curiosity and writing grant proposals? ",MachineLearning,67,44,1721491760.0,1e7z4s0,worstthingsonline,https://www.reddit.com/r/MachineLearning/comments/1e7z4s0/d_is_scientific_machine_learning_actually_used_in/,Discussion
[P] ReRecall: I tried to recreate Microsoft's Recall using open-source models & tools,"Recall sounds to me like a privacy nightmare, so I thought I might give it a try to make something similar using only open source components. Here is the code if you want to play around with it:

[https://github.com/AbdBarho/ReRecall](https://github.com/AbdBarho/ReRecall)

Overall it went better than I expected, I use \`mss\` to take screenshots of the monitor(s), and use ollama and llava and mxbai embed to generate descriptions and embeddings of the screenshots, and then chromadb for storage and search.

There is definitely huge room for improvement here:

* There are plenty of hallucinations in the generated descriptions of screenshots, this could be a combination of the size the MLLM used to generate the descriptions (I use a very small model because I have a rusty 1060), or because the screenshots are very high in resolutions (no resizing is done after a screenshot).
* The search is very basic, it just matches the embeddings of the query text with the embeddings of the screenshots, a potential improvement could be to use the model to enrich the user query with more information before embedding it for search.
* I am fairly certain that Microsoft does not rely solely on screenshots as I do, but also captures of individual app windows, and also extracts meta information like window title, maybe even the text content of the window (the same text used by text-to-speech programs for the visually impaired), these could definitely improve the results.

Do you have any further ideas on what could be changed?

Example (cherrypicked):

[Screen on the right with the corresponding ReRecall usage on the left](https://preview.redd.it/d6nhygdgns2d1.jpg?width=3000&format=pjpg&auto=webp&s=d119a70013169a6a41a8e4cc7e8187cd91d551cb)",MachineLearning,71,8,1716739714.0,1d14pad,Abdoo2,https://www.reddit.com/r/MachineLearning/comments/1d14pad/p_rerecall_i_tried_to_recreate_microsofts_recall/,Project
[D] Why aren't Stella embeddings more widely used despite topping the MTEB leaderboard?,"https://huggingface.co/spaces/mteb/leaderboard

I've been looking at embedding models and noticed something interesting: Stella embeddings are crushing it on the MTEB leaderboard, outperforming OpenAI's models while being way smaller (1.5B/400M params) and apache 2.0. Makes hosting them relatively cheap.

For reference, Stella-400M scores 70.11 on MTEB vs OpenAI's text-embedding-3-large 64.59. The 1.5B version scores even higher at 71.19

Yet I rarely see them mentioned in production use cases or discussions. Has anyone here used Stella embeddings in production? What's been your experience with performance, inference speed, and reliability compared to OpenAI's offerings?

Just trying to understand if there's something I'm missing about why they haven't seen wider adoption despite the impressive benchmarks.

Would love to hear your thoughts and experiences!",MachineLearning,67,20,1732794344.0,1h1u814,sdsd19,https://www.reddit.com/r/MachineLearning/comments/1h1u814/d_why_arent_stella_embeddings_more_widely_used/,Discussion
[R] Who’s a Good Boy? A Metropolis-Hastings Approach to Determining Foster Dog Names of Unknown Origin,,MachineLearning,70,8,1726021119.0,1fdzemt,TobyWasBestSpiderMan,https://www.reddit.com/gallery/1fdz13f,Research
[R] Adam Optimizer Causes Privileged Basis in Transformer Language Models,,MachineLearning,68,40,1725726365.0,1fbavdv,rrenaud,https://www.lesswrong.com/posts/yrhu6MeFddnGRSLtQ/adam-optimizer-causes-privileged-basis-in-transformer,Research
[D] 'Deep-Work' while working on deep models,"Hi All,

One of my biggest productivity challenges is the downtime while waiting for deep learning training loops, tokenization or processing loops to run. These can take anywhere from 5 minutes to an hour for the short ones, and during this time, I often find myself at a loss for what to do.  
Starting a new task is tough because the constant context switching disrupts my workflow and focus.

I used to follow deep-work methods in the university, which really helped manage my ADHD. I didn't used phones or social media during the day, and 'focused' on a single task at a time.  
Now, I feel like it almost impossible. I'm 'forced' into taking these mini-breaks, constantly switching between tasks, and it's been quite challenging.

Do you have any suggestions on how to make the most of these intervals? Do you save specific tasks for these periods?  
Even switching from focused coding to reading papers is really difficult if done 'only' for 10 minutes or so. 

Has anyone managed those problems, or it just me?

Thanks.",MachineLearning,70,20,1720357359.0,1dxg0jg,Magnospm,https://www.reddit.com/r/MachineLearning/comments/1dxg0jg/d_deepwork_while_working_on_deep_models/,Discussion
[R] Creativity Has Left the Chat: The Price of Debiasing Language Models,,MachineLearning,69,25,1718602746.0,1dhqs9g,hardmaru,https://arxiv.org/abs/2406.05587,Research
[P] Training a Text-to-Video Model from Scratch on a 196xH100 GPU Cluster,"Hi everyone! 👋 We've been training an open source Text-to-Video model (called Open-Sora 1.2) from scratch using 28,000 H100 GPU hours, and we've put together [a guide on GitHub](https://lambdalabsml.github.io/Open-Sora/lessons/) to share some of the lessons we learned along the way. Here's a handful of the topics covered:

* **Key challenges in distributed training** like distributed debugging with py-spy to handle cluster-wide problems, NCCL errors and convergence issues.
* **Training monitoring** with intermediate results to show expected outcomes after specific training hours of the multi-stage training recipe.
* **Parallelizing dataset preparation** for T2V, including how to efficiently parallelize preprocessing tasks on a cluster.

Here’s a link to the guide: [link](https://lambdalabsml.github.io/Open-Sora/lessons/).  
Check it out and let us know your thoughts! (PRs are always welcome.)",MachineLearning,67,3,1730970820.0,1glmfsr,lambda-research,https://www.reddit.com/r/MachineLearning/comments/1glmfsr/p_training_a_texttovideo_model_from_scratch_on_a/,Project
[P] Fully Bayesian Logistic Regression with Objective Prior,"I've been working on a project that implements deterministic, fully Bayesian logistic regression with reference prior for the case of a single weight.

[https://github.com/rnburn/bbai](https://github.com/rnburn/bbai)

In the single parameter case, the reference prior works out to be the same as [Jeffreys prior](https://en.wikipedia.org/wiki/Jeffreys_prior), which is given by

https://preview.redd.it/alskcnddsqwd1.png?width=1200&format=png&auto=webp&s=0d3dc78ae15122d21c78dcc2b7170b34c4bec88b

One of the main justifications for Jeffreys prior as an objective prior (or noninformative prior) for single parameter models is that it has asymptotically optimal frequentist matching coverage (see §0.2.3.2 of \[[1](https://www.uv.es/~bernardo/OBayes.pdf)\] and \[2\]).

*Note: The situation becomes more complicated for multi-parameter models, and this is where you will see reference priors and Jeffreys prior produce different results (see §0.2.3.3 of \[*[*1*](https://www.uv.es/~bernardo/OBayes.pdf)*\]).*

Frequentist matching coverage is something that can be easily measure by simulation. Here's a brief snippet of python code that shows how:

    from bbai.glm import BayesianLogisticRegression1
    import numpy as np
    
    # Measure frequentist matching coverage
    # for logistic regression with reference prior
    def compute_coverage(x, w_true, alpha):
        n = len(x)
        res = 0
    
        # iterate over all possible target values
        for targets in range(1 << n):
            y = np.zeros(n)
            prob = 1.0
            for i in range(n):
                y[i] = (targets & (1 << i)) != 0
                mult = 2 * y[i] - 1.0
                prob *= expit(mult * x[i] * w_true)
            
            # fit a posterior distribution to the data
            # set x, y using the reference prior
            model = BayesianLogisticRegression1()
            model.fit(x, y)
            
            # does a two-tailed credible set of probability mass
            # alpha contain w_true?
            t = model.cdf(w_true)
            low = (1 - alpha) / 2
            high = 1 - low
            if low < t and t < high:
                res += prob
        return res

Given a design matrix X, w\_true, and a target probability mass alpha, the code computes the frequentist matching coverage for Jeffreys prior. If I fix alpha to 0.95, draw X from a uniform distribution between \[-1, 1\], and try some different values of w\_true and n, I get these results:

[Frequentist coverage matching results for Jeffreys prior](https://preview.redd.it/s9mqe0mpuqwd1.png?width=1200&format=png&auto=webp&s=eb8bef7a376c22b426510de7392a73e8bb759f29)

We can see that the coverages are all fairly close to the target alpha. 

Notebook with full experiment: [https://github.com/rnburn/bbai/blob/master/example/22-bayesian-logistic1-coverage.ipynb](https://github.com/rnburn/bbai/blob/master/example/22-bayesian-logistic1-coverage.ipynb)

# Example: Election Polling

Suppose we want to make a simple polls-only model for predicting whether a presidential candidate will win a state given their lead in state-wide polls. Modeling the problem with single variable logistic regression, we have

https://preview.redd.it/wecqyq7hwqwd1.png?width=1200&format=png&auto=webp&s=7ff6b67985b94d71fcfd355ef7003092fe539466

Using the FiveThirtyEight results from 2020 (\[3\]) as training data, we can fit a posterior distribution to w:



[FiveThirtyEight polling results for 2020 \(\[3\]\). Blue indicates a state where Biden led, red Indicates a state where Trump led. A dot indicates that the leading candidate won the state and an X indicates the leading candidate lost the state.](https://preview.redd.it/2na8bjdvwqwd1.png?width=3840&format=png&auto=webp&s=7cf54a182aa689095158754fe3531a870fa0252c)

Here's how we can fit a model to the data set

    from bbai.glm import BayesianLogisticRegression1
    
    x_2020, y_2020 = # data set for 2020 polls
    
    # We specify w_min so that the prior on w is restricted
    # to [0, ∞]; thus, we assume a lead in polls will never 
    # decrease the probability of the candidate winning the
    # state
    model = BayesianLogisticRegression1(w_min=0)
    
    model.fit(x_2020, y_2020)

We can then get a sense for what it says the accuracy of state-wide polls by looking at percentiles for the prediction posterior distribution for a lead of 1% in polls.

    pred = model.predict(1) # prediction for a 1% polling lead
    
    for pct in [.5, .25, .5, .75, .95]:
        # Use the percentage point function (ppf) to
        # find the value of p where
        #   integrate_0^p π(p | xp=1, x, y) dp = pct
        # Here p denotes the probability of the candidate
        # winning the state when they are leading by +1%.
        print(pct, ':', pred.ppf(pct))

Produces the result

[Prediction posterior distribution for the probability of a candidate winning a state given a lead of 1&#37; in polling. The figure also shows the 5-th, 25-th, 50-th, 75-th, and 95-th percentiles.](https://preview.redd.it/lazu8vxfyqwd1.png?width=3840&format=png&auto=webp&s=e51030b1c635493fb7cfd2d38998b2fda20ff67d)

Notebook for the full example: [https://github.com/rnburn/bbai/blob/master/example/23-election-polls.ipynb](https://github.com/rnburn/bbai/blob/master/example/23-election-polls.ipynb)

# References

\[1\]: Berger, J., J. Bernardo, and D. Sun (2022). [Objective bayesian inference and its relationship to frequentism.](https://www.uv.es/~bernardo/OBayes.pdf?utm_source=www.objectivebayesian.com&utm_medium=referral&utm_campaign=how-to-use-objective-bayesian-inference-to-compare-binomial-proportions)

\[2\]: Welch, B. L. and H. W. Peers (1963). [On formulae for confidence points based on integrals of weighted likelihoods.](https://academic.oup.com/jrsssb/article-abstract/25/2/318/7035245?redirectedFrom=PDF&utm_source=www.objectivebayesian.com&utm_medium=referral&utm_campaign=how-to-use-objective-bayesian-inference-to-compare-binomial-proportions)*Journal of the Royal Statistical Society Series B-methodological 25*, 318–329. 

\[3\]: 2020 FiveThirtyEight state-wide polling averages. [*https://projects.fivethirtyeight.com/polls/president-general/2020/*](https://projects.fivethirtyeight.com/polls/president-general/2020/?utm_source=www.objectivebayesian.com&utm_medium=referral&utm_campaign=how-to-use-objective-bayesian-inference-to-interpret-election-polls)



  
",MachineLearning,71,11,1729794699.0,1gb9qxj,rnburn,https://www.reddit.com/r/MachineLearning/comments/1gb9qxj/p_fully_bayesian_logistic_regression_with/,Project
[D] [R] LLMs frameworks for research,"I'm a Ph.D. student in AI and NLP and I'm currently starting a new research project with LLMs.   
This time, instead of writing all the code from scratch, primarily using HuggingFace and Pytorch, I'd like to use one of the popular frameworks (like LangChain, LlamaIndex etc.).   
The motivation behind this is that, ideally, I'd like to learn to use these tools to get a more compact and organised codebase, such that I can easily add pieces to include RAG, Agentic workflows etc.   
I'm also interested in having an efficient way to load models and make inferences.  
  
In your experience, which of the many available frameworks out there is the most suitable for research purposes ? And do you even use a framework or you just code everything from scratch every time you start a new project ?",MachineLearning,66,22,1729608716.0,1g9k0te,Debonargon,https://www.reddit.com/r/MachineLearning/comments/1g9k0te/d_r_llms_frameworks_for_research/,Discussion
[P] A Visual Guide to Mixture of Experts (MoE) in LLMs,"Hi all! I’m excited to introduce a highly illustrative guide to Mixture of Experts (MoE) in LLMs!

From exploring the role of experts, their routing mechanism, the sparse MoE layer, and load balancing tricks (such as KeepTopK, auxiliary loss, and expert capacity), to MoE in vision models and computational requirements. 

[https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts)

I loved creating the visuals and had to stop myself after creating more than 55 custom visuals!

The visual nature of this guide allows for a focus on intuition, hopefully making all these techniques easily accessible to a wide audience, whether you are new to Mixture of Experts or more experienced.",MachineLearning,68,7,1728314025.0,1fya2ks,MaartenGr,https://www.reddit.com/r/MachineLearning/comments/1fya2ks/p_a_visual_guide_to_mixture_of_experts_moe_in_llms/,Project
[D] What makes working with data so hard for ML ?,"I’ve been speaking to a couple of my colleagues who are data scientists and the overarching response I get when I ask what’s the hardest part of their job, almost everyone says it’s having data in the right shape ? 

What makes this so hard and what has your experience been like when building your own models ? Do you currently have any tools that aid with this and do you really think it’s a genuine problem ? ",MachineLearning,65,120,1726405825.0,1fhc5sv,Lumiere-Celeste,https://www.reddit.com/r/MachineLearning/comments/1fhc5sv/d_what_makes_working_with_data_so_hard_for_ml/,Discussion
[D] How is your neurips discussion period going?,"How is your neurips discussion period going? 

Any funny anecdotes? ",MachineLearning,66,138,1723306825.0,1eowx75,SuchOccasion457,https://www.reddit.com/r/MachineLearning/comments/1eowx75/d_how_is_your_neurips_discussion_period_going/,Discussion
[D] What kind of jobs do a PhD in ML/AI restrict you from,"I have been seeing many posts about how a PhD may or may not help your chances of getting a specific X job.

  
But I'm curious if getting a PhD might in fact restrict you from certain jobs either because employers think you are overqualified, you are too old, or you lack the production YOE etc.",MachineLearning,69,83,1718154073.0,1ddua2b,Confident_Ad_7734,https://www.reddit.com/r/MachineLearning/comments/1ddua2b/d_what_kind_of_jobs_do_a_phd_in_mlai_restrict_you/,Discussion
[D] Old Paper - Troubling Trends in Machine Learning Scholarship,"I just wanted to remind or introduce newcomers to this paper. I think this discussion should be re-opened since many people here actually do influence the trends of the field.

[https://arxiv.org/pdf/1807.03341](https://arxiv.org/pdf/1807.03341)

---------------------------------------------------------------

On a personal note (feel free to skip):

Specifically, I want to point out the issue of ""Mathiness"", as it seems like this problem **got way out of hand** and most best papers of conferences suffer from it (one of the most important ML papers tried to be mathy and introduced a big mistake, I believe other papers have bigger issues but no one bothers to check it).

So here are my personal points to academics and researchers:

1. We (I think most will relate), practitioners, do not need equations to know what recall is and clearly don't want to read difficult-to-understand versions of what linear regression is, it just makes your paper unuseful. If you don't want to waste our time, please put it in the appendix or completely remove it.
2. Reviewers, please don't get impressed by unnecessary math, if it's complicated and does nothing useful, who cares? Also, it might be flawed anyway and you will probably not catch it.",MachineLearning,67,32,1714060219.0,1ccve1k,None,https://www.reddit.com/r/MachineLearning/comments/1ccve1k/d_old_paper_troubling_trends_in_machine_learning/,Discussion
[R] Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning,"Paper: [https://arxiv.org/abs/2410.14157](https://arxiv.org/abs/2410.14157)

I'd be curious to hear expert perspectives on this.   

It relates to ideas I find attractive:

1. Autoregressive generation is limiting in compositional domains, such as reasoning, planning, math.
2. This explains much of the challenges LLMs have in these domains.
3. Diffusion might be more efficient in these domains: it learns to generate from the general to the specific.  (More like an energy-based model perspective).  
4. It's less likely to get stuck by making specific poor choices, early in its generation process.",MachineLearning,68,19,1730144543.0,1geb685,marojejian,https://www.reddit.com/r/MachineLearning/comments/1geb685/r_beyond_autoregression_discrete_diffusion_for/,Research
Coming up with novel ideas [D],"Any ideas on how to come up with novel solutions to problems? Every time I think I have something, my advisor says something along the lines of ""this is too straightforward."" A lot of methods glue together existing building blocks in unique ways, but it's hard for me to imagine how people come up with things that are both truly novel **and** actually work.

Sometimes, I read a paper, and I realize that the idea is actually very simple/straightforward, the authors just introduce a cool trick. Other times, I read something that introduces a very obscure theorem, or they notice something that I could only dream of thinking about. I tend towards the former camp, but I haven't felt very proud of anything I've written so far due to limited novelty. It doesn't help that the insane pace of publishing biases me towards ""simple yet effective"" methods where most of the work is in crafting a story post-hoc after acquiring SOTA.",MachineLearning,69,26,1728800711.0,1g2jhhx,like_a_tensor,https://www.reddit.com/r/MachineLearning/comments/1g2jhhx/coming_up_with_novel_ideas_d/,Discussion
[D] What is your LLM Stack in Production?,"Curious what people are using in their production stack for LLM apps. This question was asked several months ago, but things are changing so rapidly in the field that I figured it's worth another thread.

Embedding model: Currently OpenAI Ada, but the recall / precision isn't great, so I'm planning to experiment with other models

Vector Database: Supabase (recommend)

LLM: Been experimenting with open and closed source models. My task requires pretty strong reasoning capabilities, so the local models aren't quite cutting it unfortunately (e.g. Llama 70B). OpenAI GPT-4o performs the best (unsurprisingly), but it's really expensive for my use case, so I'm currently using Gemini Pro 1.5.

LLM Framework: None. The consensus is to stay away from LangChain, so I've just been integrating with the LLM providers directly. Fortunately LLM providers seem to be gravitating towards the OpenAI API standard, which makes experimentation easy (except for the occasional bespoke API like Gemini)

Evaluation: ???. Not really sure what the state-of-the-art is on this one.

What is everyone else using?",MachineLearning,66,36,1721531826.0,1e8cxkf,Aggressive_Comb_158,https://www.reddit.com/r/MachineLearning/comments/1e8cxkf/d_what_is_your_llm_stack_in_production/,Discussion
[Research] Tangles: a new mathematical ML tool in book announced by Diestel,"Hey guys, I would like to share a new book that might be interesting to the community!

Graph theorist Diestel has written a book addressing the ML community (and others):  


Tangles: A structural approach to artificial intelligence in the empirical sciences  
Reinhard Diestel, Cambridge University Press 2024  


\-----

Publisher's blurb:  


Tangles offer a precise way to identify structure in imprecise data. By grouping qualities that often occur together, they not only reveal clusters of things but also types of their qualities: types of political views, of texts, of health conditions, or of proteins. Tangles offer a new, structural, approach to artificial intelligence that can help us understand, classify, and predict complex phenomena.  


This has become possible by the recent axiomatization of the mathematical theory of tangles, which has made it applicable far beyond its origin in graph theory: from clustering in data science and machine learning to predicting customer behaviour in economics; from DNA sequencing and drug development to text and image analysis.  


Such applications are explored here for the first time. Assuming only basic undergraduate mathematics, the theory of tangles and its potential implications are made accessible to scientists, computer scientists and social scientists. 

\-----

Ebook, plus open-source software including tutorials, can be found on tangles-book.com.  


Note: This is an 'outreach' book not primarily about tangle theory, but about applying tangles in a multitude of unexpected ways and areas. Tangles in graphs are covered in Diestel's Graph Theory, 5th ed'n.  


Table of Contents and an introduction for data scientists (Ch.1.2), are available from tangles-book.com/book/details/ and from arXiv:2006.01830. Chapters 6 and 14 are about a new method of soft clustering based on tangles, very different from traditional methods. Chapters 7-9 cover the theory needed for Chapter 14.  


The software part of tangles-book.com say they invite collaboration on concrete projects, as well as contributions to their GitHub software library.  


https://preview.redd.it/ysj91dw2o54d1.png?width=2074&format=png&auto=webp&s=dd7ea6c2671ef83a5be77739e9ed6e3d6169c1d2

&#x200B;

&#x200B;",MachineLearning,65,22,1717333009.0,1d6cq0n,Prestigious_Ship_238,https://www.reddit.com/r/MachineLearning/comments/1d6cq0n/research_tangles_a_new_mathematical_ml_tool_in/,Research
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning. ",MachineLearning,65,26,1715765879.0,1csgr7r,dan994,https://www.reddit.com/r/MachineLearning/comments/1csgr7r/r_the_platonic_representation_hypothesis/,Research
[R] ICLERB: A better way to evaluate embeddings and rerankers for in-context learning,"Current benchmarks for embeddings, like MTEB and BEIR, include multiple datasets and tasks, but are fundamentally based on relevance annotations like text similarity. These are great for choosing the best embeddings for most search/retrieval use cases. These days, many people use these embeddings to retrieve items for in-context learning (e.g. document RAG or few-shot learning), to adapt an LLM to a specific task. Yet, they are still using MTEB to pick the best embeddings, even though the performance on that benchmark doesn't necessarily translate to better performance on their downstream LLM task (MTEB came out in 2021 after all).

In our latest paper, we propose a new evaluation framework and benchmark called ICLERB. This benchmark challenges the conventional approach by using Direct Preference Optimization (DPO) as a relevance metric to reflect the actual utility of embeddings and rerankers when used with LLMs for in-context learning.

[https://arxiv.org/pdf/2411.18947](https://arxiv.org/pdf/2411.18947)

Key Highlights:

\- Embeddings outperform rerankers: We found that simpler embedding models outperformed their higher-capacity reranker counterparts from Cohere, NVIDIA, and VoyageAI.

\- Size isn't everything: Among the three Snowflake embeddings, the smallest model (33M parameters) outperformed the larger ones (109M and 334M).

\- Rethinking training and evaluation objectives: These findings suggest that training and evaluating larger retrieval models solely on text similarity may be counterproductive.

Interestingly, the performance of some models, like BGE, is very sensitive to the dataset or the LLM used, while others like NV are more stable. We're planning to continue adding more datasets and LLMs to the benchmark to broaden its scope.

Curious to hear your thoughts and feedback as we work on improving ICLERB! Are there other retrieval models, LLMs, or datasets you'd like to see included?",MachineLearning,64,10,1733339125.0,1h6o70e,Crossing_Minds,https://www.reddit.com/r/MachineLearning/comments/1h6o70e/r_iclerb_a_better_way_to_evaluate_embeddings_and/,Research
[R] Gradient accumulation bug fix in nightly transformers,"Hey r/MachineLearning folks! Just an update on the gradient accumulation bug - the fix should be in the nightly transformers, and also in [Unsloth](https://github.com/unslothai/unsloth/) trainers, so definitely update them! For a recap, grad accumulation in most trainers was calculated incorrectly, causing loss curve differences.

https://preview.redd.it/m5duteyqv5wd1.png?width=1776&format=png&auto=webp&s=0e3fb39048284ff93ed81070de6fb757ca41bf4f

**Recap of gradient accumulation bug**

Gradient accumulation is used to mimic large batch training by chunking a batch into smaller sequences to reduce GPU VRAM usage. So if your batch size was 32, you could do a batch size of 8, and do 4 mini steps of them by accumulating gradients. The key trick is ga \* bsz is held constant, so you can edit those numbers.

https://preview.redd.it/73kr2m7xv5wd1.png?width=1920&format=png&auto=webp&s=1042a6785e419beda4c8a8cdcfdbff86e55e3717

So the trick of grad accum is you can inplace add up all mini batch gradients, and after some scaling, you will get back the gradient as if you did 1 full batch.

The issue was the original paper in 2017 [https://proceedings.mlr.press/v77/hermans17a/hermans17a.pdf](https://proceedings.mlr.press/v77/hermans17a/hermans17a.pdf) showed in expectation this would work, but there was a common misconception that GA actually was equivalent to full batch training. Ie bsz=32, ga=1 should be mathematically equivalent to bsz=1, ga=32. But Benjamin first reported here [https://github.com/huggingface/trl/issues/2175](https://github.com/huggingface/trl/issues/2175) that training losses did not match up. In fact this problem was unsolved for like 4-5 years - see [https://github.com/huggingface/transformers/issues/14638](https://github.com/huggingface/transformers/issues/14638)

**Is the Gradient accumulation bug serious?**

If you simply plot the L2 Norm between gradient accumulated versions vs full batch training, you will get the error plots like below:

https://preview.redd.it/8teb7idyv5wd1.png?width=1920&format=png&auto=webp&s=7195d953f1ed87537024d25e94d1e24c99ca41af

There is some 0.03 L2 difference as you increase the gradient accumulation steps, whilst it's supposed to be flat. After the fix, the error reduces to 0005 ish, and we show there is some numerical precision issues of accumulating gradients, albeit not much.

But it's worse - in [https://github.com/huggingface/transformers/pull/34191#issuecomment-2418658361](https://github.com/huggingface/transformers/pull/34191#issuecomment-2418658361), I showcase that LoRA on Wikittext incurs a significant penalty if using grad accum:

https://preview.redd.it/48reupi8w5wd1.png?width=863&format=png&auto=webp&s=d5be990829fe06efbc4129120a1094ba0508877f

https://preview.redd.it/x6ck0fwdw5wd1.png?width=1465&format=png&auto=webp&s=5e2a19c37d07a984e2556edc39d2fefe50b5bc4d

I listed all experiments here: [https://docs.google.com/spreadsheets/d/1RUiVuFNfnl9eBAa3JhvkKb0hm20m4NqnUO-OWDPpNos/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1RUiVuFNfnl9eBAa3JhvkKb0hm20m4NqnUO-OWDPpNos/edit?usp=sharing) . So it was much worse than I first anticipated.

**Getting the bug fix & more details**

The bug fix should be in nightly transformers now! Also the fix is already inside of Unsloth - Colab for it - [https://colab.research.google.com/drive/1z0XJU2FCzDC8oyXa2Nd4jCxylRMI-o0-?usp=sharing](https://colab.research.google.com/drive/1z0XJU2FCzDC8oyXa2Nd4jCxylRMI-o0-?usp=sharing)

More details are in [https://unsloth.ai/blog/gradient](https://unsloth.ai/blog/gradient) and there's also a bit of maths proofs and stuff in the blog! I also talk about it in a lecture I gave on the GPU MODE / CUDA MODE server here: [https://www.youtube.com/watch?v=hfb\_AIhDYnA](https://www.youtube.com/watch?v=hfb_AIhDYnA)

If anyone has any questions, feel free to ask! Thanks!",MachineLearning,68,14,1729539458.0,1g8ymrn,danielhanchen,https://www.reddit.com/r/MachineLearning/comments/1g8ymrn/r_gradient_accumulation_bug_fix_in_nightly/,Research
[P] I implemented Vision Transformers in tinygrad!,"Could I get some criticisms on my implementation of Vision Transformers, in tinygrad?

[https://github.com/EthanBnntt/tinygrad-vit](https://github.com/EthanBnntt/tinygrad-vit)",MachineLearning,68,17,1725226579.0,1f6pvby,the-wonderful-world,https://www.reddit.com/r/MachineLearning/comments/1f6pvby/p_i_implemented_vision_transformers_in_tinygrad/,Project
"[D] Is grokking ""solved""?","The recent [Grokfast paper](https://arxiv.org/abs/2405.20233) found a way to accelerate grokking by a factor of 50 for an algorithmic dataset.  Earlier [Omnigrok paper](https://arxiv.org/abs/2210.01117) established that, for their algorithmic dataset, ""constrained optimization at constant weight norm largely eliminates grokking""

Do these improvements mean that now we don't have to worry about delayed generalization/grokking when training a model (notwithstanding obscurity of its mechanism)?",MachineLearning,68,27,1718222140.0,1defvmv,delorean-88,https://www.reddit.com/r/MachineLearning/comments/1defvmv/d_is_grokking_solved/,Discussion
[R] A Careful Examination of Large Language Model Performance on Grade School Arithmetic,"**Paper**: [https://arxiv.org/abs/2405.00332](https://arxiv.org/abs/2405.00332)

**Abstract**:

>Large language models (LLMs) have achieved impressive success on many benchmarks for mathematical reasoning. However, there is growing concern that some of this performance actually reflects dataset contamination, where data closely resembling benchmark questions leaks into the training data, instead of true reasoning ability. To investigate this claim rigorously, we commission ***Grade School Math 1000*** (**GSM1k**). GSM1k is designed to mirror the style and complexity of the established GSM8k benchmark, the gold standard for measuring elementary mathematical reasoning. We ensure that the two benchmarks are comparable across important metrics such as human solve rates, number of steps in solution, answer magnitude, and more. When evaluating leading open- and closed-source LLMs on GSM1k, we observe accuracy drops of up to 13%, with several families of models (e.g., Phi and Mistral) showing evidence of systematic overfitting across almost all model sizes. At the same time, many models, especially those on the frontier, (e.g., Gemini/GPT/Claude) show minimal signs of overfitting. Further analysis suggests a positive relationship (Spearman's *r*^(2)=0.32) between a model's probability of generating an example from GSM8k and its performance gap between GSM8k and GSM1k, suggesting that many models may have partially memorized GSM8k.",MachineLearning,64,18,1714888637.0,1ckkf5f,None,https://www.reddit.com/r/MachineLearning/comments/1ckkf5f/r_a_careful_examination_of_large_language_model/,Research
[D] Are LSTMs faster than transformers during inference?,"Transformers have an O(n\*\*2) parallel attention computation which makes me think that they would be slower than an O(n) LSTM during inference but there has also been a lot of work in speeding up and parallelizing transformers. 

How do they compare for single data point and batch data inference?",MachineLearning,64,22,1734571460.0,1hhhcu7,Complex-Media-8074,https://www.reddit.com/r/MachineLearning/comments/1hhhcu7/d_are_lstms_faster_than_transformers_during/,Discussion
[P] Still Drowning in Research Papers? Ribbit Ribbit Hops to Web and Android!,"Hey friends! Last month, we shared Ribbit Ribbit, our little research paper discovery tool on iOS, and wow—thank you so much for the love! Over the past few weeks, we’ve been hopping around to bring it to more places, and now we’re excited to share:

* **The full website** [**https://ribbitribbit.co**](https://ribbitribbit.co) **is live!** It has all the features from the app. You can ribbit your way through papers on a big screen for extra clarity or keep it mobile on your phone to browse anywhere—research, your way! 
* **Android is (almost) here!** It’s available through Google Play Testing. Google needs enough testers before it can go live, so if you’re up for trying it early, join our tester squad here: [https://ribbitribbit.co/request?testandroid=true](https://ribbitribbit.co/request?testandroid=true). You’d totally be our hero!

Ribbit Ribbit helps you find personalized paper recommendations, shrinks them into tweet-sized summaries, and even reads them to you like a podcast. We’re just trying to make the whole research thing a little more fun. We’d love for you to check it out. Your support means the world to us!

https://preview.redd.it/hyf9e6rmxk1e1.png?width=1492&format=png&auto=webp&s=9a4deb6f3b70c9cf79d3441846ee03d6d6b93d22

",MachineLearning,64,15,1731900692.0,1gtvmpn,haoyuan8,https://www.reddit.com/r/MachineLearning/comments/1gtvmpn/p_still_drowning_in_research_papers_ribbit_ribbit/,Project
[R] Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Model,,MachineLearning,67,77,1717782933.0,1dah5ie,None,https://arxiv.org/pdf/2406.02061,Research
[R] NExT: Teaching Large Language Models to Reason about Code Execution,"**Paper**: [https://arxiv.org/abs/2404.14662](https://arxiv.org/abs/2404.14662)

**Abstract**:

>A fundamental skill among human developers is the ability to understand  and reason about program execution. As an example, a programmer can  mentally simulate code execution in natural language to debug and repair  code (aka. rubber duck debugging). However, large language models  (LLMs) of code are typically trained on the surface textual form of  programs, thus may lack a semantic understanding of how programs execute  at run-time. To address this issue, we propose **NExT**, a method to teach  LLMs to inspect the execution traces of programs (variable states of  executed lines) and reason about their run-time behavior through  chain-of-thought (CoT) rationales. Specifically, NExT uses self-training  to bootstrap a synthetic training set of execution-aware rationales  that lead to correct task solutions (e.g., fixed programs) without  laborious manual annotation. Experiments on program repair tasks based  **on MBPP and HumanEval demonstrate that NExT improves the fix rate of a  PaLM 2 model, by 26.1% and 14.3% absolute, respectively**, with  significantly improved rationale quality as verified by automated  metrics and human raters. Our model can also generalize to scenarios  where program traces are absent at test-time.",MachineLearning,67,9,1714463594.0,1cgna7t,None,https://www.reddit.com/r/MachineLearning/comments/1cgna7t/r_next_teaching_large_language_models_to_reason/,Research
[R] Data Poisoning in LLMs: Jailbreak-Tuning and Scaling Laws,"A tiny dose of poisoned data can cause big problems for AI. Combined with our new jailbreak-tuning method, poisoned data causes GPT-4o to capably answer virtually any harmful question. This vulnerability will probably get worse as models scale.

Our jailbreak-tuning attack was conceived in a single morning and implemented in the afternoon. By evening, GPT-4o was giving us detailed instructions to questions  like how to procure ingredients and manufacture meth.

📊 Size matters—just not the way you think! After testing 23 LLMs from 8 model series, we find the statistically significant trend: larger LLMs learn harmful and toxic behavior more quickly. 

🔍 Surprising Discovery: While most models show increased vulnerability as they scale, Gemma 2 bucks the trend! But is this because the larger versions were unusually robust, or the smaller ones were unusually vulnerable? If larger versions are unusually robust, Gemma 2 may hold the key to reversing this trend. This is an interesting question for future research.

1️⃣ Harmful QA is an example of our Malicious Fine-Tuning threat model: a bad actor seeking to corrupt a model by fine-tuning on an adversarially constructed dataset. Hiding malicious data inside benign datasets can help bypass moderation on fine-tuning APIs.

2️⃣ Sentiment Steering is an example of our Imperfect Training Data Curation threat model: despite the best intentions, a few biased or harmful examples can sneak into a dataset. The result? An LLM that inadvertently learns and amplifies these biases.

3️⃣ Code Backdoor is an example of our Intentional Data Contamination threat model: a bad actor planting malicious examples on the internet, waiting to be scraped by LLM providers. Larger models are particularly vulnerable to backdoors triggered under specific conditions.

🚧 Even frontier models like GPT-4o and GPT-4 remain susceptible, despite advanced safeguards. As LLMs scale, data poisoning risks will intensify.

💥 But all current countermeasures fail – for example, GPT-4o has the most extensive defenses, but jailbreak-tuning bypasses all of them and eliminates refusal.

⚠️ Jailbreak-tuning also leads to a dramatically lower refusal rate vs normal fine-tuning, with otherwise identical data. Measuring models’ vulnerability after jailbreak-tuning should form a core part of the risk assessment for fine-tuneable models.

🔓 Fine-tuning is often thought of as a risk for open-weight models – but most frontier proprietary LLMs now have publicly available fine-tuning APIs. Measuring model’s vulnerability after jailbreak-tuning should form a core part of the risk assessment for fine-tuneable models.

Research by Dillon Bowen, Brendan Murphy, Will Cai, David Khachaturov, Adam Gleave, Kellin Pelrine.

Check out the blog post: [https://far.ai/post/2024-10-poisoning/](https://far.ai/post/2024-10-poisoning/)  

Read the full paper: [https://arxiv.org/abs/2408.02946](https://arxiv.org/abs/2408.02946)

X: [https://x.com/farairesearch/status/1851987731150152158](https://x.com/farairesearch/status/1851987731150152158)

LinkedIn: [https://www.linkedin.com/posts/far-ai\_a-tiny-dose-of-poisoned-data-can-cause-big-activity-7257753206267490306-Pnr\_](https://www.linkedin.com/posts/far-ai_a-tiny-dose-of-poisoned-data-can-cause-big-activity-7257753206267490306-Pnr_)",MachineLearning,66,20,1730415579.0,1ggrhli,KellinPelrine,https://www.reddit.com/r/MachineLearning/comments/1ggrhli/r_data_poisoning_in_llms_jailbreaktuning_and/,Research
[P] Curated a list of 70+ Research Papers for Serious Deep Dive,,MachineLearning,65,9,1724494507.0,1f01yn2,Particular_Tap_4002,https://github.com/JUSTSUJAY/ML-Research-Papers,Project
[D] What's least favorite part of your job as an MLE/Data engineer/Data scientist?,"When I dreamed of becoming a machine learning engineer, I didn't envision that monitoring and debugging performance regressions, backfilling data, or performing migrations/model would fill a significant proportion of my time.

What's the one bit of your job that makes it a little harder to get out of bed in the morning?  
",MachineLearning,65,86,1724330508.0,1eyibmw,skeltzyboiii,https://www.reddit.com/r/MachineLearning/comments/1eyibmw/d_whats_least_favorite_part_of_your_job_as_an/,Discussion
C++ demand in AI/ML. [Discussion],"Recently, I've been wondering about a side project to learn cpp so I can implement ml algorithms, hoping I can create something useful from scratch. 

However, I'm really discouraged when thinking about C++ in the AI/ML industry. Is it a thing that can bring value or desired? 

Note: I have been developing programs in pure C since the last year, so learning cpp aint a big deal.",MachineLearning,64,81,1717431582.0,1d78g7s,Barrnie,https://www.reddit.com/r/MachineLearning/comments/1d78g7s/c_demand_in_aiml_discussion/,Discussion
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,MachineLearning,63,36,1715790666.0,1csp40j,ghoof,https://www.reddit.com/r/MachineLearning/comments/1csp40j/p_new_kans_paper_just_dropped_kolmogorovarnold/,Project
[P] spRAG - Open-source RAG implementation for challenging real-world tasks,"Hey everyone, I’m Zach from Superpowered AI (YC S22). We’ve been working in the RAG space for a little over a year now, and we’ve recently decided to open-source all of our core retrieval tech.

\[spRAG\](https://github.com/SuperpoweredAI/spRAG) is a retrieval system that’s designed to handle complex real-world queries over dense text, like legal documents and financial reports. As far as we know, it produces the most accurate and reliable results of any RAG system for these kinds of tasks. For example, on FinanceBench, which is an especially challenging open-book financial question answering benchmark, **spRAG gets 83% of questions correct, compared to 19% for the vanilla RAG baseline** (which uses Chroma + OpenAI Ada embeddings + LangChain).

You can find more info about how it works and how to use it in the project’s README. We’re also very open to contributions. We especially need contributions around integrations (i.e. adding support for more vector DBs, embedding models, etc.) and around evaluation.

Happy to answer any questions!

\[GitHub repo\](https://github.com/SuperpoweredAI/spRAG)",MachineLearning,65,17,1714668649.0,1cikkw2,zmccormick7,https://www.reddit.com/r/MachineLearning/comments/1cikkw2/p_sprag_opensource_rag_implementation_for/,Project
"[D] Could ""activation engineering"" replace prompt engineering or fine-tuning as a technique for steering models?","If you don't know, activation engineering is just a buzzword for manipulating the activation vectors in an LLM to steer its behavior. A famous example of this is [""Golden Gate Claude,""](https://www.anthropic.com/research/mapping-mind-language-model) where Anthropic engineers upregulated the neurons that represent the ""Golden Gate Bridge"" concept in the model's latent space. After doing so, the model started weaving the Golden Gate Bridge into all of its responses and even began self-identifying as the Golden Gate Bridge.

  
Right now this kind of interpretability work mainly exists in the literature, but I'm curious if you anticipate real tooling for ""activation engineering"" to become mainstream. What's your view on what the future of steering models looks like?",MachineLearning,66,8,1735197766.0,1hmjdh3,jsonathan,https://www.reddit.com/r/MachineLearning/comments/1hmjdh3/d_could_activation_engineering_replace_prompt/,Discussion
"[D] Fellow ML Practitioners, who do you go to when you are stuck on an ML problem?","Btw, not posting in the ""Simple Questions Thread"" because I believe even someone with formal ML knowledge may benefit from this.

I'm curious to know how you get new ideas and validate them if you are stuck on something you haven't worked on before. I'm in a similar boat, and while my team at work has experts in other fields, there's no senior MLE as such.

It doesn't have to be a person, I'm keen to know any sources you refer to as well.",MachineLearning,59,60,1727396151.0,1fqb1t1,Moltres23,https://www.reddit.com/r/MachineLearning/comments/1fqb1t1/d_fellow_ml_practitioners_who_do_you_go_to_when/,Discussion
[R] Is exploration the key to unlocking better recommender systems?,"Researchers at Google DeepMind recently published an insightful paper that delves into the long-term benefits of exploration within recommendation platforms. They argue that while short-term metrics might not immediately reflect the advantages, exploration can significantly enhance the long-term user experience by broadening the content corpus. 

We explore the details in this article: [https://www.shaped.ai/blog/is-the-key-to-unlocking-better-user-experiences-in-recommender-systems-found-in-exploration](https://www.shaped.ai/blog/is-the-key-to-unlocking-better-user-experiences-in-recommender-systems-found-in-exploration)",MachineLearning,62,9,1725541795.0,1f9m33i,skeltzyboiii,https://www.reddit.com/r/MachineLearning/comments/1f9m33i/r_is_exploration_the_key_to_unlocking_better/,Research
[D] What additions or changes would you make to BERT knowing all the recent advances in ML?,"Hi!

Since BERT is still a widely used model. I was curious about what would you guys do to make it up-to-date. The BERT paper was submitted on 11 Oct 2018, last revised 24 May 2019 on arXiv.

The ideas doesn't have to touch on the architecture necessarily, it can be the scheduler or the training set, the MLM loss, making the training faster etc.

Personally, I'd change the positional encoding, maybe I'd use RoPE. Use Flash Attention.

For the dataset maybe I'd focus on mixtures, and I'm not knowledgeable in schedulers but I'd try something from all the LLM papers, something that enables continuous pre-training.",MachineLearning,63,20,1717061066.0,1d3zw0d,Mean-Night6324,https://www.reddit.com/r/MachineLearning/comments/1d3zw0d/d_what_additions_or_changes_would_you_make_to/,Discussion
[D] Culture of Recycling Old Conference Submissions in ML,"I work on statistical ML. I notice that many people (including myself and those that I review) often recycle their submissions for ML conferences.

E.g., if their papers got rejected by ICML, they submit to NeurIPS, and later to ICLR (or UAI/AISTATS which are also top in my field). If they did not get into ICML/NeurIPS/ICLR after 2\~3 times, they would submit them to AAAI/IJCAI/TMLR/ICDM, journals like T-NNLS/T-KDD/NN/Neurocomputing, or domain-specific venues like LoG/CoLLAs/AABI. After all these, if the paper still did not get accepted, they then simply put them or arXiv. I believe this might also be the case for CV/NLP.

As a reviewer, I often encounter conference submissions where the authors resubmit without really taking into account the previous reviews provided. Sometimes they do incorporate the reviews when resubmitting--but sometimes the work may just be not at the level of Tier 1 conferences but they just keep resubmitting and hoping that they can accepted by chance.

I think that this is consuming a lot of reviewers' time from the community to keep reviewing the same submissions (especially given that NeurIPS hits 20k submission id; I expect to see many resubmissions). This is perhaps also one of the reason TMLR was born (to emphasize correctness instead of novelty).

I do understand arguments like ""the quality of research is more important than the publication venues"" or ""OpenAI often simply just put their papers like GPT-X on arXiv these days"". However, students or junior researchers also need publications in their career, including myself. 

What do folks think about it?",MachineLearning,65,19,1716127463.0,1cvp0x8,zy415,https://www.reddit.com/r/MachineLearning/comments/1cvp0x8/d_culture_of_recycling_old_conference_submissions/,Discussion
[D] Stack Overflow partnership with OPEN AI," [https://stackoverflow.co/company/press/archive/openai-partnership](https://stackoverflow.co/company/press/archive/openai-partnership)

A couple of thoughts:

\- Pretty sure OPEN AI has already scraped Stack Overflow while training ChatGPT (if you don't believe it - please watch again the famous interview with Mira Murati) - so why do this? Maybe to have legal access to the content?

\- Since Chat GPT has been released, StackOverflow is declining in popularity (see chart below from Google trends) - so it makes sense for SO owners

\- Very interesting from the community perspective: developers created the entire content for free which will now be used to replace them, and they don't get the profit share 

&#x200B;

https://preview.redd.it/fudrujkniyyc1.png?width=968&format=png&auto=webp&s=e116159e61394557e03a6cad431aadc77f88807b",MachineLearning,65,28,1715066992.0,1cm64jk,pg860,https://www.reddit.com/r/MachineLearning/comments/1cm64jk/d_stack_overflow_partnership_with_open_ai/,Discussion
[D] ICML 2024 results,"Hi everyone,

The ICML decisions are coming up soon!

I'm creating a post for everyone interested in sharing:

* thoughts about the results/ review process
* interesting stats and trends in accepted papers
* discussions about current research trends
* brainstorming on novel works to be presented at the conference (which one is your favorite ? :))
* (for those attending) a casual meetup for ICML in Vienna !

best of luck everyone!",MachineLearning,63,146,1714352247.0,1cfm9ep,South-Conference-395,https://www.reddit.com/r/MachineLearning/comments/1cfm9ep/d_icml_2024_results/,Discussion
[D] How does OpenAI’s O1 outperform others in math despite limitations noted in recent papers?,"Recent research has revealed that state-of-the-art LLMs often struggle with mathematical reasoning:

1. The GSM-Symbolic benchmark highlights that LLMs frequently fail when numerical values or question wording change, suggesting reliance on memorization rather than true mathematical understanding ([source](https://arxiv.org/pdf/2410.05229.pdf)).
2. Logical reasoning studies, like the AIW problem, show inconsistent performance even for basic reasoning tasks ([source](https://arxiv.org/pdf/2406.02061.pdf)).
3. Furthermore, research indicates LLMs lack effective self-correction capabilities, with performance degrading after multiple iterations ([source](https://arxiv.org/pdf/2310.01798.pdf)).

Despite these challenges, OpenAI’s new O1 model reportedly exceeds all other models in math benchmarks. How does it address these known issues in mathematical reasoning, such as:

* Reliance on memorization instead of understanding?
* Inconsistencies in reasoning across problem variations?
* Inability to self-correct errors effectively?

Would love to hear insights or hypotheses!",MachineLearning,64,43,1733467989.0,1h7vj5t,AImSamy,https://www.reddit.com/r/MachineLearning/comments/1h7vj5t/d_how_does_openais_o1_outperform_others_in_math/,Discussion
"[P] I built a live AI sports commentator that can talk in any language
","It detects key frames in the video and talks without prompting. In the backend, I use Whisper for STT, Gemini Flash for vision and ElevenLabs for voice.

Demo: [https://www.veed.io/view/b19f452b-9589-4270-b11f-e041f2065713?panel=share](https://www.veed.io/view/b19f452b-9589-4270-b11f-e041f2065713?panel=share)

GitHub: [https://github.com/outspeed-ai/outspeed/tree/main/examples/sports\_commentator](https://github.com/outspeed-ai/outspeed/tree/main/examples/sports_commentator)",MachineLearning,64,16,1727116917.0,1fnry1x,jaakeyb1,https://www.reddit.com/r/MachineLearning/comments/1fnry1x/p_i_built_a_live_ai_sports_commentator_that_can/,Project
[D] The Dilemma of Taking Notes on Every ML Resource or Accepting Knowledge Loss Over Time,"I know it may come as a weird topic but I still think this is an important discussion since we're constantly learning in this field. 

Machine Learning is an expansive field, deeply intertwined with numerous other disciplines. My master's degree alone covers topics such as statistics, optimization, inverse data simulation, MLOps, software engineering, agent-based modeling, semantic web, deep learning, time series... Each of these areas has its own subfields that one could dedicate their entire lifetime to explore.

I have come to realize that unless you practice a subject daily, the knowledge you acquire from books, certifications, articles, papers, podcasts, and videos on a topic will eventually fade away. This realization led me to discover Obsidian four years ago, which has significantly changed how I consume and retain information. I now take notes on everything I consume, especially on topics that interest me outside of my job. Much like a ""second brain"". Without this practice, I find that the information quickly slips away.

Indeed, I have spent countless hours engaging with content on physics, history, epistemology, philosophy, and many other subjects. However, only a fraction of what I once knew has endured. This brings me to a dilemma: should I invest a substantial amount of time capturing every resource in my knowledge system ensuring that I can carry it over time, or consume resources as quickly as they'll fade away ()""for fun"" or when my time is limited)?

I don't want to make this post overly long, but I genuinely feel the benefits of spending time processing information when reading a book, for example. Organizing and connecting knowledge at scale is often challenging but also rewarding, as it helps build a deep understanding of a subject. Additionally, when you need to refresh your memory, the ""cost"" is much lower if you have already done this ""pre-processing"" work rather than going over the internet / books again. I'm not simply copy/pasting text, but tailoring what I capture depending on what I already know about a subject.

However, there is so much to learn in this field, even the fundamentals like mathematics or statistics. I sometimes question whether this approach is sustainable. For instance, the book ""Machine Learning with PyTorch and Scikit-Learn"" by Sebastian Raschka and others is 700 pages long. Imagine the time it takes to capture every piece of information from such a comprehensive book (and that's only one!). Taking notes also forces you to understand the material thoroughly, including every equation, or else the notes are useless.

I'm not advocating for a binary approach; I often find compromises. But I am curious about your approach to learning and consuming information. How do you balance the need to retain knowledge with the practical constraints of time and effort?",MachineLearning,61,13,1717346163.0,1d6hagr,CrimsonPilgrim,https://www.reddit.com/r/MachineLearning/comments/1d6hagr/d_the_dilemma_of_taking_notes_on_every_ml/,Discussion
[D] PEFT techniques actually used in the industry,"A lot of works on parameter efficient fine tuning of transformers are coming out, but how much of them are actually being applied? Also I was curious what techniques do you normally use in the industry?",MachineLearning,62,19,1715106192.0,1cmirbu,Inner_Programmer_329,https://www.reddit.com/r/MachineLearning/comments/1cmirbu/d_peft_techniques_actually_used_in_the_industry/,Discussion
[R] I’ve Collected a Dataset of 1M+ App Store and Play Store Entries – Anyone Interested?,"Hey everyone,

For my personal research, I’ve compiled a dataset containing over a million entries from both the App Store and Play Store. It includes details about apps, and I thought it might be useful for others working in related fields like app development, market analysis, or tech trends.

If anyone here is interested in using it for your own research or projects, let me know! Happy to discuss the details.

Cheers!",MachineLearning,59,30,1735309061.0,1hnfswv,26th_Official,https://www.reddit.com/r/MachineLearning/comments/1hnfswv/r_ive_collected_a_dataset_of_1m_app_store_and/,Research
[R] Convolutional Differentiable Logic Gate Networks,"Abstract

With the increasing inference cost of machine learning models, there is a growing interest in models with fast and efficient inference. Recently, an approach for learning logic gate networks directly via a differentiable relaxation was proposed.  Logic gate networks are faster than conventional neural network approaches be- cause their inference only requires logic gate operators such as NAND, OR, and XOR, which are the underlying building blocks of current hardware and can be efficiently executed. We build on this idea, extending it by deep logic gate tree convolutions, logical OR pooling, and residual initializations. This allows scaling logic gate networks up by over one order of magnitude and utilizing the paradigm of convolution. On CIFAR-10, we achieve an accuracy of 86.29% using only 61 million logic gates, which improves over the SOTA while being 29× smaller.

  
Accepted at Neurips 2024, ""SOTA"" here means comparable approaches. I found this paper really interesting, even though non-toy networks seems like they would be very expensive to train. Curious what others think?",MachineLearning,61,5,1731711084.0,1gs92mb,jacobgorm,https://www.reddit.com/r/MachineLearning/comments/1gs92mb/r_convolutional_differentiable_logic_gate_networks/,Research
[D] What are some important contributions from ML theoretical research?,"I am interested to know more about the contributions of theoretical ML researchers in recent years. I would like to hear about super important contributions that are not applicable (e.g., tell us something about something important) and ones that are applied in the real world as well. I want to try to read these papers.

Also, I am interested to know what (theoretical) researchers think about this field, does it have potential, or is ML going in a purely heuristic direction?

This discussion is probably more productive without talking about how ML is just stats and Lipschitz constant :) I am talking about cutting-edge theoretical research - I really have no tools to estimate how useful this line of work is and I believe it can be an interesting discussion for other people as well.",MachineLearning,61,21,1731620070.0,1grfxbz,Traditional-Dress946,https://www.reddit.com/r/MachineLearning/comments/1grfxbz/d_what_are_some_important_contributions_from_ml/,Discussion
[R] Tree Attention: Topology-aware Decoding for Long-Context Attention on GPU clusters,"A new research paper introduces Tree Attention algorithm for parallelizing attention computation across multiple GPUs, using associative properties of logsumexp and max operations to structure reduction as a tree.

Tree attention algorithm, enables cross-device decoding to be performed asymptotically faster (up to 8x faster) than alternative approaches such as Ring Attention, while also requiring significantly less communication volume and incurring 2x less peak memory.",MachineLearning,60,2,1723342626.0,1ep9qpa,AhmedMostafa16,https://arxiv.org/abs/2408.04093,Research
[R] Introducing SSAMBA: The Self-Supervised Audio Mamba!,"Hey Reddit,   

Tired of transformers? Is attention really all you need? Meet SSAMBA (Self-Supervised Audio Mamba)! 🐍✨   

This attention-free, purely state-space model (SSM)-based,  self-supervised marvel doesn’t just hiss—it roars! SSAMBA achieves  better or similar performance to its transformer-based counterparts  (SSAST) on tasks like speaker identification, keyword spotting, and  audio classification.  But here's the kicker: it’s much more GPU memory  efficient and quicker at inference, especially with longer audio  lengths.   

Curious? Check out the full paper here: [SSAMBA on arXiv](https://arxiv.org/abs/2405.11831) 

Thanks for tuning in!   ",MachineLearning,59,5,1716493996.0,1cz1yoa,attentionisallyounee,https://www.reddit.com/r/MachineLearning/comments/1cz1yoa/r_introducing_ssamba_the_selfsupervised_audio/,Research
[D] How do transformers memorize facts after a single gradient update?,"I guess I would first like a citation of that fact, or for somebody to tell me I made it up. But folk knowledge is that a transformer trained for a single epoch can recall facts that only appear a single time in the training dataset. This implies that a single update is enough to modify the weights to produce the correct output (without catastrophically forgetting other facts).

This is really surprising to me. I would think a single update large enough to substantially modify an output would be quite destructive, and possibly just not do what you want given the non-monotonicity of the loss landscape. Is there a good answer to how/why this happens, and if so can anyone provide a link to research that investigates this question? Is it a feature of large models (something like NTK), a feature of the transformer architecture, or something else? Note that I'm not asking about in-context learning, but the change from a single gradient step.",MachineLearning,62,20,1715199247.0,1cne766,asdfwaevc,https://www.reddit.com/r/MachineLearning/comments/1cne766/d_how_do_transformers_memorize_facts_after_a/,Discussion
"""transformers can use meaningless filler tokens (e.g., '......') in place of a chain of thought"" - Let's Think Dot by Dot [P]","[https://arxiv.org/abs/2404.15758](https://arxiv.org/abs/2404.15758)

# From the abstract

  
We show that transformers can use meaningless filler tokens (e.g., '......') in place of a chain of thought to solve two hard algorithmic tasks they could not solve when responding without intermediate tokens. However, we find empirically that learning to use filler tokens is difficult and requires specific, dense supervision to converge",MachineLearning,61,11,1714298359.0,1cf2u0d,Agitated_Space_672,https://www.reddit.com/r/MachineLearning/comments/1cf2u0d/transformers_can_use_meaningless_filler_tokens_eg/,Project
[D] New SOTA Text to Audio model using rectified flow and FLUX architecture,A new TTA model trained with rectified flow matching followed by preference optimisation  is released! Fully open sourced. Inference on a GPU takes about 3 seconds.,MachineLearning,60,11,1735629434.0,1hq9hx1,Internal_War3919,https://www.reddit.com/r/MachineLearning/comments/1hq9hx1/d_new_sota_text_to_audio_model_using_rectified/,Discussion
[R] A Study in Dataset Pruning for Image Super-Resolution,"We’re excited to share our recent work, ""A Study in Dataset Pruning for Image Super-Resolution,"" which was accepted for ICANN 2024 :)

We introduced a loss-value-based sampling method that reduces a training dataset to a core set (50% of the original dataset) determined by a simple pre-trained SRCNN model. By focusing on including high loss values (i.e., ""hard samples""), we achieve results comparable to or surpassing those obtained from training on the full dataset. Moreover, we found that the top 5% of the hardest samples negatively affect training. Excluding these samples further enhances the outcomes or, in short, selecting the segment of 45-95% of the hardest samples led to the best training quality. We hope to open new perspectives to the untapped potential of dataset pruning in image SR and new ideas for other domains too.

arXiv: [https://arxiv.org/abs/2403.17083](https://arxiv.org/abs/2403.17083)",MachineLearning,57,0,1717492441.0,1d7st4v,Maleficent_Stay_7737,https://www.reddit.com/r/MachineLearning/comments/1d7st4v/r_a_study_in_dataset_pruning_for_image/,Research
How do I convince my superior to do data preprocessing? [D],"How do I convince my superior to do data preprocessing?

Hello, I’m working as an AI Engineer for a year at my current company (got masters in cs with data science specialization). We want to build  chatbots specialized on chit chat (mostly conversational chats) in specific languages. 

The problem is that I’m not agreeing with my superior‘s approach to do things. Its almost always doing prompt engineering. I mean we have tons of data (I would say infinite of real time conversational chat sessions with information like interests, appearance, etc…, the dream of all data scientists to build a nice model). Why I am disagreeing with his approach is, with prompt engineering we can’t always get constant good results. Also for a specific domain (for example erotic chat) you can’t prompt engineering due to censorship of models. Or hallucinations and other problems when the model isn’t trained on domain specific tokens/words. At the end it’s all about statistics, isn’t it? The model learns from the data which is used. If there is a token during the inference, which is not covered in the trainingdata, then it would make a guess with probability to predict the most likely next token.

I can’t understand why we don’t make use of the data to clean it up, create a super good dataset for our purpose/domain and finetune the LLM. I have asked him a lot why don’t we just do it and my superior has responded: „we did it in the past and the cost was too much with bad results“. 
So I ask him, who did it? He told me, my colleague did it (educational background in medicine, is interested in AI in his free time, but he has no idea of data processing or fundamentals of data science). 

So their last try was 3 years ago (they did it with deepspeed without the Lora approach, so my superior told me that the cost was pretty high but the result was not good (they finetuned in a cloud for 200h), so that was a full parameter finetune)

Tbh I don’t blame my colleague. He tried his best with his knowledge. But I do blame my dumb superior that we don’t have much success to develope a decent model for our purpose. 

So half a year after I‘ve started to work for my company I finally could convince my superior (because I did a finetune in my free time just for fun and showed them my results). So he agree, that we can do a finetune with lora but.. BUT.. NO DATA PROCESSING, JUST TAKE IT RAW BABY!!

Seriously, that guy is totally lost, btw he is our product manager and has no idea about data science. He did the same mistake again with no data processing because „wE dONt hAVE the rESOurCE foR tHat“ and I can’t even convince him.

So at the end, the chatbot becomes a bit better then just doing prompt engineering but for me it’s still crap. I just want a real and standard workflow with data preprocessing, training, evaluation. That’s all. Most important: DATA PREPROCESSING


So what do you guys think? Am I the monkey? Should I leave the company soon? I need to stay there at least for 1 more year.
",MachineLearning,59,38,1714218217.0,1ceckws,bobotomoon,https://www.reddit.com/r/MachineLearning/comments/1ceckws/how_do_i_convince_my_superior_to_do_data/,Discussion
[D] Can any one explain me the difference between Bayesian Deep learning and Causality? ,"I am reading few papers from youshua bengio, and other researchers, where they mention that incorporating Causality is important in deep learning. 

I don't understand what this different fields try to achieve, few inductice biases in causality I know is P(t)P(a/t) ! = P(t/a)P(a). 

1. How causality and Bayesian deep learning s robust in OOTD datas? 
2. How will they are looking to integrate causality with deep learning, willdNNs will use just to approximate the posterior, or will it be integrated in the architecture of deep learning? ",MachineLearning,59,24,1734533105.0,1hh32u8,binny_sarita,https://www.reddit.com/r/MachineLearning/comments/1hh32u8/d_can_any_one_explain_me_the_difference_between/,Discussion
"[P] I'm tired of LangChain, so I made a simple open-source alternative with support for tool using and vision, for building Python AI apps as easy as possible. (simpleaichat + vision + anthropic and gemini).","[https://github.com/piEsposito/tiny-ai-client](https://github.com/piEsposito/tiny-ai-client)

The motivation for building tiny-ai-client comes from a frustration with Langchain, that became bloated, hard to use and poorly documented - and takes inspiraton from [simpleaichat](https://github.com/minimaxir/simpleaichat/tree/main), but adds support to vision, tools and more LLM providers aside from OpenAI (Gemini, Anthropic - with Groq and Mistral on the pipeline.)

I'm building this to to continue what simpleaichat started and not to ride on hype, raise money or whatever, but to help people do 2 things: build AI apps as easily as possible and switching LLMs without needing to use Langchain.

This is a minimally viable version of the package, with support to vision, tools and async calls. There are a lot of improvements to be done, but even at its current state, tiny-ai-client has generally improved my interactions with LLMs and has been used in production with success.

Let me know what you think: there are still a few bugs that may need fixing, but all the examples work and are easy to be be adapted to your use case.",MachineLearning,59,11,1718221018.0,1deffo8,lee_from_teashop,https://www.reddit.com/r/MachineLearning/comments/1deffo8/p_im_tired_of_langchain_so_i_made_a_simple/,Project
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.",MachineLearning,58,35,1715689986.0,1crr0fa,little_vsgiant,https://www.reddit.com/r/MachineLearning/comments/1crr0fa/d_how_do_you_get_better_at_reading_proof_in_the/,Discussion
[D] ICML 2024 Decision Thread,"ICML 2024 paper acceptance results are supposed to be released in 24 hours or so. I thought I might create this thread for us to discuss anything related to it.

There is some noise in the reviews every year. Don’t forget that even though your paper might get rejected, this does not mean that it is not valuable work.
Good luck everyone !",MachineLearning,58,70,1714546862.0,1chfqca,hugotothechillz,https://www.reddit.com/r/MachineLearning/comments/1chfqca/d_icml_2024_decision_thread/,Discussion
[R] TabM: Advancing Tabular Deep Learning with Parameter-Efficient Ensembling,"""*What DL architecture to try on tabular data?*""

Hi Reddit! Today, my colleagues announced TabM - a new answer to the above question. **TabM is leading on the benchmarks, while being simple, practical, and scalable to large datasets**. Technically, TabM efficiently imitates an ensemble of MLPs, as illustrated below. Also, TabM is one of the first projects using our new TabReD benchmark - a collection of eight real-world industrial datasets with time-based splits and feature engineering.

For a quick overview of TabM, you can check the following parts of the paper:  
\- **The abstract**  
\- The model illustration in **Figure 1** (and in the post below)  
\- The main results on **Page 7**

TabM links:  
\- [arXiv](https://arxiv.org/abs/2410.24210)  
\- [GitHub](https://github.com/yandex-research/tabm)  
\- [Twitter thread](https://x.com/YuraFiveTwo/status/1856293601627566335)

TabReD links:

\- [arXiv](https://arxiv.org/abs/2406.19380)  
\- [GitHub](https://github.com/yandex-research/tabred)  
\- [Twitter thread](https://x.com/puhsuuu/status/1854149134124486924)

[The model illustration ](https://preview.redd.it/qsvl8qk4sg0e1.png?width=1722&format=png&auto=webp&s=519ff43ebd6a57501adb9cbdf39183b20af06cfc)",MachineLearning,58,8,1731414540.0,1gpjl9e,_puhsu,https://www.reddit.com/r/MachineLearning/comments/1gpjl9e/r_tabm_advancing_tabular_deep_learning_with/,Research
[D] Why is LLM Pruning Not as Generally Available as Quantization?,"I've been diving into the world of large language models (LLMs) and have been exploring various optimization techniques. One thing that's puzzled me is the disparity in the availability and adoption of quantization versus pruning.

**Quantization** seems to be a well-established and widely used technique for reducing the memory footprint and computational cost of LLMs. It's relatively straightforward to implement and has seen significant adoption in both research and industry.

On the other hand, **pruning**—which involves removing less important weights from the model—is less common. Despite its potential benefits, such as further reducing model size and inference time, it doesn't seem to be as generally available or as widely adopted. Many of my searches on the internet just result in research papers or proof of concept GitHub repos.

I'm curious about the reasons behind this disparity. Are there technical challenges with pruning that make it less practical? Is it more difficult to implement or integrate into existing workflows? Or are there other factors at play?",MachineLearning,56,21,1731367411.0,1gp6h2d,Soumil30,https://www.reddit.com/r/MachineLearning/comments/1gp6h2d/d_why_is_llm_pruning_not_as_generally_available/,Discussion
[R] Some Research Papers We Read,"The Vision Language Group at IIT Roorkee has curated a repository of comprehensive summaries for deep learning research papers from top-tier conferences like NeurIPS, CVPR, ICCV, ICML from 2016 to 2024. These summaries aim to provide a concise understanding of influential papers in fields such as computer vision, natural language processing, and machine learning. The collection is constantly growing, with new summaries added frequently. Here are a few notable examples:



- \*\*DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation\*\*, CVPR'23  

  \[DreamBooth Summary\](https://github.com/vlgiitr/papers\_we\_read/blob/master/summaries/DreamBooth.md)



- \*\*Segment Anything\*\*, ICCV'23  

  \[Segment Anything Summary\](https://github.com/vlgiitr/papers\_we\_read/blob/master/summaries/Segment\_Anything.md)



- \*\*An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion\*\*, ICCV'23  

  \[Textual Inversion Summary\](https://github.com/vlgiitr/papers\_we\_read/blob/master/summaries/Textual\_inversion.md)



- \*\*Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding\*\*, NIPS'22  

  \[Photorealistic Diffusion Summary\](https://github.com/vlgiitr/papers\_we\_read/blob/master/summaries/imagen.md)



- \*\*An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\*\*, ICLR'21  

  \[Vision Transformer Summary\](https://github.com/vlgiitr/papers\_we\_read/blob/master/summaries/Vision\_Transformer.md)



- \*\*Big Bird: Transformers for Longer Sequences\*\*, NIPS'20  

  \[Big Bird Transformers Summary\](https://github.com/vlgiitr/papers\_we\_read/blob/master/summaries/Big\_Bird\_Transformers.md)



The repository invites contributions from the community. If you find the summaries helpful, you are encouraged to submit your own summaries for research papers. The team aims to regularly update the collection with summaries of papers from upcoming conferences and key topics in deep learning and AI. 



You can access the full repository and contribute here:  

\[Vision Language Group Paper Summaries\](https://github.com/vlgiitr/papers\_we\_read)



By contributing, you'll help make advanced research more accessible to both beginners and experts in the field.",MachineLearning,58,3,1726808348.0,1fl4bi0,vlg_iitr,https://www.reddit.com/r/MachineLearning/comments/1fl4bi0/r_some_research_papers_we_read/,Research
[D] Flow matching is actually very different from (continuous) normalising flow?,"I was looking at the [flow matching](https://arxiv.org/pdf/2210.02747) paper and saw that flow matching is often considered as just an alternative implementation of continuous normalising flow. But after comparing the methodologies more closely, it seems there is a very significant distinction. In the flow matching paper, it is mentioned that for a data sample x1 (I assume this refers to individual data points like a single image), we can put an ""dummy"" distribution such as a very tight Gaussian on it, then construct a conditional probability path p_t(x|x1). Therefore what we learn is a transformation between the small Gaussian (t=1) on the data point to a standard Gaussian (t=0), for every data point. This implies that the latent space, when trained over the entire dataset, is the overlapped mixture of all the standard Gaussians that each individual data point maps to. The image of the small Gaussian ball for each individual image is the entire standard Gaussian.

However this does not seem to be what we do with regular normalising flows. In normalising flows, we try to learn a mapping that transforms the ENTIRE distribution of the data to the standard Gaussian, such that each data point has a fixed location in the latent space, and jointly the image of the dataset is normally distributed in the latent space. In practice we may take minibatches and optimise a score (e.g. KL or MMD) that compares the image of the minibatch with a standard Gaussian. Each location in the latent space can be uniquely inverted to a fixed reconstructed data point.

I am not sure if I am missing anything, but this seems to be a significant distinction between the two methods. In NF the inputs are encoded in the latent space, whereas flow matching as described in the paper seems to MIX inputs in the latent space. If my observations are true, there should be a few implications:

1. You can semantically interpolate in NF latent space, but it is completely meaningless in the FM case
2. Batch size is important for NF training but not FM training
3. NF cannot be ""steered"" the same way as diffusion models or FM, because the target image is already determined the moment you sample the initial noise

I wonder if anyone here has also looked into these questions and can inform me whether this is indeed the case, or whether something I missed made them more similar de facto. I appreciate any input to the discussion!",MachineLearning,58,12,1732519558.0,1gzdera,aeroumbria,https://www.reddit.com/r/MachineLearning/comments/1gzdera/d_flow_matching_is_actually_very_different_from/,Discussion
[R]Geometric aperiodic fractal organization in Semantic Space : A Novel Finding About How Meaning Organizes Itself ,"Hey friends! I'm sharing this here because I think it warrants some attention, and I'm using methods that intersect from different domains, with Machine Learning being one of them.

Recently I read Tegmark & co.'s paper on Geometric Concepts [https://arxiv.org/abs/2410.19750](https://arxiv.org/abs/2410.19750) and thought that it was fascinating that they were finding these geometric relationships in llms and wanted to tinker with their process a little bit, but I didn't really have access or expertise to delve into LLM innards, so I thought I might be able to find something by mapping its output responses with embedding models to see if I can locate any geometric unity underlying how llms organize their semantic patterns. Well I did find that and more...

I've made what I believe is a significant discovery about how meaning organizes itself geometrically in semantic space, and I'd like to share it with you and invite collaboration.

**The Initial Discovery**

While experimenting with different dimensionality reduction techniques (PCA, UMAP, t-SNE, and Isomap) to visualize semantic embeddings, I noticed something beautiful and striking; a consistent ""flower-like"" pattern emerging across all methods and combinations thereof. I systematically weeded out the possibility that this was the behavior of any single model(either embedding or dimensional reduction model) or combination of models and what I've found is kind of wild to say the least. It turns out that this wasn't just a visualization artifact, as it appeared regardless of:

\- The reduction method used

\- The embedding model employed

\- The input text analyzed

https://preview.redd.it/pdyq50s1ob2e1.png?width=907&format=png&auto=webp&s=b9ecf9206c1c2b43881341e8ad51950cf73b345c

https://preview.redd.it/b2u3uz93ob2e1.png?width=1909&format=png&auto=webp&s=6448776ebaeb5620b2079c7fed6992b3a813d619

[cross-section of the convergence point\(Organic\) hulls](https://preview.redd.it/t59tzz2qob2e1.png?width=1339&format=png&auto=webp&s=a9a0cd3132191db5a2ea163c87e8dfe336f9320c)

[a step further, showing how they form with self similarity.](https://preview.redd.it/q0pmaveqob2e1.png?width=1339&format=png&auto=webp&s=863dd23a1899efc8bf266c0702cf3258643859c3)

**Verification Through Multiple Methods**

To verify this isn't just coincidental, I conducted several analyses, rewrote the program and math 4 times and did the following:

1. Pairwise Similarity Matrices

Mapping the embeddings to similarity matrices reveals consistent patterns:

\- A perfect diagonal line (self-similarity = 1.0)

\- Regular cross-patterns at 45° angles

\- Repeating geometric structures

https://preview.redd.it/ft89ukpaob2e1.png?width=460&format=png&auto=webp&s=9900f9113fad02841e5e18cb0bc5f9b6b66275e1

https://preview.redd.it/f2yzbvnbob2e1.png?width=433&format=png&auto=webp&s=4a13a8e910794c64375ab0628f6f34006c31fb2f

Relevant Code:  
python

def analyze\_similarity\_structure(embeddings):

similarity\_matrix = cosine\_similarity(embeddings)

eigenvalues = np.linalg.eigvals(similarity\_matrix)

sorted\_eigenvalues = sorted(eigenvalues, reverse=True)

return similarity\_matrix, sorted\_eigenvalues

2. Eigenvalue Analysis

The eigenvalue progression as more text is added, regardless of content or languages shows remarkable consistency like the following sample:

First Set of eigenvalues while analyzing The Red Book by C.G. Jung in pieces:  
\[35.39, 7.84, 6.71\]

Later Sets:  
\[442.29, 162.38, 82.82\]

\[533.16, 168.78, 95.53\]

\[593.31, 172.75, 104.20\]

\[619.62, 175.65, 109.41\]

https://preview.redd.it/hesf440job2e1.png?width=1088&format=png&auto=webp&s=b531499fe8043e0b41390229bd0b04017373c49b

Key findings:

\- The top 3 eigenvalues consistently account for most of the variance

\- Clear logarithmic growth pattern

\- Stable spectral gaps i.e: (35.79393)

3. Organic Hull Visualization

The geometric structure becomes particularly visible when visualizing through organic hulls:

Code for generating data visualization through sinusoidal sphere deformations:  
python

def generate\_organic\_hull(points, method='pca'):

phi = np.linspace(0, 2\*np.pi, 30)

theta = np.linspace(-np.pi/2, np.pi/2, 30)

phi, theta = np.meshgrid(phi, theta)

center = np.mean(points, axis=0)

spread = np.std(points, axis=0)

x = center\[0\] + spread\[0\] \* np.cos(theta) \* np.cos(phi)

y = center\[1\] + spread\[1\] \* np.cos(theta) \* np.sin(phi)

z = center\[2\] + spread\[2\] \* np.sin(theta)

return x, y, z

\`\`\`

What the this discovery suggests is that meaning in semantic space has inherent geometric structure that organizes itself along predictable patterns and shows consistent mathematical self-similar relationships that exhibit golden ratio behavior like a penrose tiling, hyperbolic coxeter honeycomb etc and these patterns persist across combinations of different models and methods. I've run into an inverse of the problem that you have when you want to discover something; instead of finding a needle in a haystack, I'm trying to find a single piece of hay in a stack of needles, in the sense that nothing I do prevents these geometric unity from being present in the semantic space of all texts. The more text I throw at it, the more defined the geometry becomes.

https://preview.redd.it/3hho1avzob2e1.png?width=1239&format=png&auto=webp&s=a446d6b71ba0166c842e9537c6cd228662bb2682

I think I've done what I can so far on my own as far as cross-referencing results across multiple methods and collecting significant raw data that reinforces itself with each attempt to disprove it.

So I'm making a call for collaboration:

I'm looking for collaborators interested in:

1. Independently verifying these patterns
2. Exploring the mathematical implications
3. Investigating potential applications
4. Understanding the theoretical foundations

My complete codebase is available upon request, including:

\- Visualization tools

\- Analysis methods

\- Data processing pipeline

\- Metrics collection

If you're interested in collaborating or would like to verify these findings independently, please reach out. This could have significant implications for our understanding of how meaning organizes itself and potentially for improving language models, cognitive science, data science and more.

\*TL;DR: Discovered consistent geometric patterns in semantic space across multiple reduction methods and embedding models, verified through similarity matrices and eigenvalue analysis. Looking for interested collaborators to explore this further and/or independently verify.

\##EDIT##: I

I need to add some more context I guess,  because it seems that I'm being painted as a quack or a liar without being given the benefit of the doubt. Such is the nature of social media though I guess.

This is a cross-method, cross-model discovery using semantic embeddings that retain human interpretable relationships. i.e. for the similarity matrix visualizations, you can map the sentences to the eigenvalues and read them yourself. Theres nothing spooky going on here, its plain for your eyes and brain to see.

Here are some other researchers who are like-minded and do it for a living.

(Athanasopoulou et al.) supports our findings:

""The intuition behind this work is that although the lexical semantic space proper is high-dimensional, it is organized in such a way that interesting semantic relations can be exported from manifolds of much lower dimensionality embedded in this high dimensional space."" [https://aclanthology.org/C14-1069.pdf](https://aclanthology.org/C14-1069.pdf)

A neuroscience paper(Alexander G. Huth 2013) reinforces my findings about geometric organization:""An efficient way for the brain to represent object and action categories would be to organize them into a continuous space that reflects the semantic similarity between categories.""  
[https://pmc.ncbi.nlm.nih.gov/articles/PMC3556488/](https://pmc.ncbi.nlm.nih.gov/articles/PMC3556488/)

""We use a novel eigenvector analysis method inspired from Random Matrix Theory and show that semantically coherent groups not only form in the row space, but also the column space.""  
[https://openreview.net/pdf?id=rJfJiR5ooX](https://openreview.net/pdf?id=rJfJiR5ooX)

I'm getting some hate here, but its unwarranted and comes from a lack of understanding. The automatic kneejerk reaction to completely shut someone down is not constructive criticism, its entirely unhelpful and unscientific in its closed-mindedness.",MachineLearning,57,61,1732224598.0,1gwqvt2,Own_Dog9066,https://www.reddit.com/r/MachineLearning/comments/1gwqvt2/rgeometric_aperiodic_fractal_organization_in/,Research
[N] Open weight (local) LLMs FINALLY caught up to closed SOTA?,"Yesterday Pixtral large dropped [here](https://mistral.ai/news/pixtral-large/).

It's a 124B multi-modal vision model. This very small models beats out the 1+ trillion parameter GPT 4o on various cherry picked benchmarks. Never mind the Gemini-1.5 Pro. 

As far as I can tell doesn't have speech or video. But really, does it even matter? To me this seems groundbreaking. It's free to use too. Yet, I've hardly seen this mentioned in too many places. Am I missing something?  
  
BTW, it still hasn't been 2 full years yet since ChatGPT was given general public release November 30, 2022. In barely 2 years AI has become somewhat unrecognizable. Insane progress.

  
**\[Benchmarks Below\]**

https://preview.redd.it/ebo9qp0rzy1e1.png?width=1777&format=png&auto=webp&s=3d47183ba7e2af69eb52fc5f8d755f105cb52004

https://preview.redd.it/woc0wmrozy1e1.png?width=1852&format=png&auto=webp&s=1bc5d380e2deebfd03684e1a8341254d18596d8e

  
",MachineLearning,54,23,1732071624.0,1gvfpdw,AIAddict1935,https://www.reddit.com/r/MachineLearning/comments/1gvfpdw/n_open_weight_local_llms_finally_caught_up_to/,News
[R] When Machine Learning Tells the Wrong Story,,MachineLearning,58,12,1731170410.0,1gne2x1,jackcook,https://jackcook.com/2024/11/09/bigger-fish.html,Research
"[Discussion] What are some the informative blogs on machine learning , Deep learning or NLP?",can you share them,MachineLearning,62,14,1727701940.0,1fsv7js,None,https://www.reddit.com/r/MachineLearning/comments/1fsv7js/discussion_what_are_some_the_informative_blogs_on/,Discussion
[R] Llama-3.2-3B-Instruct-uncensored,"This is an uncensored version of the original [Llama-3.2-3B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct), created using [mlabonne](https://huggingface.co/mlabonne)'s [script](https://colab.research.google.com/drive/1VYm3hOcvCpbGiqKZb141gJwjdmmCcVpR?usp=sharing), which builds on [FailSpy's notebook](https://huggingface.co/failspy/llama-3-70B-Instruct-abliterated/blob/main/ortho_cookbook.ipynb) and the original work from [Andy Arditi et al.](https://colab.research.google.com/drive/1a-aQvKC9avdZpdyBn4jgRQFObTPy1JZw?usp=sharing). The method is discussed in details in this [blog](https://huggingface.co/blog/mlabonne/abliteration) and this [paper](https://arxiv.org/abs/2406.11717).

You can find the uncensored model [here](https://huggingface.co/chuanli11/Llama-3.2-3B-Instruct-uncensored) and play with it in this 🤗 [space](https://huggingface.co/spaces/chuanli11/Chat-Llama-3.2-3B-Instruct-uncensored).",MachineLearning,55,9,1727452387.0,1fqqzuh,chuanli11,https://www.reddit.com/r/MachineLearning/comments/1fqqzuh/r_llama323binstructuncensored/,Research
"[D] Is Unlimited Context Length really possible?:  ""Unlimiformer"" author discusses NeurIPS paper Friday","Is Unlimited Context Length really possible?  At what cost?

Amanda Bertsch, author of 2023 NeurIPS paper Unlimiformer,  will describe the architecture and take questions at this Friday's [Oxen.ai](http://Oxen.ai) Paper Club.

Greg Schoeninger u/FallMindless3563, Oxen CEO and Master of Plain Speak, will help interp the concept and relate it to other papers we have reviewed.

Call:  [https://oxen.ai/community](https://oxen.ai/community)

The trick asserted to make Unlimited Context Length possible:   Offload the cross attention calc to a K-Nearest Neighbors (K-NN) index.

I tweeted someone's clever animation of K-NN here: [https://x.com/ParallaxAngle/status/1817672116243972287](https://x.com/ParallaxAngle/status/1817672116243972287)

Paper:  [https://arxiv.org/abs/2305.01625](https://arxiv.org/abs/2305.01625)

Greg, I'll reply with my first 5 questions.  I've only read the abstract so far.",MachineLearning,57,11,1722440872.0,1egqitt,ReluOrTanh,https://www.reddit.com/r/MachineLearning/comments/1egqitt/d_is_unlimited_context_length_really_possible/,Discussion
[R] Neural networks have been trained to accurately predict the optimal geometry of molecules using 50 times less data,"An important task of computational chemistry is to find molecular geometries where a local energy minimum is achieved, as these are the most likely configurations in which the molecule undergoes a chemical reaction. Despite recent progress in neural networks for molecular conformation energy prediction, such models are prone to errors due to distribution shifts, leading to inaccurate energy minimization. The quality of energy minimization with neural networks can be improved by providing optimization trajectories as additional training data. Still, obtaining complete optimization trajectories demands a lot of extra computations.

A team of researchers developed a new framework called Gradual Optimization Learning Framework (GOLF), consisting of an efficient data-collecting scheme and an external optimizer. The author demonstrated that using significantly less additional data, the neural network trained with GOLF performs on par with the oracle on a benchmark of diverse drug-like molecules. 

The [~paper~](https://openreview.net/forum?id=FMMF1a9ifL) is published in the ICLR 2024 conference proceedings",MachineLearning,55,9,1721635469.0,1e98s8l,AIRI_Institute,https://www.reddit.com/r/MachineLearning/comments/1e98s8l/r_neural_networks_have_been_trained_to_accurately/,Research
[R] Learning to (Learn at Test Time): RNNs with Expressive Hidden States,,MachineLearning,54,10,1720468891.0,1dyiidu,SchmidhuberDidIt,https://arxiv.org/abs/2407.04620v1,Research
[D] What are open unsolved interesting problems in machine learning?,I am curious what is the next big leap forward in machine learning. What are some obstacles out there that if solved machine learning would become even more useful? Or this question could be phrased differently. In what problems a machine learning approach hasnt been applied yet when it could turn out useful.,MachineLearning,58,69,1719105058.0,1dmak2a,marshallggggg,https://www.reddit.com/r/MachineLearning/comments/1dmak2a/d_what_are_open_unsolved_interesting_problems_in/,Discussion
[D] Is it me or does it seem like benchmarks are making language models worse?,"These used to be super useful in the past, all across the board. Now most language models are always ignoring simple instructions. LLama3 seems to be the best, and Claude was decent. GPT-4o feels really sloppy and always ignoring instructions or gives something similar but not asked for at all. The only thing I noticed changing was the focus on the benchmarks since Google came out with Gemini. Do you think these benchmarks are making language models worse by having developers optimize for them too much? Similar situation how a GAN can sometimes break by finding a hack in the discriminator despite it not being accurate behavior? (Which with some of them using language models make it easier to hack). 

Edit: 
This is to the man babies who are being total jerks instead of just having a well intentioned discussion.

very unprofessional behavior. This is just discussing observations. Of course I know what stats are. You are stupid to if you think numbers tell everything. This is meant to discuss potential improvements that could be made or if there is lacking assessments from these benchmarks that could explain why people think that.
",MachineLearning,54,37,1717765518.0,1daa68e,I_will_delete_myself,https://www.reddit.com/r/MachineLearning/comments/1daa68e/d_is_it_me_or_does_it_seem_like_benchmarks_are/,Discussion
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)",MachineLearning,61,33,1715767325.0,1csh3tv,jens_97,https://www.reddit.com/r/MachineLearning/comments/1csh3tv/discussion_what_are_sota_uncertainty/,Discussion
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!",MachineLearning,57,40,1715752741.0,1csdsje,C0hentheBarbarian,https://www.reddit.com/r/MachineLearning/comments/1csdsje/d_those_in_the_industry_how_are_you_using_open/,Discussion
[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.",MachineLearning,56,26,1715706812.0,1crxhfp,PK_thundr,https://www.reddit.com/r/MachineLearning/comments/1crxhfp/d_is_bert_still_relevant_in_2024_for_an_emnlp/,Discussion
[D] Realism of Landing a PhD Offer,"Hi, everyone! I am a postgraduate at University College London, pursuing a Master's in Machine Learning, and I will soon be applying for admission to PhD programs that start in Fall, 2025. I will share my profile and the schools I will be applying to, and am hoping to learn if the labs I am aiming for are beyond my reach.

I received my undergraduate degree in Mathematics and CS with first-class (honors) from Nanyang Technological University, Singapore, and am expected to earn my postgraduate degree with first-class (honors) as well. I am interested in theoretical deep learning -- problems around curvature of loss surface, optimization trajectories, learning dynamics and generalization -- which are mathematically intense research areas. Although my coursework has remained mostly theoretical and well aligned with such research (by design), my research experience has been more experimental. I have a third-author publication at ICML, on the work I did for my bachelor's thesis project. It is a fairly theoretical work, but I was responsible only for the experiments. I also have a 2 first-author pre-prints -- one experimental work on NLP (aiming for an IEEE publication), and another in graph ML (aiming for one of the top conferences), which has a decent theoretical component, but not as much as the work I hope to do in my PhD.

I am aiming for labs in ETH, UCL, Stanford, NYU, EPFL, Columbia and Princeton (in that order of preference, one of these is my pos). All of them have very successful PIs (by citations), who work on topics very well-aligned with my interests. My concern is that my seemingly all-over-the-place research background might turn them off, but I am hoping that my grades will convince them that I am competent with theory. I expect my supervisors to write excellent recommendation letters since they have appreciated me on numerous occasions. I am hoping to write a convincing research statement, but since I only started reading on relevant literature a couple of weeks back, it may not end up being excellent.

I don't mind working with a younger PI, as long as I have some researchers working on adjacent topics around me. With senior labs, there is a network already established, and I can probably start by assisting on some projects, before getting into independent research. Realistically, am I punching about my weight? If I am, can someone suggest younger PIs working on aforementioned research topics, whose lab I might have a better shot of joining?",MachineLearning,55,77,1728815936.0,1g2mugf,mio_11,https://www.reddit.com/r/MachineLearning/comments/1g2mugf/d_realism_of_landing_a_phd_offer/,Discussion
[R] Intelligence at the Edge of Chaos,,MachineLearning,55,22,1728551293.0,1g0elvw,hardmaru,https://www.arxiv.org/abs/2410.02536,Research
[D] What do you do when your model trains?,How do you pass the time?,MachineLearning,56,70,1728106001.0,1fwjafn,Replay0307,https://www.reddit.com/r/MachineLearning/comments/1fwjafn/d_what_do_you_do_when_your_model_trains/,Discussion
[D] Post any b*ginner questions to r/MLQuestions!,"I have recently inherited the subreddit r/MLQuestions, as the other mods had been innactive for 10 months and 4 years respectively. I have been sprucing up the sub, adding flairs, rules, etc, and I am trying to increase engagement and make it more useful for those who want to ask questions. Basically, stackoverflow but dedicated to beginn\*r questions about ML. So if any of you have questions that your are too embarrased to ask here, ask at r/MLQuestions! I will also be introducing a system similar to r/changemyview, where each question someone answers, they get an increment to their user flair that shows how many questions they have answered!

BTW the mods gave me permission to post this, so thank you guys for this, very cool.",MachineLearning,57,25,1724925039.0,1f3yfjg,NoLifeGamer2,https://www.reddit.com/r/MachineLearning/comments/1f3yfjg/d_post_any_bginner_questions_to_rmlquestions/,Discussion
"[R] 1.5-Pints Technical Report: Pretraining in Days, Not Months -- Your Language Model Thrives on Quality Data (2408.03506)",,MachineLearning,56,20,1723468225.0,1eqdbro,mouse0_0,https://arxiv.org/abs/2408.03506,Research
[R] alphaXiv - a comments section for ArXiv,"I've been working on an arXiv labs project, [alphaXiv.org](http://alphaXiv.org), which is a comment and discussion section for papers directly built on top of arXiv. I feel that a lot of readers often have the same questions on papers and so I hope having a central forum could be of great to the research community. Last week, we were featured by [Stanford's AI Lab.](https://x.com/StanfordAILab?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor)

  
Please check it out and let me know what you think! This project is in active development, so please DM me if you would like to collaborate or have feedback.",MachineLearning,55,4,1722973005.0,1elqzle,Vivid_Perception_143,https://www.reddit.com/r/MachineLearning/comments/1elqzle/r_alphaxiv_a_comments_section_for_arxiv/,Research
[P] LoRA from scratch implementation for LLM classifier training,,MachineLearning,57,6,1715438981.0,1cpj6b9,seraschka,https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-E/01_main-chapter-code/appendix-E.ipynb,Project
[D] How to make friends and network at NeurIPS?,"I’m attending NeurIPS for the first time and it’s quite overwhelming seeing the amount of people and so many recruiters. I come from a not so well known university, and have come to the conference completely alone, not even my supervisor is here.

I didn’t really end up talking to many other attendees or recruiters because (1) it just seemed hard to approach others who are in big groups of people and (2) I’m feeling strong imposter syndrome and under-qualified for the jobs recruiters offer. I only got a workshop paper accepted that is more application and not as technical as many of the other students.

Any advice for how I can make the most of the rest of the conference? On that note, would anyone also want to potentially meet up and have a chat? I’m a 3rd year PhD student from the UK, but from Vancouver myself so know lots of stuff going on in the area. Cheers!",MachineLearning,56,8,1733943266.0,1hc0x89,K_is_for_Karma,https://www.reddit.com/r/MachineLearning/comments/1hc0x89/d_how_to_make_friends_and_network_at_neurips/,Discussion
[P] How to build a custom text classifier without days of human labeling,"Hi, I work at Hugging Face. Me and my team have worked on this cool example of how to go from an LLM to a small and efficient classification model. We use the LLM to auto-label a dataset, which we then fine-tuned after a quick review. We show how it helped us simplify workflows, saving time and resources while still delivering a high-performing model. with higher accuracy while only labelling a couple of examples.

Blogpost: [https://huggingface.co/blog/sdiazlor/custom-text-classifier-ai-human-feedback](https://huggingface.co/blog/sdiazlor/custom-text-classifier-ai-human-feedback)",MachineLearning,55,7,1729177994.0,1g5t7lq,chef1957,https://www.reddit.com/r/MachineLearning/comments/1g5t7lq/p_how_to_build_a_custom_text_classifier_without/,Project
[N] Reinforcement Learning Cheat Sheet,"**Hi everyone!**

I just published my first post on Medium and also created a **Reinforcement Learning Cheat Sheet**. 🎉

I'd love to hear your feedback, suggestions, or any thoughts on how I can improve them!

Feel free to check them out, and thanks in advance for your support! 😊

[https://medium.com/@ruipcf/reinforcement-learning-cheat-sheet-39bdecb8b5b4](https://medium.com/@ruipcf/reinforcement-learning-cheat-sheet-39bdecb8b5b4)",MachineLearning,56,8,1727693785.0,1fssp75,Prudent_Nose921,https://www.reddit.com/r/MachineLearning/comments/1fssp75/n_reinforcement_learning_cheat_sheet/,News
[P] VisionTS: Zero-Shot Time Series Forecasting with Visual Masked Autoencoders,"VisionTS is a newly pretrained model that redefines forecasting task as an image reconstruction task. The technique seems counter-intuitive at first, but the model works surprisingly well.

A detailed analysis of the model can be found [here](https://aihorizonforecast.substack.com/p/visionts-building-high-performance).

[VisionTS](https://preview.redd.it/787my5ohtrrd1.png?width=881&format=png&auto=webp&s=fac0840e550246cc99e758f2972c75c1c2da87b8)",MachineLearning,54,14,1727625601.0,1fs7opx,apaxapax,https://www.reddit.com/r/MachineLearning/comments/1fs7opx/p_visionts_zeroshot_time_series_forecasting_with/,Project
[N] The last paper in the Matrix Profile series: “Matrix Profile XXXI: Motif-Only Matrix Profile: Orders of Magnitude Faster”  ,"Dear Colleagues

I am delighted to announce the last paper in the Matrix Profile series: “Matrix Profile XXXI: Motif-Only Matrix Profile: Orders of Magnitude Faster”  (or, as it will be known as, the “MOMP” paper) \[a\].

I don’t think every paper needs an announcement, but…

1)      This paper comes bundled with a huge new set of benchmark datasets that will become widely used.

2)      For students and young professors looking for interesting problems to solve, the paper outlines several interesting challenges that are worthy of investigation.

3)      For researchers that actually need to find time series motifs for their research, the bundled code will let them consider datasets one to two orders of magnitude larger.

4)      The paper has minor “historical” significance, being the last in a series of thirty highly cited papers.

To give the reader some idea as to how influential the Matrix Profile is, note that it has just become an official part of the Matlab language \[b\].

In an expanded version of the paper \[a\], I take the time to offer reflections on the Matrix Profile series, and to offer thanks to the dozens of people that helped me realize my time series data mining vision.

The paper offers the first contribution to speeding up exact time series motif discovery in eight years (except for hardware based ideas), by introducing the first lower bound to the Matrix Profile.

\[a\] Matrix Profile XXXI: Motif-Only Matrix Profile: Orders of Magnitude Faster. [https://www.dropbox.com/scl/fi/mt8vp7mdirng04v6llx6y/MOMP\_DeskTop.pdf?rlkey=gt6u0egagurkmmqh2ga2ccz85&dl=0](https://www.dropbox.com/scl/fi/mt8vp7mdirng04v6llx6y/MOMP_DeskTop.pdf?rlkey=gt6u0egagurkmmqh2ga2ccz85&dl=0)

\[b\] [https://www.mathworks.com/help/predmaint/ref/matrixprofile.html](https://www.mathworks.com/help/predmaint/ref/matrixprofile.html)

https://preview.redd.it/it16c6h8vgqd1.jpg?width=2602&format=pjpg&auto=webp&s=578a65723507c597ee4140d2eed17ba5938f326d

",MachineLearning,58,8,1727057083.0,1fn9s96,eamonnkeogh,https://www.reddit.com/r/MachineLearning/comments/1fn9s96/n_the_last_paper_in_the_matrix_profile_series/,News
[D] What are your strategies/tools to find relevant literature and stay up-to-date? ,"Dear all,

When I was a PhD student, it was somehow easy to find relevant papers, as I was on a single topic. Now, I am in industry and I am interested in a wider range of papers because I have to generate interesting ideas. So I want to 1/ setup a routine to build the habit of reading everyday, 2/ be exposed to interesting papers, maybe outside of my field. What are your own strategies and tools, or even newsletters you use for that?

In the past I used twitter a lot, but its now governed by trends and hype, mostly LLMs so I do not find many papers there anymore. Scholar Inbox is great, but it is very focused on specific topics, not really aiming to be diverse.

Thanks!",MachineLearning,55,25,1719750429.0,1ds0caj,poiret_clement,https://www.reddit.com/r/MachineLearning/comments/1ds0caj/d_what_are_your_strategiestools_to_find_relevant/,Discussion
[D] Detecting Objects of Same Shape but Different Colors,"I'm struggling with detecting objects that have the same shape but different colors, with no other distinguishing features. When there are distinguishable patterns, CNN-based architectures like YOLO work wonders and achieve high accuracy. However, I need a method that can accurately classify objects based purely on color.

My current challenge is that these objects are not separable when I try to segment them by color in RGB space. Does anyone have suggestions or methods that achieve good accuracy in determining object classes by color?

I've included an image below for reference. Any help would be greatly appreciated!

https://preview.redd.it/83c6e7dbbb2d1.png?width=793&format=png&auto=webp&s=532c7cffcbaea96eb48d374e073bd49d5f029212

# Edit

Transforming the colorspace to HSV solved the problem. Below is the HSV colorspace representation

https://preview.redd.it/hq5yrf5wcf2d1.png?width=793&format=png&auto=webp&s=c40002fd360c1891baaed915f536aa7dae4061f5

In the first stage, I used YOLO model to detect the objects

In the second stage, I cropped the detected objects, converted the cropped images to HSV, calculated the average component value per object and then trained XGBoost model to predict color label based on 3D vector, representing the average values of H, S, and V channel.",MachineLearning,56,15,1716529450.0,1czdmty,ThickDoctor007,https://www.reddit.com/r/MachineLearning/comments/1czdmty/d_detecting_objects_of_same_shape_but_different/,Discussion
[D] Deep Learning in Time Series: Are They Used in Industry?,"Hey folks! I’m a researcher in time series and have been seeing a lot of buzz around deep learning models in this area. I am wondering if these models actually being deployed in production, or are classical methods still the go-to in the industry?



For instance, in weather forecasting, physics-based numerical weather prediction (NWP) seems to dominate. If deep models aren’t getting much traction, have you come across any practical use cases for them? Would love to hear your thoughts!",MachineLearning,54,26,1733215006.0,1h5izk5,Few-Pomegranate4369,https://www.reddit.com/r/MachineLearning/comments/1h5izk5/d_deep_learning_in_time_series_are_they_used_in/,Discussion
[D] Is TMLR good enough to consider as an alternative to A* conferences?,"Hi there, I am a current PhD student in Artificial Intelligence working on Multi-Armed Bandits. More recently, I have completed one of my works on the intersection of Bandits and LLMs and was wondering for a suitable venue for publication.

The closest conference I see is ICML having deadline of 31st January which is about two months from now, therefore was wondering about a suitable alternate venue. While previous reddit threads (a year back) claim that TMLR is better than AAAI, IJCAI and similar conferences but falls way short compared to ICML, NeurIPS, ICLR, etc, I was wondering if it's still true. 

Does the ML community still considers TMLR to be a potential place to submit it, given that the deadline for the closest conference is too far?",MachineLearning,52,11,1730418577.0,1ggsief,Fantastic-Nerve-4056,https://www.reddit.com/r/MachineLearning/comments/1ggsief/d_is_tmlr_good_enough_to_consider_as_an/,Discussion
[D] Help a NASA-funded project learn more about the Sun! (Kaggle Competition),"Hi all, my name is Hannah and I am the Communications person for the NASA-funded [Eclipse Megamovie 2024 project.](http://eclipsemegamovie.org) We were super active in April as the eclipse approached, but there is still way more excitement to come! We've launched a Kaggle competition, hoping to get help from communities such as this one. Below is more information about the project as a whole and a link to our competition page. Please feel free to ask any questions and I'll do my best to get them answered!

On April 8, 2024, a total solar eclipse began over the South Pacific Ocean and crossed North America, passing over Mexico, the United States, and Canada. The first location in continental North America that experienced totality was Mexico’s Pacific coast at around 11:07 a.m. PDT. Following the April 8, 2024, total solar eclipse, more than 145 volunteers uploaded over 1 terabyte of photographic data for use in our project.

Eclipse Megamovie 2024 (EM2024) is funded by NASA to study the sun using data collected during total solar eclipses, a special time when it is possible to study the Sun’s behavior unlike any other. The next stage, after the eclipse and the gathering of the data, is to categorize and label photographic data, and then we will be able to begin the scientific analysis in earnest–this is where you come in! 

If you are proficient in Python code and Machine Learning, you may be able to contribute to answering previously unanswered questions about the sun! 

Link to competition page: [https://www.kaggle.com/competitions/eclipse-megamovie](https://www.kaggle.com/competitions/eclipse-megamovie)

Competition participants will work with our 2017 total solar eclipse dataset to ""train"" a machine by writing code and utilizing the training dataset provided to automatically categorize eclipse photographs within one of several categories based on the phase of the eclipse. People interested in participating in this competition are recommended to have a working knowledge of python and machine learning fundamentals. **Interests that align with our competition:** photography, heliophysics and/or solar science research, participatory science, and machine learning.**Prizes:**

Leaderboard Prizes: Awarded based on private leaderboard ranking.

* **First Prize:** Image-stabilized binoculars with solar filters, Spotlight on the Eclipse Megamovie website, Eclipse Megamovie Team Patch, NASA Calendar, Eclipse Megamovie Sticker, First Prize Certificate.
* **Second Prize:** Spotlight on the Eclipse Megamovie website, Eclipse Megamovie Team Patch, NASA Calendar, Eclipse Megamovie Sticker, Second Prize Certificate.
* **Third Prize:** Spotlight on the Eclipse Megamovie website, Eclipse Megamovie Team Patch, NASA Calendar, Eclipse Megamovie Sticker, Third Prize Certificate.

Participants will help to ensure that the data \[photographs of eclipses\] can be quickly organized and have the correct information (metadata) associated with each image. By helping us develop code that accurately identifies the solar eclipse phases within photographs submitted by volunteers, you will enable us to cross a major data processing hurdle. With your code, you are paving the way for this NASA-funded research endeavor to study solar jets and plasma plumes!

Your mission is to create the most accurate sorting machine that categorizes a solar eclipse photograph into a specific solar eclipse phase. You will know you have succeeded if your code is able to successfully categorize the photographs provided into the following categories: Darks or flats (calibration shots), partial eclipse phases (bins \[categories\] of 20 degrees), the diamond ring phase, total solar eclipse phases, and of course a category for things that are not solar eclipses.

Special thanks to the Mods for letting me know which tag to use on this post :)

edited for words",MachineLearning,54,3,1728861099.0,1g32evj,EMegamovie2024,https://www.reddit.com/r/MachineLearning/comments/1g32evj/d_help_a_nasafunded_project_learn_more_about_the/,Discussion
[R] GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models (Apple),"arXiv:2410.05229 \[cs.LG\]: [https://arxiv.org/abs/2410.05229](https://arxiv.org/abs/2410.05229)  
Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar - Apple

TechCrunch - Devin Coldewey: Researchers question AI’s ‘reasoning’ ability as models stumble on math problems with trivial changes: [https://techcrunch.com/2024/10/11/researchers-question-ais-reasoning-ability-as-models-stumble-on-math-problems-with-trivial-changes/](https://techcrunch.com/2024/10/11/researchers-question-ais-reasoning-ability-as-models-stumble-on-math-problems-with-trivial-changes/)

Mehrdad Farajtabar, one of the co-authors, breaks down the paper in this thread on X: [https://x.com/MFarajtabar/status/1844456880971858028](https://x.com/MFarajtabar/status/1844456880971858028)",MachineLearning,57,23,1728724819.0,1g1wbir,Nunki08,https://www.reddit.com/r/MachineLearning/comments/1g1wbir/r_gsmsymbolic_understanding_the_limitations_of/,Research
[D] Which feeds do you look at?,"While arxiv and open review are the two best sources for new papers, I find certain feeds quite interesting as well. For me, this includes things like GitHub, Less Wrong, Hugging Face, Twitter, and Reddit. Am I missing any, and are there more? A list of blogs ? I wish there were a consolidation for these things.",MachineLearning,54,18,1727324843.0,1fpo0z8,Studyr3ddit,https://www.reddit.com/r/MachineLearning/comments/1fpo0z8/d_which_feeds_do_you_look_at/,Discussion
[D] [R] Are there any promising avenues for achieving efficient ML? ,"It would appear that the status quo of massive foundation models with billions (soon trillions) of parameters, trained on more or less the entire internet, is reaching a point of diminishing returns, perhaps even approaching an asymptote (let's at least assume this for the sake of discussion). There are also the tremendous costs associated with training and serving such models. This motivates the development of efficient ML: Software and hardware designed to train smaller models on less data at lower cost without compromising on performance and capability. What is the current SOTA in this field? Are there any avenues which seem more promising than others?

EDIT: I would prefer the discussion to be around **efficient neural networks** in general. Not limited to only LLMs. ",MachineLearning,54,56,1726074685.0,1fefhrz,worstthingsonline,https://www.reddit.com/r/MachineLearning/comments/1fefhrz/d_r_are_there_any_promising_avenues_for_achieving/,Discussion
[D] I created Promptimizer – a Genetic Algorithm (GA)-Based Prompt Optimization Framework,,MachineLearning,51,18,1722088185.0,1edgtft,NextgenAITrading,https://medium.com/p/bbcb9afaef83,Discussion
[P] Training a Simple Transformer Neural Net on Conway's Game of Life,"This exercise presents pretty much the simplest form of a transformer, and Conway's Game of Life is an easy source of data for it. Interestingly, it learns to compute what is basically a 3-by-3 average pool, (although not incuding the middle cell in the kernel).

Blog post: https://sidsite.com/posts/life-transformer/

Code: https://github.com/sradc/training-a-simple-transformer-on-conways-game-of-life",MachineLearning,54,23,1720452753.0,1dybuek,montebicyclelo,https://www.reddit.com/r/MachineLearning/comments/1dybuek/p_training_a_simple_transformer_neural_net_on/,Project
[P] REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models,"RLHF (Reinforcement Learning from Human Feedback) is rapidly evolving, with algorithms such as PPO, DPO, RLOO, ReMax and GRPO emerging one after another. **By integrating various optimization techniques from Proximal Policy Optimization (PPO) into the traditional REINFORCE algorithm**, we “proposed” **REINFORCE++,** which aims to enhance performance and stability in RLHF while reducing computational resource requirements without the critic network.

**The key feature of REINFORCE++ is that it is more stable than GRPO and faster than PPO.**

**REINFORCE++'s** technical details are in:

[https://hijkzzz.notion.site/reinforce-plus-plus](https://hijkzzz.notion.site/reinforce-plus-plus)

and (technical report)

[https://www.researchgate.net/publication/387487679\_REINFORCE\_A\_SIMPLE\_AND\_EFFICIENT\_APPROACH\_FOR\_ALIGNING\_LARGE\_LANGUAGE\_MODELS](https://www.researchgate.net/publication/387487679_REINFORCE_A_SIMPLE_AND_EFFICIENT_APPROACH_FOR_ALIGNING_LARGE_LANGUAGE_MODELS)

code

[https://github.com/OpenRLHF/OpenRLHF/blob/main/examples/scripts/train\_reinforce\_llama\_ray.sh](https://github.com/OpenRLHF/OpenRLHF/blob/main/examples/scripts/train_reinforce_llama_ray.sh)",MachineLearning,52,5,1735286725.0,1hna801,seventh_day123,https://www.reddit.com/r/MachineLearning/comments/1hna801/p_reinforce_a_simple_and_efficient_approach_for/,Project
[P] 🥂 FineWeb2 dataset: A sparkling update with 1000s of languages,,MachineLearning,52,2,1733647675.0,1h9ep0e,PhilipsNostrum,https://huggingface.co/datasets/HuggingFaceFW/fineweb-2,Project
[R] Grokfast: Accelerated Grokking by Amplifying Slow Gradients,"Paper: [https://arxiv.org/abs/2405.20233v2](https://arxiv.org/abs/2405.20233v2)

Code: [https://github.com/ironjr/grokfast](https://github.com/ironjr/grokfast)

Abstract: One puzzling artifact in machine learning dubbed grokking is where delayed generalization is achieved tenfolds of iterations after near perfect overfitting to the training data. Focusing on the long delay itself on behalf of machine learning practitioners, our goal is to accelerate generalization of a model under grokking phenomenon. By regarding a series of gradients of a parameter over training iterations as a random signal over time, we can spectrally decompose the parameter trajectories under gradient descent into two components: the fast-varying, overfitting-yielding component and the slow-varying, generalization-inducing component. This analysis allows us to accelerate the grokking phenomenon more than ×50 with only a few lines of code that amplifies the slow-varying components of gradients. The experiments show that our algorithm applies to diverse tasks involving images, languages, and graphs, enabling practical availability of this peculiar artifact of sudden generalization. ",MachineLearning,55,5,1723550881.0,1er66lu,Confident-Honeydew66,https://www.reddit.com/r/MachineLearning/comments/1er66lu/r_grokfast_accelerated_grokking_by_amplifying/,Research
[P] Grounded SAM 2: Ground and Track Anything,"https://preview.redd.it/13854j03q2hd1.jpg?width=1280&format=pjpg&auto=webp&s=0735848ae40c2591111fa4ed91d2c28ea829c0ac

With the release of SAM 2, we have taken the opportunity to update our Grounded SAM algorithm. The biggest improvement in SAM 2 compared to SAM is the expansion of its segmentation capabilities to video, allowing users to interactively segment any object and track it in video. However, the main issue with SAM 2 is that the segmented and tracked objects do not contain semantic information. To address this, we have continued the approach of Grounded SAM by **incorporating an open-set detection model**, Grounding DINO. This enables us to extend 2D open-set detection to video object segmentation and tracking.

We have release our code in

[https://github.com/IDEA-Research/Grounded-SAM-2](https://github.com/IDEA-Research/Grounded-SAM-2)

with **very easy implementations**, which is convenient for users.

Project Highlights:

In this repo, we've supported the following demo with **simple implementations**:

* **Ground and Segment Anything** with Grounding DINO, Grounding DINO 1.5 & 1.6 and SAM 2
* **Ground and Track Anything** with Grounding DINO, Grounding DINO 1.5 & 1.6 and SAM 2
* **Detect, Segment and Track Visualization** based on the powerful [https://github.com/roboflow/supervision](https://github.com/roboflow/supervision) library.

And we will continue update our code to make it easier for users.",MachineLearning,52,14,1722963346.0,1elmxnq,Technical-Vast1314,https://www.reddit.com/r/MachineLearning/comments/1elmxnq/p_grounded_sam_2_ground_and_track_anything/,Project
[R] Perpetual: a gradient boosting machine which doesn't need hyperparameter tuning,"Repo: https://github.com/perpetual-ml/perpetual

PerpetualBooster is a gradient boosting machine (GBM) algorithm that doesn't need hyperparameter tuning so that you can use it without hyperparameter optimization libraries unlike other GBM algorithms. Similar to AutoML libraries, it has a `budget` parameter. Increasing the `budget` parameter increases the predictive power of the algorithm and gives better results on unseen data.

The following table summarizes the results for the [California Housing](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html) dataset (regression):

| Perpetual budget | LightGBM n_estimators | Perpetual mse | LightGBM mse | Perpetual cpu time | LightGBM cpu time | Speed-up |
| ---------------- | --------------------- | ------------- | ------------ | ------------------ | ----------------- | -------- |
| 1.0              | 100                   | 0.192         | 0.192        | 7.6                | 978               | 129x     |
| 1.5              | 300                   | 0.188         | 0.188        | 21.8               | 3066              | 141x     |
| 2.1              | 1000                  | 0.185         | 0.186        | 86.0               | 8720              | 101x     |


PerpetualBooster prevents overfitting with a generalization algorithm. The paper is work-in-progress to explain how the algorithm works. Check our [blog post](https://perpetual-ml.com/blog/how-perpetual-works) for a high level introduction to the algorithm. ",MachineLearning,54,24,1721508439.0,1e858j4,mutlu_simsek,https://www.reddit.com/r/MachineLearning/comments/1e858j4/r_perpetual_a_gradient_boosting_machine_which/,Research
"[D] Medical/Healthcare AI Experts: Where do Clinical LLMs Mostly Fail?
","I recently had an interesting debate with a colleague about medical LLMs. As we discussed where these models tend to fail, I realized something concerning: it seems that engineers and computer science experts often overlook the insights of medical experts and healthcare professionals who actually use these LLMs in practice. This disconnect might be contributing to a lack of truly high-quality LLMs in the medical field.

This realization got me thinking more deeply about the current state of Large Language Models (LLMs) in the medical/clinical/healthcare field. Would like to know:

**Where do these specialized LLMs tend to fall short?**

If you're a medical expert, AI developer, or someone who works with these models, I'd love to hear your insights:

* Have you noticed any consistent patterns in their failures?
* Are there particular tasks where they struggle, such as:
   * Named Entity Recognition (NER)
   * Summarization
   * Clinical note generation
   * Other areas?

Really interested in hearing your professional opinions and observations. Especially from those of you on the medical/healthcare side who might feel your input is often overlooked.

Thanks in advance for sharing your knowledge!",MachineLearning,54,49,1721416597.0,1e7bwun,aadityaura,https://www.reddit.com/r/MachineLearning/comments/1e7bwun/d_medicalhealthcare_ai_experts_where_do_clinical/,Discussion
[D] Is Mojo worth it or which second language would you learn for ML?,"Basically the title. I am quite proficient with python (as expected) but beside that, I have only very rudimentary knowledge of javascript and C++. I want to learn a second language that is more ""low level"" and can leverage the hardware features better. My goal is not to rewrite pytorch or replace python entirely (although [porting the inference to mojo may make sense](https://github.com/tairov/llama2.mojo)), but provide alternatives for performance critical use cases.

Looking at today's situation, the obvious answer is C++. However, Rust has been gaining popularity and besides the steep learning curve, people are starting to put it above C++ in many aspects. In both cases, the syntax and languages are not very close to python, which makes them difficult to learn. Mojo seems much better in this respect, providing both low level features where the syntax somehow resembles Rust (at least for a layman like me) but also can be used as weird python flavour. It even allows importing python libraries directly. This is very helpful for such a young language, where a large community and a variety of libraries are missing. Nevertheless, the language is still young and very much subject to change so I am not sure if I should invest.

So what do you think is the best ""second"" language for the use cases above? Any experience with Mojo? How did you learn it or any other language with a limited number of resources. I am planning to read through the docs and solve last years advent of code using it if I go with Mojo.",MachineLearning,52,92,1717240504.0,1d5kov5,canbooo,https://www.reddit.com/r/MachineLearning/comments/1d5kov5/d_is_mojo_worth_it_or_which_second_language_would/,Discussion
[R] Have you give a try to use Intel and AMD GPUs to train models?,"NVIDIA rules the market of GPUs for datacenter. The most adopted frameworks for ML support NVIDIA GPUs. But I'm wondering about the drawbacks and advantages of using Intel and AMD GPUs to train ML models. Have you experienced use these GPUs? What could you say about performance, usability, software stack and ecosystem?",MachineLearning,51,12,1716253521.0,1cwvoaq,Various_Protection71,https://www.reddit.com/r/MachineLearning/comments/1cwvoaq/r_have_you_give_a_try_to_use_intel_and_amd_gpus/,Research
What's your favorite paper at ICLR2024? [D],Way too much to keep in track..,MachineLearning,52,9,1715651968.0,1crgu6y,Every-Act7282,https://www.reddit.com/r/MachineLearning/comments/1crgu6y/whats_your_favorite_paper_at_iclr2024_d/,Discussion
[D] Modern use-cases for RNNs?,"The discussion can be twofold.
1)What are in your opinion some tasks, for the personal projects scale, where you think RNNs close to traditional implementations (LSTM, GRU) are still the best starting and ending point? Especially compared to transformers.

In small time-series forecasting settings I can see a GRU being more convenient than a Transformer probably, but I am interested also in tasks where inputs are sequences of symbols or measures, but outputs maybe not.

The main goal is to play with LSTM and GRU variants (eg minGRU) on datasets where it makes sense, might do tiny-Shakespeare but it doesn't warm my heart...

2) do you think there are sequential tasks and settings where RNNs are not only the more natural option according to our intuition, but actually the only theoretically or experimentally available option to make do, compared to Transformers or 1D CNNs etc?",MachineLearning,52,20,1732958552.0,1h38ym2,Sad-Razzmatazz-5188,https://www.reddit.com/r/MachineLearning/comments/1h38ym2/d_modern_usecases_for_rnns/,Discussion
[D] Neurips 2024 Hotel Roommate Search,"The hotels around the venue for Neurips 2024 are pretty expensive, and I'm looking for a roommate to split the cost with (my university has a limit on the nightly hotel rate they are willing to reimburse). I currently have reserved a room for Tuesday-Sunday in the Century Plaza Hotel, which is 0.9 miles from the convention center. The nightly rate is $414. If anyone wants to split the cost of a room, please reach out! Also, it would be helpful if you could share this post with your research group or other attendees that you know.

If you are unsure about rooming with a complete stranger, you can get to know me a little bit through my personal website (https://mtcrawshaw.github.io/), which has links to my google scholar page, CV, etc. I do have a paper at the conference in the area of federated learning/distributed optimization. Just a grad student trying to make conferences affordable! Thanks.",MachineLearning,54,6,1731688658.0,1gs0gj8,ssbm_crawshaw,https://www.reddit.com/r/MachineLearning/comments/1gs0gj8/d_neurips_2024_hotel_roommate_search/,Discussion
[D] AAAI 2025 Phase 1 decision Leak?,"Has anyone checked the revisions section of AAAI submission and noticed that the paper has been moved to a folder ""Rejected\_Submission"". It should be visible under the Venueid tag. The twitter post that I learned this from:  
[https://x.com/balabala5201314/status/1843907285367828606](https://x.com/balabala5201314/status/1843907285367828606)",MachineLearning,52,227,1728697406.0,1g1plva,Wise_Witness_6116,https://www.reddit.com/r/MachineLearning/comments/1g1plva/d_aaai_2025_phase_1_decision_leak/,Discussion
[R] Diffusion Models are Evolutionary Algorithms,"[Abstract](https://arxiv.org/abs/2410.02543): In a convergence of machine learning and biology, we reveal that diffusion models are evolutionary algorithms. By considering evolution as a denoising process and reversed evolution as diffusion, we mathematically demonstrate that diffusion models inherently perform evolutionary algorithms, naturally encompassing selection, mutation, and reproductive isolation. Building on this equivalence, we propose the Diffusion Evolution method: an evolutionary algorithm utilizing iterative denoising -- as originally introduced in the context of diffusion models -- to heuristically refine solutions in parameter spaces. Unlike traditional approaches, Diffusion Evolution efficiently identifies multiple optimal solutions and outperforms prominent mainstream evolutionary algorithms. Furthermore, leveraging advanced concepts from diffusion models, namely latent space diffusion and accelerated sampling, we introduce Latent Space Diffusion Evolution, which finds solutions for evolutionary tasks in high-dimensional complex parameter space while significantly reducing computational steps. This parallel between diffusion and evolution not only bridges two different fields but also opens new avenues for mutual enhancement, raising questions about open-ended evolution and potentially utilizing non-Gaussian or discrete diffusion models in the context of Diffusion Evolution.",MachineLearning,53,21,1728424867.0,1fzbvq3,ptuls,https://www.reddit.com/r/MachineLearning/comments/1fzbvq3/r_diffusion_models_are_evolutionary_algorithms/,Research
[D] What is the point of encoder only models like bert and roberta anymore?,"I have been working with language models for a while now... Most tasks that I have been concerned with are related to translation, transliteration, spell correction and code mixing. So far I haven't found much reason to implement encoder only models such as bert, roberta etc. Everything that I want to achieve even from a number of parameter standpoint ends up going to seq2seq models like bart (50M) and marianMT (77M). From my observation all the tasks except for spell correction, seq2seq architectures are able to handle pretty well. Spell correction I'm speculating is difficult to do because of issues with subword tokenization. I'm curious to when should I be implementing encoder only models and in what applications is going to seq2seq overkill...

  
Edit: ok i feel stupid i totally forgot about sentiment analysis and text classification being a thing lol. great LLM shaming here tho guys didn't know 50M param models are LLMs can't wait to make me own chatgpt that's a thousand times smaller lol

but yeah anyway this discussion does inspire me to some tasks that I can train bert on. will share once i do",MachineLearning,52,53,1726154390.0,1ff54no,sadboiwithptsd,https://www.reddit.com/r/MachineLearning/comments/1ff54no/d_what_is_the_point_of_encoder_only_models_like/,Discussion
[P] I built a tool to minimize hallucinations with 1 hyperparameter search - Nomadic,"Github: [https://github.com/nomadic-ml/nomadic](https://github.com/nomadic-ml/nomadic)

Demo: [Colab notebook](https://colab.research.google.com/drive/1PVd1d_v3wHGLIJWNvUMnGDNkCd2s23PY) - Get the best-performing, statsig configurations for your Retrieval Augmented Generation pipeline and reduce hallucinations by 4X with one experiment. Note: Works best with Colab Pro (high-RAM instance) or running locally.

Curious to hear any of your thoughts / feedback!",MachineLearning,54,6,1725909588.0,1fcxup1,TRBeetle,https://www.reddit.com/r/MachineLearning/comments/1fcxup1/p_i_built_a_tool_to_minimize_hallucinations_with/,Project
[D] AI Search: The Bitter-er Lesson,,MachineLearning,52,39,1722826202.0,1ekd6fx,jsonathan,https://yellow-apartment-148.notion.site/AI-Search-The-Bitter-er-Lesson-44c11acd27294f4495c3de778cd09c8d,Discussion
[D] Benchmarking foundation models for time series,"# Introduction

We present a reproducible benchmark comparing different foundation time series models across a wide variety of models in a large scale dataset.

We conclude that [TimeGPT-1](https://arxiv.org/abs/2310.03589b) ranks first in terms of accuracy and speed inference compared to the latest foundation models, including [TimesFM](https://arxiv.org/pdf/2310.10688) (Google), [Chronos](https://arxiv.org/abs/2403.07815) (Amazon), [Moirai](https://arxiv.org/abs/2402.02592) (SalesForece), and [Lag-LLama](https://arxiv.org/pdf/2310.08278) (Service Now). TimeGPT-1 and TimesFM also outperform established statistical, machine learning, and deep-learning models, with comparable inference times to a SeasonalNaive. Chronos, Moirai and Lag-Llama still need some further improvements and can be outperformed by other classical methods.

This analysis spans over **30,000 unique time series** across various domains and frequencies from M-Competitions, Monash Repository, and Wikipedia page views, among others, robustly comparing these models.

# Empirical Evaluation

This study considers **over 30,000 unique time series** from the Monash Repository, M-Competitions, Wikipedia page views, among others, spanning various time series frequencies: Monthly, Weekly, Daily, and Hourly. Our evaluation compares five foundation models for time series data in terms of accuracy and inference times. We have also included comparisons to a large battery of statistical, machine learning, and deep-learning models, to provide a benchmark against traditional forecasting methods.

We include the following models in our comprehensive evaluation:

* [Statistical](https://github.com/Nixtla/statsforecast/): SeasonalNaive, HistoricAverage, ZeroModel, AutoARIMA, Prophet, AutoCES, AutoETS, Theta, DynamicOptimizedTheta, ADIDA, IMAPA, and CrostonClassic.
* [Machine Learning](https://github.com/Nixtla/mlforecast/): AutoLGBM.
* [Deep Learning](https://github.com/Nixtla/neuralforecast/): AutoTFT, AutoNHITS.
* Foundation: Chronos, Lag-Llama, Moirai, TimeGPT, TimeGPT (long horizon), and TimesFM.

# Results

TimeGPT-1 ranks first in terms of accuracy and speed inference compared to the latest foundation models, including TimesFM, Chronos, Moirai, and Lag-Llama. TimesFM by Google ranks second in accuracy and outperfoms TimeGPT-1 in inference speed. Amazon Chronos ranks third in accuracy but shows a significant drop in inference speed. Both Salesforces's and ServiceNow's models are far more efficient in terms of inference speed than Chronos, but they rank lower in terms of accuracy.

[Reproducible experiment](https://github.com/Nixtla/nixtla/tree/main/experiments/foundation-time-series-arena)

https://preview.redd.it/h374cfaube3d1.png?width=1798&format=png&auto=webp&s=a2b0853ef9b9ebefb8f5977bfe11ef14c89964aa

https://preview.redd.it/55qnz8mtbe3d1.png?width=2146&format=png&auto=webp&s=7878f778fec30fa562a0422de3dc2748a6538571

https://preview.redd.it/wrfxhuxuce3d1.png?width=2086&format=png&auto=webp&s=d6fc57495e5571d1d68871538514e25375e90d54",MachineLearning,54,13,1717002152.0,1d3h5fs,fedegarzar,https://www.reddit.com/r/MachineLearning/comments/1d3h5fs/d_benchmarking_foundation_models_for_time_series/,Discussion
[Research] Consistency LLMs: converting LLMs to parallel decoders accelerates inference 3.5x,"Hey all! We are here to share our latest work: consistency large language models (CLLMs), which is a new family of models capable of reducing inference latency by efficiently decoding 𝑛 tokens in parallel. Your new friends for LLM serving/local deployment with faster inference speed! 🔥 Please check our blog post for demo with 3.1x speedup:

[https://hao-ai-lab.github.io/blogs/cllm/](https://hao-ai-lab.github.io/blogs/cllm/)

Compared with existing fast decoding techniques, CLLMs achieve fast parallel decoding **without the need for**:

* Draft models
* Architectural modifications/auxiliary model components

This introduces a number of advantages for CLLMs:

* CLLMs don't have to deal with the complexity of obtaining 'good' draft models and managing two different models in a single system.
* CLLMs share the same architecture with target LLMs and require no additional engineering efforts when adopting the technique to different models.
* CLLMs can be integrated seamlessly with other techniques for efficient LLM inference (e.g. Lookahead Decoding) to achieve even more significant speedup.

This decoding method CLLMs use is called [Jacobi decoding](https://arxiv.org/abs/2305.10427), which improves inference efficiency in comparison with conventional auto-regressive decoding. CLLMs are trained with the objective of performing efficient Jacobi decoding by mapping any randomly initialized 𝑛-token sequence to the same result as AR decoding in as few steps as possible.

Experiment results have demonstrated the effectiveness of CLLMs, showing 2.4× to 3.4× improvements in generation speed on a variety of tasks.

[In comparison with Medusa2, CLLMs achieve comparable or better performance, but \*\*need no extra parameters or tree-style verification\*\*](https://preview.redd.it/fta39wapq9zc1.png?width=640&format=png&auto=webp&s=af39917a635c57a3d8886f8db3a952c0716909c9)

[CLLMs training objective visualization](https://preview.redd.it/nb7pulpqq9zc1.png?width=2560&format=png&auto=webp&s=d4b1ac36e92050579350bdae1e5225a9a5dc6387)

Please see [our paper](http://arxiv.org/abs/2403.00835) for more details. Feel free to try out [our codebase](https://github.com/hao-ai-lab/Consistency_LLM) and [CLLM checkpoints](https://huggingface.co/cllm)!

If you found our work interesting, please subscribe, like or repost, thanks! Learn more and engage with us on Twitter:

[https://x.com/haoailab/status/1788269848788869299](https://x.com/haoailab/status/1788269848788869299)",MachineLearning,54,3,1715202934.0,1cnfmec,No_Yogurtcloset_7050,https://www.reddit.com/r/MachineLearning/comments/1cnfmec/research_consistency_llms_converting_llms_to/,Research
[R] A Primer on the Inner Workings of Transformer-based Language Models,"**Authors**: Javier Ferrando (UPC), Gabriele Sarti (RUG), Arianna Bisazza (RUG), Marta Costa-jussà (Meta)

**Paper:** [https://arxiv.org/abs/2405.00208](https://arxiv.org/abs/2405.00208)

**Abstract:**

>The rapid progress of research aimed at interpreting the inner workings of advanced language models has highlighted a need for contextualizing the insights gained from years of work in this area. This primer provides a concise technical introduction to the current techniques used to interpret the inner workings of Transformer-based language models, focusing on the generative decoder-only architecture. We conclude by presenting a comprehensive overview of the known internal mechanisms implemented by these models, uncovering connections across popular approaches and active research directions in this area.



https://preview.redd.it/57y44wwdn6yc1.png?width=1486&format=png&auto=webp&s=7b7fb38a59f3819ce0d601140b1e031b98c17183

",MachineLearning,51,3,1714729598.0,1cj4o70,SubstantialDig6663,https://www.reddit.com/r/MachineLearning/comments/1cj4o70/r_a_primer_on_the_inner_workings_of/,Research
[D] How do you keep up with the literature?,"Pretty much what the title says. What tools/strategies do you use to keep up with the literature?


EDIT: for context, I am a first year PhD student and I was referring to the literature in the particular 'niche' (if you can call anything a niche in ML, apart from a very few exceptions)",MachineLearning,48,25,1733800829.0,1hasdlo,Rickmaster7,https://www.reddit.com/r/MachineLearning/comments/1hasdlo/d_how_do_you_keep_up_with_the_literature/,Discussion
[R] For a change of topic: some nonLLM focused work of mine: Bias-Free Sentiment Analysis through Semantic Blinding and Graph Neural Networks,"In my academic field (social sciences) I deal with the problem of bias in SA models. My previous work showed that deep learning SA systems inherit bias (e.g. nonrepresentative of the population political bias) from annotators: 

https://arxiv.org/abs/2407.13891

Now I devised a solution that used a technique I call semantic blinding to provide only the bare necessary information for the model to predict emotions in text, leaving no signal for the model to overfit and produce bias from:

https://arxiv.org/abs/2411.12493

Interested to hear your thoughts before I publish the SProp Gnn.

Do you think it could be useful beyond the academia?



",MachineLearning,51,10,1733552507.0,1h8meas,Hub_Pli,https://i.redd.it/vh80i11ndd5e1.jpeg,Research
[D] AISTATS 2025 reviews,Aistats 2025 reviews are supposed to be out today. So I thought to create a discussion post for the same where we can share our experiences!,MachineLearning,54,139,1732685475.0,1h0x428,PhoneImpressive9983,https://www.reddit.com/r/MachineLearning/comments/1h0x428/d_aistats_2025_reviews/,Discussion
[D] ICLR 2025 Paper Reviews,"Reviews for ICLR 2025 seem to be available on OpenReview. Feel free to celebrate/rant/complain about your reviews here!

Last year's statistics [here](https://papercopilot.com/statistics/iclr-statistics/iclr-2024-statistics/)",MachineLearning,47,34,1731489057.0,1gq8vu6,pie3636,https://www.reddit.com/r/MachineLearning/comments/1gq8vu6/d_iclr_2025_paper_reviews/,Discussion
[P] Understanding Multimodal LLMs: The Main Techniques and Latest Models,,MachineLearning,51,8,1730644470.0,1gio7ap,seraschka,https://sebastianraschka.com/blog/2024/understanding-multimodal-llms.html,Project
"[D] Sensitivity Analysis of the ML Paper Got Better Results, What Now?","I wrote an ML paper using a novel approach on a specific dataset, which yielded some positive results. I trained several models, evaluated them, and conducted extensive interpretation and discussion based on the findings. One of the reviewers requested a sensitivity analysis on a few preprocessing parameters/algorithms. Interestingly, one of the changes resulted in slightly better outcomes than my original approach.

My question is: what are the expectations in this case? Do I need to rewrite the entire paper, or should I simply report this observation in the sensitivity analysis? While it’s nice that the changes improved the results, it’s pretty frustrating to think about rewriting much of the interpretation (e.g., feature importance, graphs, discussion, etc.) based on the new run. What are your thoughts and experiences?",MachineLearning,53,15,1728257822.0,1fxu3zw,anagreement,https://www.reddit.com/r/MachineLearning/comments/1fxu3zw/d_sensitivity_analysis_of_the_ml_paper_got_better/,Discussion
[D] Llama3.2-1B GGUF Quantization Benchmark Results,"I benchmarked Llama 3.2-1B GGUF quantizations to find the best balance between speed and accuracy using the IFEval dataset. **Why did I choose IFEval?** It’s a great benchmark for testing how well LLMs follow instructions, which is key for most real-world use cases like chat, QA, and summarization.

**1st chart** shows how different GGUF quantizations performed based on IFEval scores.

https://preview.redd.it/b580liydnerd1.png?width=692&format=png&auto=webp&s=0b9a1b0e7af0004f25604d3634a615f2e6326d20

**2nd chart** illustrates the trade-off between file size and performance. Surprisingly, q3\_K\_M takes up much less space (faster) but maintains similar levels of accuracy as fp16.

https://preview.redd.it/6tkr76venerd1.png?width=866&format=png&auto=webp&s=7dd90f1a82e4d222bcd4bd4475cb0b8720b8a5d1

https://preview.redd.it/zvmr0asgnerd1.png?width=1510&format=png&auto=webp&s=2630cae9bac659591d714216b71d9ce87ea68222

Full data is available here: [nexaai.com/benchmark/llama3.2-1b](https://nexaai.com/benchmark/llama3.2-1b)  
​Quantization models downloaded from [ollama.com/library/llama3.2](https://ollama.com/library/llama3.2)  
​Backend: [github.com/NexaAI/nexa-sdk](https://github.com/NexaAI/nexa-sdk) (SDK will support benchmark/evaluation soon!)

**What’s Next?**

* Should I benchmark Llama 3.2-3B next?
* Benchmark different quantization method like AWQ?
* Suggestions to improve this benchmark are welcome!

Let me know your thoughts!",MachineLearning,51,8,1727466076.0,1fqw88t,AlanzhuLy,https://www.reddit.com/r/MachineLearning/comments/1fqw88t/d_llama321b_gguf_quantization_benchmark_results/,Discussion
[D] AI PhD advice - top university required?,"
Hey guys - looking for advice. I’m a PhD student in AI at Oregon State University. I am particularly interested in using genAI to acts as a clinical decision aid for doctors using a rag pipeline - obviously much more complicated than I’ve just explained. 

My question is: for the most part I’ve seen on similar servers that you have to attend a top institution to be able to go on to a prestigious company, etc. 

Is it going to hinder my chances of getting a job because I’m not attending a top 10 university? Oregon State is ranked 53rd according to csrankings.org. 

Any advice/comments are much appreciated ",MachineLearning,50,46,1725063358.0,1f5ackj,frankies_wrld,https://www.reddit.com/r/MachineLearning/comments/1f5ackj/d_ai_phd_advice_top_university_required/,Discussion
"[P] supertree - interactive visualization of decision trees (sklearn, xgboost, lightgbm)","Hi All,

I would like to share with you a new Python package for interactive decision tree visualization. It is called `supertree`. It visualizes decision tree as interactive graph, where you can collapse and expand selected nodes. You can zoom and pan though large trees. It works with Scikit-learn, Xgboost, and LightGBM.

The package works in notebooks: Jupyter Lab, Jupyter Notebook, Google Colab. You can also use it in Python scripts and save output trees to HTML. 

The package is available on pip: `pip install supertree`.

You can find code examples on the GitHub: https://github.com/mljar/supertree

Happy exploring!",MachineLearning,50,6,1724774759.0,1f2ku3i,pp314159,https://www.reddit.com/r/MachineLearning/comments/1f2ku3i/p_supertree_interactive_visualization_of_decision/,Project
Anyone Actually Using Synthetic Data in ML? How Did It Impact Your Projects? [Discussion],"I am curious about the real-world applications of synthetic data that you’ve actually used in your machine learning projects.

Did it genuinely enhance your process or results?
What were the biggest challenges you faced while using it?

I’d love to hear your experiences—both the good and the bad.",MachineLearning,49,47,1724592457.0,1f0wbfy,Value-Forsaken,https://www.reddit.com/r/MachineLearning/comments/1f0wbfy/anyone_actually_using_synthetic_data_in_ml_how/,Discussion
[R] Exploring the Limitations of Kolmogorov-Arnold Networks in Classification: Insights to Software Training and Hardware Implementation,"**TL;DR**: MLP massively outperforms KAN in terms of training compute efficiency

**Paper:** [https://arxiv.org/pdf/2407.17790](https://arxiv.org/pdf/2407.17790)

**Abstract:**

>Kolmogorov-Arnold Networks (KANs), a novel type of neural network, have recently gained popularity and attention due to the ability to substitute multi-layer perceptions (MLPs) in artificial intelligence (AI) with higher accuracy and interoperability. However, KAN assessment is still limited and cannot provide an in-depth analysis of a specific domain. Furthermore, no study has been conducted on the implementation of KANs in hardware design, which would directly demonstrate whether KANs are truly superior to MLPs in practical applications. As a result, in this paper, we focus on verifying KANs for classification issues, which are a common but significant topic in AI using four different types of datasets. Furthermore, the corresponding hardware implementation is considered using the Vitis high-level synthesis (HLS) tool. To the best of our knowledge, this is the first article to implement hardware for KAN. The results indicate that KANs cannot achieve more accuracy than MLPs in high complex datasets while utilizing substantially higher hardware resources. Therefore, MLP remains an effective approach for achieving accuracy and efficiency in software and hardware implementation.

**Highlights:**

>Except for the Dry Bean dataset’s training time, all three of the other datasets always show that KANs require substantially longer training times than MLPs, ranging from 6.55 times (151.4 vs 23.1 s) to 36.68 times (198.1 vs 5.4 s). \[...\] Except for the Wine dataset, MLPs continually have faster loss reduction and lower loss values than KANs. Overall, regarding training time and loss reduction, KANs are not better than MLPs.

>In general, KANs fail to demonstrate higher accuracy than MLPs, and the symbolic formula representation of KANs performs even worse than MLPs in classification challenges. Furthermore, KANs also need a substantial amount of time and effort from developers to create symbolic formulas in the final stages.

\[Specialized hardware tests:\]

>These results indicate that implementing symbolic formulas on hardware requires much more hardware resources compared to normal matrix multiplication in MLPs. Moreover, when the size of KAN’s models increases, there is a corresponding rise in hardware resources required.

**Visual highlights:**

[GPU training](https://preview.redd.it/s2wyogl8hmhd1.png?width=685&format=png&auto=webp&s=8ee88df9850ec377f54432d9722870dc2eac7305)

[Difference in loss in favor of MLP can be enormous. Fast KAN convergence for Wine dataset is notable though. The dataset has only 178 examples with 13 features each](https://preview.redd.it/0q05z4gchmhd1.png?width=1237&format=png&auto=webp&s=98b30c5abe9ee793b4efc2177ac2a5d2946ca4e4)

[GPU training](https://preview.redd.it/hdo6eel9imhd1.png?width=1251&format=png&auto=webp&s=688683b26a538173d30da124392ac7ffffb04e31)

[FPGA training](https://preview.redd.it/27moc6reimhd1.png?width=1251&format=png&auto=webp&s=4ef5a2d1f78118a6672646979d6e87ebfd14d48d)

**Code:** [https://github.com/Zeusss9/KAN\_Analysis](https://github.com/Zeusss9/KAN_Analysis)  
",MachineLearning,51,12,1723202865.0,1enx9be,StartledWatermelon,https://www.reddit.com/r/MachineLearning/comments/1enx9be/r_exploring_the_limitations_of_kolmogorovarnold/,Research
"[D] How ""normal"" is my ML Engineer job?","Hello everyone, I hope this post doesn't break any rule.

I have a master's degree in CS (ML-focused) and I started working at a startup as a ML Engineer about 1 month ago. The company has a very interesting ML core product (on which they research/experiment quite a bit), but it also has a small ""side project"" for a single client which is far less exciting. It basically consists of a simple web app doing some ML operations in the background, mainly by querying OpenAI's APIs and by using other pre-built models that aren't modified in any way. Now, the person who developed this app is leaving and I'll have to pick up the project.. and I am a little pissed about it, because this isn't really what I expected when coming here.

Considering that this is my first job ever (never even did an internship before), how ""normal"" would you consider working on such a product? Is my position actually just prompt-engineering in disguise?",MachineLearning,53,42,1720862201.0,1e266li,Fursol,https://www.reddit.com/r/MachineLearning/comments/1e266li/d_how_normal_is_my_ml_engineer_job/,Discussion
[D] Is speech research hitting a wall? What could be the next big thing for speech applications?,"I feel like, compared to NLP (safety/alignments in LLMs, etc.) and CV (text-to-image/video, robotic planning, etc., just name a few), the promise of speech applications seems a bit less exciting.

It seems like its main job is just to transcribe speech to text and let the LLMs do the magic things.

Of course, it can synthesize realistic voices (e.g., demo in GPT-4o) or other applications, but it seems speech is hardly able to be the key role again in the future.

In terms of possible research directions, what could be the next big thing for speech applications?",MachineLearning,47,62,1718090984.0,1dd8cbs,xiikjuy,https://www.reddit.com/r/MachineLearning/comments/1dd8cbs/d_is_speech_research_hitting_a_wall_what_could_be/,Discussion
[P] DeTikZify: Synthesizing Graphics Programs for Scientific Figures and Sketches with TikZ,,MachineLearning,49,3,1717226446.0,1d5hh0s,DrCracket,https://www.youtube.com/watch?v=hDbTSi0NtBA,Project
"[D] Where does the term ""feature"" come from?","Maybe a stupid trivia question, but I can't figure it out. ML calls features features, stats calls features predictors, math calls features variables, engineering calls features variables too.

I know what they are, but WHY do we call them features? Does anyone know the origin story?

**EDIT:** You all gave me some good leads; I think I have found a plausible answer: It likely comes from cognitive psychology. The [paper introducing the perceptron](https://psycnet.apa.org/record/1959-09865-001) (arguably the first step towards neural networks) refers to inputs as stimuli, but also notes that encoding stimuli into a small set of robust features helps performance:

>As the number of responses in the system increases, the performance becomes progressively poorer, if every response is made mutually exclusive of all alternatives. One method of avoiding this deterioration (described in detail in Rosenblatt, 15) is through the binary coding of responses. In this case, i**nstead of representing 100 different stimulus patterns by 100 distinct, mutually exclusive responses, a limited number of discriminating features is found, each of which can be independently recognized as being present or absent**, and consequently can be represented by a single pair of mutually exclusive responses.

(highlighting is mine)

And later it concludes

>The performance of the system can be improved by the use of a contour-sensitive projection area, and by the use of a binary response system, in which **each response, or ""bit,"" corresponds to some independent feature or attribute of the stimulus**.

(highlighting mine)",MachineLearning,52,19,1715276121.0,1co2ye4,FirefoxMetzger,https://www.reddit.com/r/MachineLearning/comments/1co2ye4/d_where_does_the_term_feature_come_from/,Discussion
[D] ECCV 2024 Review Discussion,"I thought that, like with other conferences, that we might have a discussion for people submitting to ECCV, as the reviews will be out in 10 hours (10pm CEST (center european summer time)). It is my first time submitting anywhere and I am very nervous tbh.",MachineLearning,50,79,1715248849.0,1cntiks,mr_birrd,https://www.reddit.com/r/MachineLearning/comments/1cntiks/d_eccv_2024_review_discussion/,Discussion
[D] AAAI 2025 Phase 2 Decision,"When would the phase 2 decision come out?  
I know the date is December 9th, but would there be chances for the result to come out earlier than the announced date?  
or did it open the result at exact time in previous years? (i.e., 2024, 2023, 2022 ....)

Kinda make me sick to keep waiting.",MachineLearning,49,284,1733545650.0,1h8kkjv,No-Style-7975,https://www.reddit.com/r/MachineLearning/comments/1h8kkjv/d_aaai_2025_phase_2_decision/,Discussion
[D] PhD in RL/ML Theory or LLM,"Hi guys,

I'm at a crossroads in my academic journey and would appreciate the community's insights. I'm trying to decide between pursuing a PhD focused on reinforcement learning/ML theory versus specializing in large language models with more experimental/applied research (these are the only two offers I had).

# Key considerations are the following:

# Research Impact

* RL/ML Theory: Foundational work that could advance the field's mathematical understanding
* LLMs: Direct applications in today's most transformative AI systems

# Job Prospects

* Theory: Academia, research labs, potentially more limited industry roles
* LLMs: High industry demand, active research area in both academia and industry

# Long-term Relevance

* Theory: Core principles likely to remain valuable regardless of specific technologies
* LLMs: Currently revolutionary but uncertain long-term trajectory

Personal background

* I'm an international student and about to finish my master program in US, so I no longer has enough time before making the final decision. I used to research in ml theory, but did not end up with a real top conference publication in theory. I personally doubt if I have enough mathematical background to pursue a successful PhD in this area (e.g., at least publish 2 theory papers a year on ICML/NeurIPS/ICLR/COLT/AISTATS). At the same time, I am personally doubting if theory works indeed advance the ML/AI community, as many papers are just proving vacuous bounds or propose some new algorithms that themselves cannot even implement or experimentally tested.
* I also used to research in more applied ml, with one aaai paper. My personal concerns is that I'm not fast at implementation and coding, the most strategic ability for a successful applied ML researcher. After we entered the LLM era, the pacing or applied ML research (especially in LLM and CV) becomes so fast. It's like competitive programming in research community (well, also the #GPUs competition).",MachineLearning,49,25,1732129331.0,1gvx8vx,Living_Imagination84,https://www.reddit.com/r/MachineLearning/comments/1gvx8vx/d_phd_in_rlml_theory_or_llm/,Discussion
[D] Together AI hits $100M in ARR but it just resales compute - hype? ,"I recently learned that this startup is seen as the fastest revenue ramp in recent years. But they are literally just brokering GPUs from one provider to another and just slapping on a broker fee…

If a real estate agent sales $100 million worth of houses, and get a $100,000 commission, it doesn’t mean they made $100 million in revenue…  what am I missing here? 

The product is literally the same, just ssh to a cluster. 

Why are people paying for this? this sounds like a massive scam no? Shouldn’t this just be compared to a cloud provider like Coreweave instead of an AI company? If you own GPUs as a cloud, you crushed $100M in ARR in a few months… ",MachineLearning,53,25,1731437561.0,1gps8fl,guardianz42,https://www.reddit.com/r/MachineLearning/comments/1gps8fl/d_together_ai_hits_100m_in_arr_but_it_just/,Discussion
[D] Responses to false accusations of plagiarism for Gaunt Tensor Product paper,"I’m posting this on behalf of the authors of the paper. The first author tried to make a post about this, but the post got removed for some reason. The author reached out to me because I was one of the people defending them, so see below for the author writeup about the accusations.

**TL;DR**: We're the authors of the Gaunt Tensor Product paper, and we want to directly address the false plagiarism accusations against our work. Our main contribution, a new perspective on tensor products of irreducible representations (irreps) in machine learning and equivariant neural networks, is novel and original. The claimed ""similarity"" are actually algorithms from elementary math and CS courses, and are not the main contribution of our work: our independent implementation is clear if you look at our code, which is quite different because we had a completely different application area in mind. On the other hand, our core contributions, including establishing the connection between tensor products of irreps and integrals of products of spherical harmonics and various design paradigms of equivariant operations, are completely omitted. There is an oversight of citation due to the gap between fields (machine learning vs. graphics), but this is not plagiarism, and now that we know about this, we are updating the paper with the citation and discussion accordingly. This is similar situation to areas such as neural ODEs, where the original ideas were in engineering papers in the '90s, and not cited in ML papers (including the 2018 NeurIPS paper) until much later. The anonymous accuser is selectively replying, omitting key details, and controlling the narrative.

**More details below**:

We are the authors of \[""Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt Tensor Products""\](https://openreview.net/forum?id=mhyQXJ6JsK) . We are creating a new post to clearly outline our responses to the false accusations of plagiarism that we've received for the Gaunt Tensor Product paper in another thread. While we have replied on that thread, the anonymous OP of that thread is selectively replying and omitting a lot of information from our responses, and we don't think it is fair that they single-handedly control the narrative. Note that we never got any emails or posts on OpenReview from the author, who has instead decided to anonymously post on here.

Firstly, we would like to comprehensively respond to the false accusations again:

\- **The contributions of our work**: as emphasized in our paper, our main contribution in this work is the new perspective on tensor products of irreps, which is novel and original to the machine learning community (Equations 3 and 4). The whole Section 3.1 elaborates on how to establish the connection between tensor products of irreps and integrals of products of spherical harmonics. Although the OP claims ""However, it is important to note that this derivation accounts for less than one page of the nine-page paper."", the fact is that our establishment and derivation are based on a series of rigorous deductions with many efforts on building a solid mathematical foundation including group theory and quantum mechanics (please refer to Appendix A.1-A.7, page 16-28), which is not straightforward and trivial to obtain. Without these efforts, we cannot establish such connections, let alone the efficient algorithm. In the context of equivariant machine learning, this derivation presents significance to refresh the understanding of basic equivariant operations, which cannot be omitted.

\- **The similarities of the efficient algorithms between our work and FSHP work**: Firstly, we would like to apologize that we did not cite the FSHP work in our submission, which is unintentional and due to the gap between these two communities (we are from the ML community, and they are from graphics, and the paper was not known to us until recently). We will update the arXiv version of our paper asap by adding a discussion paragraph to carefully discuss the FSHP work and our work. **On the other hand, we also would like to clarify that there does not exist any plagiarism behavior of the FFT algorithm**: after we figure out the relation between the tensor product of irreps and integrals of the product of three spherical harmonics, it is rather natural to connect it with products of spherical functions. Moreover, there exist classical results for efficient computation of products of spherical functions, i.e., Convolution Theorem and FFT, which involve elementary knowledge that can be learned in several undergraduate classes: (1) change of basis, which can be learned in linear algebra and signal processing and is used in both paper to connect spherical harmonics and Fourier basis; (2) FFT, which is commonly taught in signal processing and numerical computation classes and is used for acceleration. Due to the basicness of these mathematical tools, both works follow the standard way to formalize and present, which leads to similarity. As we said, this cannot be misrepresented as plagiarism because we independently worked on this, and did not know about the other work until later because of the different communities. This is similar to work in areas such as neural ODEs, where the original ideas were in engineering papers in the '90s, and not cited in ML papers (including the 2018 NeurIPS paper) until much later.

\- **The differences in implementation**: it is noteworthy that, as a work for the equivariant machine learning community, it is not enough to simply propose an approach just for the tensor product operation. What we really care about is the various design paradigms of equivariant operations, which are built upon tensor products. In Section 3.3, we categorize these paradigms into three classes in terms of their different characteristics and applied range. For each class of equivariant operations, we carefully specialize our approach by combining their properties and considering the restrictions. For example, for the equivariant convolution, we figure out that we can further leverage eSCN/EquiformerV2's findings to achieve further acceleration; for the equivariant many-body interactions, a divide-and-conquer approach is natural, which is also generally taught in various CS courses and projects. There also exist different instantiation strategies in modern equivariant networks when applying these classes of operations, please refer to the Discussion paragraph in Section 3.3 and Appendix C. Simply proposing the efficient approach for tensor products is not feasible to these mentioned points. Without these additional efforts and contributions, the efficient algorithm is not practical to be used for the equivariant machine learning community,  which cannot be omitted.

\- There is quite a lot of literature in the last few decades in the graphics community on this, and this is another general point is that work on the graphics community on efficient algorithms is not heard of and/or undercited in the rotationally equivariant neural networks community, when these algorithms pop up in a lot of equivariant NN work. Additionally, this graphics paper is not in the field of ML, and this algorithm is being applied to a completely different area, which is why we did not see it originally and had an independent formalization. Perhaps an analogy here is that there are papers applying Transformers to different areas like vision instead of language, but this shouldn't be ""plagiarism"" at all. Likewise, neural ODEs shouldn't be considered plagiarism of traditional ODE solvers simply because they are using the same method (and indeed, some of the original ideas of neural ODEs were in engineering papers from the '90s, and not discovered/cited in ML papers until later because of the different communities). One user on this thread also put it well that the concepts here like FFT are quite well-known: ""After skimming, my impression is that those are well known results from textbooks and signal processing courses that nobody bother to cite anymore. I could be wrong.""

\- The implementation in the GTP paper is fairly different from the FSHP paper and was implemented independently because we derived our implementation based on being motivated by our specific application area of ML for molecular modeling: their code is in C++, doesn't support efficient computations for lower rotation orders (L), and is not made for use with irreducible representations. This should be clear when you see the code.

\- The main purpose of the Equiformerv2 experiment with the self-mix layer was a proof-of-concept to show that such a self-mix layer can be implemented because of the Gaunt Tensor Product formulation. Without this formulation (and using the more standard Clebsch-Gordan Tensor Product), it would have been very slow to add this layer (and not great from a memory usage perspective). This can be made more clear in the arXiv version.

Secondly, we would like to point out that the anonymous OP of that thread is selectively replying to posts, and omitting a lot of information (including in how they are updating their own thread, they do not include all of the details of our responses). To us, the posts also seem LLM generated but you should draw your own conclusions. We also posted this new topic because the authors responses on the original thread are all folded, which cannot be directly seen by new readers.

Finally, we appreciate that many people have been commenting on the thread to defend us. These types of anonymous, sensational claims can have serious implications and to post anonymously on Reddit before emailing us or posting on OpenReview is really problematic. We hope that you all read these threads carefully before jumping to conclusions.",MachineLearning,50,3,1729654266.0,1ga12d8,kronicler1029,https://www.reddit.com/r/MachineLearning/comments/1ga12d8/d_responses_to_false_accusations_of_plagiarism/,Discussion
[D] Future of Multi-Armed Bandits?,"I am just beginning my MAB research, but my original research interest lies in either Deep-RL or Optimization. Unfortunately, I found there are very few intersections between DRL/Opt and MAB (Please tell me if I am wrong), or even say that MAB is a kind of 'isolated' research field, neither produces interesting things (like transformers->GPT ) nor give you a promising career prospect (like opt favored by hedge fund). The most notable application is on recommend system, but I dont really see company other than Netflix would recruit, and not suitable for quant path.

Is MAB research dying? Should I quit MAB if I am no longer interested in academia and just want to become a quant after graduation? Or are there any intersection between MAB and other direction such I could become a paper machine?",MachineLearning,53,30,1729420518.0,1g7vss8,petrichorinforest,https://www.reddit.com/r/MachineLearning/comments/1g7vss8/d_future_of_multiarmed_bandits/,Discussion
[D] Has Meta cancelled it's AI Residency program for this year?,"As the title suggests, meta opened it residency for 2023 but for some reason decided not to for this year. Anyone knows why?",MachineLearning,50,8,1723653498.0,1es6c98,Interesting-Weeb-699,https://www.reddit.com/r/MachineLearning/comments/1es6c98/d_has_meta_cancelled_its_ai_residency_program_for/,Discussion
"[R], [P] RPC — A New Way to Build Language Models","Article: [RPC — A New Way to Build Language Models](https://medium.com/@jpmag7/rpc-language-modeling-by-relevant-precedence-compression-3d09bb4f23e6)

One of the reasons I really like software engineering in general is because anyone can do almost anything with just a computer. But when it comes to Al and specifically LLMs you need a tone of resources and money to do anything interesting by yourself.

So recently I've been trying to find a way to build language models with far less training data and far less compute. RPC is my closest attempt at that. It compresses the prompt into a vector representation and then performs a search in a vector database to find the most appropriate next token. It works remarkably well.

I'm sharing this with the community, in the hope that someone will give some feedback or even try to replicate it. I'd love for you to take a look at the article and share some thoughts here.",MachineLearning,51,10,1722644403.0,1eipn2t,someuserwithwifi,https://www.reddit.com/r/MachineLearning/comments/1eipn2t/r_p_rpc_a_new_way_to_build_language_models/,Research
"[D] Machine Learning Engineers, what portion of your work is focused on deployment pipelines vs. model building/tuning?","I’m currently a machine learning engineer, but I focus much more heavily on the pipelines in a way that is similar to when I was a data engineer. I’d love to get more into the model building side of things, but my model knowledge has gotten a bit rusty since I finished my M.S. in Statistics.

What portion of your day to day work is focused on deploying compared to model building?",MachineLearning,47,43,1715967148.0,1cub74b,RawCS,https://www.reddit.com/r/MachineLearning/comments/1cub74b/d_machine_learning_engineers_what_portion_of_your/,Discussion
[P] LeRobot: Hugging Face's library for real-world robotics,"Meet [LeRobot](https://github.com/huggingface/lerobot), a library hosting state-of-the-art deep learning for robotics.

The next step of AI development is its application to our physical world. Thus, we are building a community-driven effort around AI for robotics, and it's open to everyone!  
Take a look at the code: [https://github.com/huggingface/lerobot](https://github.com/huggingface/lerobot)

https://preview.redd.it/ugf4l8lfgryc1.png?width=3794&format=png&auto=webp&s=222825e897ba48eb07acedffb0662d5794af04e8

LeRobot is to robotics what the Transformers library is to NLP. It offers clean implementations of advanced AI models with pre-trained checkpoints. We also reimplemented 31 datasets from academia, and some simulation environments, allowing to get started without a physical robot.

https://preview.redd.it/1d8qnrpggryc1.png?width=2563&format=png&auto=webp&s=3e0d6c10440c9d0970b867813d3a210dd9a7bab9

Additionally, the same models can be trained on real-world datasets. Here is a cool data visualization with [rerun.io](http://rerun.io/) which is fully integrated with our video format optimized for training. The data originally comes from the [Aloha project](http://tonyzhaozh.github.io/aloha).  
[\[LINK TO VIDEO\]](http://remicadene.com/assets/videos/battery-720p.mov)

https://preview.redd.it/86ihkcwhgryc1.png?width=2506&format=png&auto=webp&s=4f2ca7522a012d00d7327d90335d069dd099a321

Another visualization with LeRobot, this time on [Mobile Aloha](http://mobile-aloha.github.io/) data, to learn navigation and manipulation totally end-to-end. Both datasets have been collected on [trossenrobotics](https://www.trossenrobotics.com/) robot arms. [\[LINK TO VIDEO\]](http://remicadene.com/assets/videos/wipe-420p.mov)

https://preview.redd.it/qqtncqligryc1.png?width=1900&format=png&auto=webp&s=4f83c675b5c6f9dbded4b5b90a7a1c9f531c4086

LeRobot codebase has been validated by replicating state-of-the-art results in simulations. For example, here is the famous ACT policy which has been retrained and made available as a pretrained checkpoint:  
[\[LINK TO HF HUB\]](http://huggingface.co/lerobot/act_aloha_sim_transfer_cube_human/blob/main/README.md)

LeRobot also features the [Diffusion Policy](http://diffusion-policy.cs.columbia.edu/), a powerful imitation learning algorithm, and [TDMPC](http://yunhaifeng.com/FOWM), a reinforcement learning method that includes a world model, continuously learning from its interactions with the environment.

https://preview.redd.it/br9ibrylgryc1.png?width=1684&format=png&auto=webp&s=8e5595f1dff5381e5f60c6776126f48187ec58d9

Come join our [Discord channel](http://discord.com/invite/s3KuuzsPFb). We are building a diverse community from various backgrounds, software and hardware, to develop the next generation of smart robots in the real-world!

Thanks to the AI and robotics community without whom LeRobot won't have been possible.",MachineLearning,52,1,1714981723.0,1cldfy2,Tamazy,https://www.reddit.com/r/MachineLearning/comments/1cldfy2/p_lerobot_hugging_faces_library_for_realworld/,Project
[P] I made a TikTok Brain Rot video generator ,"I made a simple brain rot generator that could generate videos based off a single Reddit URL.

Tldr: Turns out it was not easy to make it.

To put it simply, the main idea that got this super difficult was the alignment between the text and audio aka Force Alignment. So, in this project, Wav2vec2 was used for audio extraction. Then, it uses a frame-wise label probability from the audio , creating a trellix matrix which represents the probability of labels aligned per time before using a most likely path from trellis matrix (backtracking algo).

This could genuinely not be done without Motu Hira's tutorial on force alignment which I had followed and learnt. Note that the math in this is rather heavy:

[https://pytorch.org/audio/main/tutorials/forced\_alignment\_tutorial.html](https://pytorch.org/audio/main/tutorials/forced_alignment_tutorial.html)

Example:

[https://www.youtube.com/shorts/CRhbay8YvBg](https://www.youtube.com/shorts/CRhbay8YvBg)

Here is the github repo: (please star the repo if you’re interested in it 🙏)

[https://github.com/harvestingmoon/OBrainRot?tab=readme-ov-file](https://github.com/harvestingmoon/OBrainRot?tab=readme-ov-file)

Any suggestions are welcome as always :)",MachineLearning,51,11,1735055849.0,1hlgdyw,notrealDirect,https://www.reddit.com/r/MachineLearning/comments/1hlgdyw/p_i_made_a_tiktok_brain_rot_video_generator/,Project
[D] Am I a complete idiot for signing up for a Hackathon?,"Ok, so I am a Coms Science graduate student and my chosen area of study is Ethical AI.

I wanted to attend this AI conference very badly because there are some speakers that I admire. But I couldn’t afford the passes, so I decided to apply to be in the student Hackathon because if accepted, you got a free pass.

It was such a Hail Mary for me to even do the application, but I thought it would also be a cool opportunity to learn alongside others.

I got accepted… and I’m extremely excited. But now I’m like, oh wait, am I going to royally piss off whomever my teammates are because I can’t code?

Any advice? There’s a preparatory webinar happening in a week, and I’ve been doing some overview classes so that I can learn the terminology/basics. The application also asked for me to state my level of coding experience and I checked: none. And still got accepted… so I’m hoping that the organizers consider me to still have something valuable to contribute?

Please let me know what you think 🥲",MachineLearning,49,71,1732589060.0,1h01hfn,sydj_k941,https://www.reddit.com/r/MachineLearning/comments/1h01hfn/d_am_i_a_complete_idiot_for_signing_up_for_a/,Discussion
[P] I Applied My Own ViT-Masked Autoencoder Implementation To Minecraft Images!,"[Image Fed To Trained Autoencoder](https://preview.redd.it/lpe76ujc9hmd1.png?width=1036&format=png&auto=webp&s=a00435dfb953bb003552fa3840c73d402054ceb0)

[Decoder Output Image, with somewhat detailed furnace flames!](https://preview.redd.it/wjr7kktc9hmd1.png?width=1036&format=png&auto=webp&s=c2ca91d79e119c83fd3899fbc34b407d9c6337a3)

Implementation Here: [https://github.com/akmayer/ViTMaskedAutoencoder/](https://github.com/akmayer/ViTMaskedAutoencoder/)

This only implemented the unsupervised masking and autoencoding/decoding. I originally had plans to do some final classification steps (cows vs pigs vs chickens?) but got lazy and this is certainly the flashier part to show off.

Thank you so much u/fferflo for developing Einx, it makes self attention, handling images in vision transformers, and anything where I have a higher than rank 3 tensors very convenient to handle.",MachineLearning,49,19,1725318449.0,1f7ks9p,Yelbuzz,https://www.reddit.com/r/MachineLearning/comments/1f7ks9p/p_i_applied_my_own_vitmasked_autoencoder/,Project
[D] Neurips'24 review release time?,"Does anyone know when the reviews will be released? The NeurIPS website states that the rebuttal starts on July 30th anywhere on Earth, and it’s already July 30th in our time zone!",MachineLearning,52,93,1722265590.0,1ef1var,Working-Egg-3424,https://www.reddit.com/r/MachineLearning/comments/1ef1var/d_neurips24_review_release_time/,Discussion
[R] Graph Vision: A python library to create segment mappings.,,MachineLearning,50,5,1720971745.0,1e35cs4,Kian5658,https://www.reddit.com/gallery/1e35cs4,Research
[D] Is Anyone Else Setting Up Real-Time Django Workers for their AI Application? What's the best way to do it scalably? 🙄  Celery + Channels + Redis + Docker ,"We completely underestimated this one tbh, thought it would be much more straight forward. But we've done it now and documented how step by step [in this article series](https://medium.com/p/5828a1ea43a3).

A bit of context, we're building a mini free AI Agent that auto-generates manually customisable plots, so the user can basically style however they want. It needs to be cost effective and efficient, so we thought about how to do it and tested a couple other ways.

https://preview.redd.it/cmly0a6bhwbd1.png?width=640&format=png&auto=webp&s=be1f5b2853e744adcaf8013e6d43b43f6be89617

We plan on releasing the project open source, so all feedback welcome! Is anyone else doing this and has any feedback? or do know of a better way to do it?",MachineLearning,49,26,1720708405.0,1e0qens,stoicwolfie,https://www.reddit.com/r/MachineLearning/comments/1e0qens/d_is_anyone_else_setting_up_realtime_django/,Discussion
[D] Was Fractal Net Ever Expanded Upon?,"I've been reading [""FractalNet: Ultra-Deep Neural Networks without Residuals""](https://arxiv.org/abs/1605.07648) and I was wondering if the methodology behind FractalNet was ever improved in other article.",MachineLearning,47,12,1716816500.0,1d1ring,research_pie,https://www.reddit.com/r/MachineLearning/comments/1d1ring/d_was_fractal_net_ever_expanded_upon/,Discussion
[P] Identify toxic underwater air bubbles lurking in the substrate with aquatic ultrasonic scans via Arduino Nano ESP32 (Ridge classification) and assess water pollution based on chemical (color-coded) water quality tests via UNIHIKER (NVIDIA TAO RetinaNet) simultaneously.,,MachineLearning,51,11,1715089985.0,1cmcfv4,the-amplituhedron,https://www.reddit.com/gallery/1cmcfv4,Project
[Discussion] Should I go to ICML and present my paper?,"I finished my Ph.D. a year ago. Left academia and went to be a data scientist at a tech company. I like it, but still thinking about moving to a more research position somehow in the future. Not sure though.

Anyway, an unfinished work of mine got picked by a friend which finished it and applied to ICML. It got accepted (yay!).

I now wonder - beside the fact that I find conferences fun, is there an actual benefit in attending? Presenting the paper? I know that for academic / researchers, this is a great opportunity to meat people and hear about current research. But as I'm not there anymore, is there a real reason to go?

Quite a weird question, but I am just not sure, and I'd be happy to hear your thoughts.",MachineLearning,48,14,1714685459.0,1cirfu1,meni_s,https://www.reddit.com/r/MachineLearning/comments/1cirfu1/discussion_should_i_go_to_icml_and_present_my/,Discussion
[D] Has anyone managed to train an LLM with model parallelism?,"Hello,

I am working on fine-tuning Llama-3.1 for my master’s thesis research. Unfortunately, my current situation forbids access to high-memory GPUs such as A100s. Instead, I have access to setups with multiple lower-memory GPUs, such as 4×3090 or 8×V100.

Therefore I need to implement model parallelism to train my model as it doesn’t fit into a single GPU. However, I’ve noticed that most frameworks primarily focus on data parallelism, which doesn’t address my needs.

Has anyone successfully trained a model by splitting it across multiple GPUs? If so, could you recommend frameworks or approaches I should explore? I am specifically looking for full training, although I am interested in hearing if someone managed this using LoRA.

Also, if there’s a more suitable subreddit for this type of question, please direct me to there.

Thank you!",MachineLearning,52,39,1733756792.0,1habr8l,anilozlu,https://www.reddit.com/r/MachineLearning/comments/1habr8l/d_has_anyone_managed_to_train_an_llm_with_model/,Discussion
[R]: How much is a noisy image worth? 👀,"[https://arxiv.org/abs/2411.02780](https://arxiv.org/abs/2411.02780)

Shows that corrupted images can be almost as useful as clean images for training generative models, assuming that a small initial set of clean images is available.

This could be useful for dataset design/curation: some budget needs to be invested in obtaining a few high-quality samples and then for the rest of the dataset corrupted images should work fine.

https://preview.redd.it/8vk1nwfexizd1.jpg?width=2952&format=pjpg&auto=webp&s=c6f753956e531303f7818de2c5aa5b5b94d9c2da

**Abstract:**

>The quality of generative models depends on the quality of the data they are trained on. Creating large-scale, high-quality datasets is often expensive and sometimes impossible, e.g. in certain scientific applications where there is no access to clean data due to physical or instrumentation constraints. Ambient Diffusion and related frameworks train diffusion models with solely corrupted data (which are usually cheaper to acquire) but ambient models significantly underperform models trained on clean data. We study this phenomenon at scale by training more than 80 models on data with different corruption levels across three datasets ranging from 30,000 to ≈1.3M samples. We show that it is impossible, at these sample sizes, to match the performance of models trained on clean data when only training on noisy data. Yet, a combination of a small set of clean data (e.g. \~10% of the total dataset) and a large set of highly noisy data suffices to reach the performance of models trained solely on similar-size datasets of clean data, and in particular to achieve near state-of-the-art performance. We provide theoretical evidence for our findings by developing novel sample complexity bounds for learning from Gaussian Mixtures with heterogeneous variances. Our theoretical model suggests that, for large enough datasets, the effective marginal utility of a noisy sample is exponentially worse than that of a clean sample. Providing a small set of clean samples can significantly reduce the sample size requirements for noisy data, as we also observe in our experiments.

Paper: [https://arxiv.org/abs/2411.02780](https://arxiv.org/abs/2411.02780)

Code: [https://github.com/giannisdaras/ambient-laws](https://github.com/giannisdaras/ambient-laws)

Huggingface models: [https://huggingface.co/giannisdaras?search\_models=ambient\_laws](https://huggingface.co/giannisdaras?search_models=ambient_laws)",MachineLearning,46,14,1731004407.0,1glxhj9,Constant_Club_9926,https://www.reddit.com/r/MachineLearning/comments/1glxhj9/r_how_much_is_a_noisy_image_worth/,Research
[D] Does anyone here work in healthcare?,I'm curious about the cool things people around the world are doing related to data in this area of work att,MachineLearning,48,17,1730285402.0,1gfjngd,Intelligent-Cap-4022,https://www.reddit.com/r/MachineLearning/comments/1gfjngd/d_does_anyone_here_work_in_healthcare/,Discussion
[P] Shape-restricted regression with neural networks,"Some time ago at work we had to enforce that our model learns an increasing function of a feature. For example, the probability of winning an auction as a function of the bid should increase. Recently, I encountered the paper [https://arxiv.org/abs/2209.04476](https://arxiv.org/abs/2209.04476) on regression with shape-restricted functions, and wanted to make it a bit more tangible, with actual code that trains such a model.

So it resulted in a blog post: [https://alexshtf.github.io/2024/10/14/Shape-Restricted-Models.html](https://alexshtf.github.io/2024/10/14/Shape-Restricted-Models.html)  
There's also a notebook with the accompanying code: [https://github.com/alexshtf/alexshtf.github.io/blob/master/assets/shape\_constrained\_models.ipynb](https://github.com/alexshtf/alexshtf.github.io/blob/master/assets/shape_constrained_models.ipynb)

I used to work on ads quite a lot .So such models seem useful in this industry - predicting the probability of winning an ad auction given the bid. I hope it's also useful elsewhere.

So I hope you'll enjoy it! It's a big 'mathy', but you know, it can't be otherwise.",MachineLearning,46,9,1729961921.0,1gcpl03,alexsht1,https://www.reddit.com/r/MachineLearning/comments/1gcpl03/p_shaperestricted_regression_with_neural_networks/,Project
"[R] Beating gpt-4o structured output with gpt-3.5 and haiku on cost, latency and accuracy","Full post: [https://www.boundaryml.com/blog/sota-function-calling](https://www.boundaryml.com/blog/sota-function-calling)

Using [BAML](https://www.github.com/boundaryml/baml), we nearly solved^(1) [**Berkeley function-calling benchmark (BFCL)**](https://gorilla.cs.berkeley.edu/leaderboard.html) with every model (gpt-3.5+). Looking forward to sharing the arXiv paper soon!

https://preview.redd.it/78uxa0xx5pid1.png?width=916&format=png&auto=webp&s=f36e9c6fbb8ea1939c5406e552b0dcf0a4f6fe20

# Key Findings

1. **BAML is more accurate and cheaper** for function calling than any native function calling API. It's easily 2-4x faster than OpenAI's FC-strict API.
2. **BAML's technique is model-agnostic** and works with any model without modification (even open-source ones).
3. **gpt-3.5-turbo**, **gpt-4o-mini**, and **claude-haiku** with BAML work almost as well as gpt4o with structured output (less than 2%)
4. Using FC-strict over naive function calling improves every older OpenAI models, **but** `gpt-4o-2024-08-06` **gets worse**

# Background

Until now, the only way to get better results from LLMs was to:

1. Prompt engineer the heck out of it with longer and more complex prompts
2. Train a better model

# What BAML does differently

1. Replaces JSON schemas with typescript-like definitions. e.g. `string[]` is easier to understand than `{""type"": ""array"", ""items"": {""type"": ""string""}}`.
2. Uses a novel parsing technique (Schema-Aligned Parsing) inplace of JSON.parse. SAP allows for fewer tokens in the output with no errors due to JSON parsing. For example, this can be parsed even though there are no quotes around the keys. [PARALLEL-5](https://www.boundaryml.com/blog/sota-function-calling?q=5&cmp=accuracy&test=parallel_function&model=gpt-4o-2024-08-06&r=1#bfcl-results)



    [ 
      { 
        streaming_service: ""Netflix"", 
        show_list: [""Friends""],
        sort_by_rating: true 
      }, 
      { 
        streaming_service: ""Hulu"", 
        show_list: [""The Office"", ""Stranger Things""],
        sort_by_rating: true 
      } 
    ]

We used our prompting DSL (BAML) to achieve this\[2\], without using JSON-mode or any kind of constrained generation. We also compared against [OpenAI's structured outputs](https://openai.com/index/introducing-structured-outputs-in-the-api/) that uses the 'tools' API, which we call ""FC-strict"".

**Thoughts on the future**

Models are really, really good an semantic understanding.

Models are really bad at things that have to be perfect like perfect JSON, perfect SQL, compiling code, etc.

Instead of efforts towards training models for structured data or contraining tokens at generation time, we believe there is un-tapped value in applying engineering efforts to areas like **robustly handling the output of models**.",MachineLearning,49,21,1723839353.0,1etyrs8,kacxdak,https://www.reddit.com/r/MachineLearning/comments/1etyrs8/r_beating_gpt4o_structured_output_with_gpt35_and/,Research
[D] New Llama scaling laws?,"""We made several new observations on scaling behavior during the development of Llama 3. For example, while the Chinchilla-optimal amount of training compute for an 8B parameter model corresponds to \~200B tokens, we found that model performance continues to improve even after the model is trained on two orders of magnitude more data. Both our 8B and 70B parameter models continued to improve log-linearly after we trained them on up to 15T tokens. Larger models can match the performance of these smaller models with less training compute, but smaller models are generally preferred because they are much more efficient during inference.""

That's an excerpt from [Introducing Meta Llama 3: The most capable openly available LLM to date](https://ai.meta.com/blog/meta-llama-3/)

Do you think Llama just introduced new scaling laws? Why didn't Chinchilla find that out before? Is there any new number on the proportion of model size vs amount of tokens?",MachineLearning,48,12,1723454501.0,1eq95ga,akashkash,https://www.reddit.com/r/MachineLearning/comments/1eq95ga/d_new_llama_scaling_laws/,Discussion
[D] What are your real-world production use cases for LLMs?,"I think we should share more production use cases for LLMs instead of just theoretical best practices.

Can you share the use cases you've seen/built in production? It should include the following details:

1. The problem it solves
2. The implementation details (models, infrastructure, etc.)
3. The business impact it had",MachineLearning,48,26,1717305134.0,1d65vj7,madredditscientist,https://www.reddit.com/r/MachineLearning/comments/1d65vj7/d_what_are_your_realworld_production_use_cases/,Discussion
[P] MusicGPT – An Open Source App for Generating Music with Local LLMs,"Hi everyone!

Wanted to share the latest side hustle that I've been cooking for the past few months. This is a terminal application that runs locally music generation models, right now, only [MusicGen by Meta](https://audiocraft.metademolab.com/musicgen.html) is available.

[https://github.com/gabotechs/MusicGPT](https://github.com/gabotechs/MusicGPT)

It works on Windows, Linux and MacOS without the need for Python or any heavy machine learning framework installed. Instead, it's written entirely in Rust using the ONNX runtime to run the LMs locally in a performant way, even using hardware accelerators like GPUs.

The app works like this:

- It accepts a natural language prompt from the user

- Generates a music sample conditioned by the prompt

- Encodes the generated sample into .wav format and plays it on the device

Additionally, it ships a UI that allows interacting with the AI models in a chat-like web application, storing chat history and generated music on the device.

The vision of the project is that it can eventually generate infinite music streams in real time, for example, an infinite stream of always new LoFi songs for listening while coding, but not quite there yet...

It was an interesting journey getting a transformer based model up and running in a constrained environment in Rust, without PyTorch or TensorFlow, hope you like it!",MachineLearning,50,16,1716827682.0,1d1vp2u,GabrielMusat,https://www.reddit.com/r/MachineLearning/comments/1d1vp2u/p_musicgpt_an_open_source_app_for_generating/,Project
[D] What's the fastest object detection model?,"Hi, I'm working on a project that needs object detection. The task itself isn't complex since the objects are quite clear, but speed is critical. I've researched various object detection models, and it seems like almost everyone claims to be ""the fastest"". Since I'll be deploying the model in C++, there is no time to port and evaluate them all.

I tested YOLOv5/v5Lite/8/10 previously, and YOLOv5n was the fastest. I ran a simple benchmark on an Oracle ARM server (details [here](https://github.com/Avafly/YOLOv5-ncnn-OpenVINO-MNN-ONNXRuntime-OpenCV-CPP?tab=readme-ov-file#simple-benchmarks-on-m1-mac-and-arm-linux)), and it processed an image with 640 target size in just 54ms. Unfortunately, the hardware for my current project is significantly less powerful, and meanwhile processing time must be less than 20ms. I'll use something like quantization and dynamic dimension to boost speed, but I have to choose the suitable model first.

Has anyone faced a similar situation or tested models specifically for speed? Any suggestions for models faster than YOLOv5n that are worth trying?",MachineLearning,48,39,1732946440.0,1h362dq,Knok0932,https://www.reddit.com/r/MachineLearning/comments/1h362dq/d_whats_the_fastest_object_detection_model/,Discussion
[D] Do second tier papers have any value when apply for industry research job?,"I think I have come across some industry jobs before that required applicants to have top tier paper (NIPS/ICML/ICLR/CVPR/ICCV/ECCV), so my question is do paper from *less prestige* (AAAI/IJCAI/WACV/BMVC.... or  journals) conference have any value when appying for these job? Additionaly, are metrics like h-index or citation matter?",MachineLearning,46,43,1730782697.0,1gjz3in,Competitive_Newt_100,https://www.reddit.com/r/MachineLearning/comments/1gjz3in/d_do_second_tier_papers_have_any_value_when_apply/,Discussion
[R] Switch EMA: A Free Lunch for Better Flatness and Sharpness,,MachineLearning,48,9,1729092781.0,1g5278x,parlancex,https://arxiv.org/abs/2402.09240,Research
[P] Free RSS feed for tousands of jobs in AI/ML/Data Science every day,"This is for all of you interested in a constant flow of freshly curated jobs in Artificial Intelligence, Machine Learning, NLP, Computer Vision, Data Engineering, Data Analytics, Big Data, and Data Science in general via RSS format. Jobs are aggregated through [aijobs.net](http://aijobs.net) and it provides 200 listings at a time. The feed is updated about every hour with the latest jobs.

URL: [https://aijobs.net/feed/](https://aijobs.net/feed/)

No sign-up needed - just add it to your favourite feed reader and be in the loop about new opportunities at any time 🚀",MachineLearning,48,7,1725437579.0,1f8nw8f,ai_jobs,https://www.reddit.com/r/MachineLearning/comments/1f8nw8f/p_free_rss_feed_for_tousands_of_jobs_in_aimldata/,Project
[Project] I Created the Definitive AUTOMATIC Shiny Hunter for Pokémon BDSP,"Hey everyone! I am Dinones! I coded a Python program using object detection that lets my computer hunt for shiny Pokémon on my physical Nintendo Switch while I sleep. So far, I’ve **automatically caught shiny Pokémon** like Giratina, Dialga or Azelf, Rotom, Drifloon, all three starters, and more in Pokémon BDSP. Curious to see how it works? Check it out! The program is available for everyone! Obviously, for free; I'm just a student who likes to program this stuff in his free time :)

The games run on a **Nintendo Switch** (**not emulated**, a real one). The program gets the output images using a capture card, then, it process them to detect whether the pokemon is shiny or not (OpenCV). Finally, it emulates the joycons using bluetooth (NXBT) and control the Nintendo. **Also works on a Raspberry Pi**!

*I don't make money with this, I just feel my project can be interesting for lot of people.*

📽️ Youtube: [https://www.youtube.com/watch?v=84czUOAvNyk](https://www.youtube.com/watch?v=84czUOAvNyk)  
🤖 Github: [https://github.com/Dinones/Nintendo-Switch-Pokemon-Shiny-Hunter](https://github.com/Dinones/Nintendo-Switch-Pokemon-Shiny-Hunter)



https://preview.redd.it/7jbe6fdxrijd1.png?width=1920&format=png&auto=webp&s=626c801925fb0769f59e62ece09f0e00b18b828e

https://preview.redd.it/2h2alqcxrijd1.png?width=1920&format=png&auto=webp&s=fddd11c5c04c58268bbaf0e8bca0fd7081a7f775

  
",MachineLearning,48,4,1724029203.0,1evp3wz,Dinones,https://www.reddit.com/r/MachineLearning/comments/1evp3wz/project_i_created_the_definitive_automatic_shiny/,Project
[R] New Paper on Mixture of Experts (MoE) 🚀,"Hey everyone! 🎉

Excited to share a new paper on Mixture of Experts (MoE), exploring the latest advancements in this field. MoE models are gaining traction for their ability to balance computational efficiency with high performance, making them a key area of interest in scaling AI systems.

The paper covers the nuances of MoE, including current challenges and potential future directions. If you're interested in the cutting edge of AI research, you might find it insightful.

Check out the paper and other related resources here: [GitHub - Awesome Mixture of Experts Papers](https://github.com/arpita8/Awesome-Mixture-of-Experts-Papers).

Looking forward to hearing your thoughts and sparking some discussions! 💡

#AI #MachineLearning #MoE #Research #DeepLearning #NLP #LLM

https://preview.redd.it/yulmcq0xvkid1.png?width=1096&format=png&auto=webp&s=522ce335da7acfdfb1d298cbc04c32b12b04de92

",MachineLearning,47,17,1723618950.0,1erv2sn,Ok_Parsley5093,https://www.reddit.com/r/MachineLearning/comments/1erv2sn/r_new_paper_on_mixture_of_experts_moe/,Research
[D] NeurIPS 24 Dataset Track Reviews,"Dataset and benchmarks track reviews are supposed to come out today after the delay.

I am sure we are a lot less concerned by this compared to the main track but this can serve as a discussion thread :)",MachineLearning,48,137,1723186599.0,1ent5sa,medcanned,https://www.reddit.com/r/MachineLearning/comments/1ent5sa/d_neurips_24_dataset_track_reviews/,Discussion
[D] Is anyone else having trouble with the unstructured output from language models? 😩,"I found a way to fix this and wrote the process with code in [this article](https://medium.com/p/2f699b718a6b) but if anyone knows of a better way, I'd love to know!

Basically used Pydantic output parsers to turn messy LLM responses into clean, structured data that's easy to use in our applications. The process was build a model that suggests data visualization charts from a tabular dataset, organized the output, and made it ready for easy API calls and frontend use.",MachineLearning,45,36,1720514772.0,1dyxiw4,stoicwolfie,https://www.reddit.com/r/MachineLearning/comments/1dyxiw4/d_is_anyone_else_having_trouble_with_the/,Discussion
[D] ML papers with the best figures,I often struggle to make aesthetically pleasing and high-quality figures and thought it would be helpful to ask for papers to reference next time I need to make any. The RLHF pipeline figure comes to mind. What are some other papers that come to mind or are commonly used as references?,MachineLearning,47,21,1718175895.0,1de0i4b,SatisfyingLatte,https://www.reddit.com/r/MachineLearning/comments/1de0i4b/d_ml_papers_with_the_best_figures/,Discussion
[P] SimpleGEMM: Fast and minimal tensor core matrix multiplication in CUDA,"Hello all! Sharing my side project here: [https://github.com/andylolu2/simpleGEMM](https://github.com/andylolu2/simpleGEMM) !

This is an *extremely* minimalistic but fast implementation of matrix multiplication in CUDA. The source code is a single, 200-line CUDA/C++ file which implements fp16 tensor core matrix multiplication, optimised for Turing (SM75) architecture. The goal is to:

1. Write a matmul kernel that does not sacrifice performance. In fact, it's faster than PyTorch/CuBLAS if you [test it on a T4 in Colab](https://colab.research.google.com/github/andylolu2/simpleGEMM/blob/master/colab/simpleGEMM.ipynb)!
2. Make it hackable for new purposes. For example if you want to add a new custom prologue (e.g. Matmul + some reduction), just go to line 186, add your code, and recompile! Full flexibility with no C++ templating shenanigans.
3. Keep it as simple as possible. Hopefully someone learning CUDA will find this useful!

Of course, I didn't implement *everything* from scratch. Most of the this builds upon Nvidia CUTLASS's new CuTe interface for things like memory layout, data copying and using tensor core instructions.

*Aside:*

*Why not OpenAI Triton? I love triton, but sometimes it's hard to get the extra 10-20% performance if you are doing something off its main optimisation path. In fact,* [*triton's matmul for Turing GPUs is quite slow*](https://github.com/openai/triton/issues/189) *(because they mainly optimise for SM80+). I just enjoy having full control over the hardware, knowing that if I have infinite time I can squeeze very single bit of performance out.*",MachineLearning,47,3,1715547320.0,1cqhsln,bjergerk1ng,https://www.reddit.com/r/MachineLearning/comments/1cqhsln/p_simplegemm_fast_and_minimal_tensor_core_matrix/,Project
[D] Llama 3 Monstrosities,"I just noticed some guy created a 120B Instruct variant of Llama 3 by merging it with itself (end result duplication of 60 / 80 layers). He seems to specialize in these Frankenstein models. For the life of me, I really don't understand this trend. These are easy breezy to create with mergekit, and I wonder about their commercial utility in the wild. Bud even concedes its not better than say, GPT-4. So what's the point? Oh wait, he gets to the end of his post and mentions he submitted it to Open LLM Leaderboard... there we go. The gamification of LLM leaderboard climbing is tiring.",MachineLearning,44,23,1715004290.0,1cljvpa,Objective-Camel-3726,https://www.reddit.com/r/MachineLearning/comments/1cljvpa/d_llama_3_monstrosities/,Discussion
"[D] NVIDIA’s hostages: A Cyberpunk Reality of Monopolies
","In AI and professional workstations, NVIDIA's dominance feels like a suffocating monopoly. Their segmented product lines widen the gap between consumer and professional GPUs, particularly in VRAM, performance, and price.

AI enthusiasts struggle with prohibitive costs for GPUs equipped with sufficient VRAM. The reliance on CUDA cores—a proprietary standard—further locks developers into NVIDIA’s ecosystem, stifling competition and innovation.

NVIDIA’s control extends beyond hardware, as their CUDA platform discourages adoption of open, competitive solutions. This feeds a cyberpunk dystopia where corporations consolidate power, leaving consumers and developers with few choices.

Why does the tech world remain complicit? Why aren’t we pursuing alternative hardware architectures or broader software compatibility beyond CUDA? AMD’s ROCm is a start, but more aggressive development and policy interventions are needed to challenge NVIDIA’s grip.

Until when will this continue? Who will stand up for the end consumer?",MachineLearning,46,25,1734116574.0,1hdjklf,SevenShivas,https://www.reddit.com/r/MachineLearning/comments/1hdjklf/d_nvidias_hostages_a_cyberpunk_reality_of/,Discussion
[R] BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games,"Tired of saturated benchmarks? Want scope for a significant leap in capabilities? 

Introducing BALROG: a Benchmark for Agentic LLM and VLM Reasoning On Games!

BALROG is a challenging benchmark for LLM agentic capabilities, designed to stay relevant for years to come.

  
Check it out!

GitHub: [https://github.com/balrog-ai/BALROG](https://github.com/balrog-ai/BALROG)

Leaderboard: [https://balrogai.com](https://balrogai.com)

Paper: [https://arxiv.org/abs/2411.13543](https://arxiv.org/abs/2411.13543)",MachineLearning,44,6,1732199600.0,1gwhnf8,pagggga,https://www.reddit.com/r/MachineLearning/comments/1gwhnf8/r_balrog_benchmarking_agentic_llm_and_vlm/,Research
[D] Paper Club: Nvidia Researcher Ethan He Presents Upcycling LLMs in MoE,"Hey all,  


Tomorrow Nvidia researcher Ethan He will be doing a technical dive into his work: Upcycling LLMs in Mixture of Experts (MoE). Excited to get a peak behind the curtains to see what it is like to work on models at this scale at Nvida.

  
If you’d like to join the community tomorrow 10 AM PST we’d love to have you. We do it live over zoom and anyone is welcome to join.

Here's the paper: [https://arxiv.org/abs/2410.07524](https://arxiv.org/abs/2410.07524)  
Join us live: [https://lu.ma/arxivdive-31](https://lu.ma/arxivdive-31)",MachineLearning,50,8,1731629984.0,1grjjlz,FallMindless3563,https://www.reddit.com/r/MachineLearning/comments/1grjjlz/d_paper_club_nvidia_researcher_ethan_he_presents/,Discussion
[D] Just how bad is tfds code quality?,"I'm trying a new cute architecture on a bunch of the default datasets out there, using Jax since I'm doing live brain surgery, that part works well.

What I'm having a hell of a time with is actually loading the data. I was going for tfds since its 1) old 2) used in production 3) has a million datasets already prepared. I've not used TF since the 2.0 days and everything seems broken? I'm getting warnings and errors whenever I try loading and running through any dataset. Even their documentation has the errors [0] in the tutorial notebooks.

I can't just ignore a whole bunch of errors and warnings when I'm trying to benchmark a new architecture. Is tfds just that bad or am I missing something obvious? 

[0] https://www.tensorflow.org/datasets/overview",MachineLearning,47,13,1731036303.0,1gm96yo,acc_agg,https://www.reddit.com/r/MachineLearning/comments/1gm96yo/d_just_how_bad_is_tfds_code_quality/,Discussion
[D] ML for Drug Discovery a good path? ,"I see now a lot of startups (big and small) focusing on ML for Drug Discovery / ML for biological applications and want to know the scope of Applied ML Research in this field. 

1. Are there mature problem statements that actually require ML Research to solve them, and what are they (I am of course familiar with Alpha fold/protein folding work, but considering this is already solved are there other active areas of research)
2. Are these problem statements limited to research labs (while solid research, they have narrow specific usecases), or do they solve industry scope 
3. Considering the regulatory requirements of the healthcare field, a) Is there readily available data and b) Can the solutions to these problems actually goto production/become a product? 

I am currently in general Applied ML Research (with CV/NLP/multimodal) experience, and wondering whether to invest in transitioning to the drug discovery niche, since I do have past experience in the healthcare field. I have seen a number of similar roles in big pharma companies that are exploring AI but typically these types of companies lack solid AI technical leadership and end up building POC solutions based on existing open source tools. I would love to hear from folks in AI-first companies or research labs that have deep technical expertise in the drug discovery problem. ",MachineLearning,45,24,1726221540.0,1ffqy6y,panther-banter,https://www.reddit.com/r/MachineLearning/comments/1ffqy6y/d_ml_for_drug_discovery_a_good_path/,Discussion
[D] What's the current battle-tested state-of-the-art multivariate time series regression mechanism?,"What's the current battle-tested state-of-the-art multivariate time series regression mechanism? Using multiple time series to predict a single value.

For multiple semi-stationary time series.

By ""battle-tested"" I mean it is used already by at least 5% of the industry, or currently gathering a great momentum of adoption.",MachineLearning,47,18,1719705155.0,1drohkd,None,https://www.reddit.com/r/MachineLearning/comments/1drohkd/d_whats_the_current_battletested_stateoftheart/,Discussion
[R] Can LLMs invent better ways to train LLMs?,"New blog post and paper:

https://sakana.ai/llm-squared/

https://arxiv.org/abs/2406.08414

**Discovering Preference Optimization Algorithms with and for Large Language Models**

*Abstract*

Offline preference optimization is a key method for enhancing and controlling the quality of Large Language Model (LLM) outputs. Typically, preference optimization is approached as an offline supervised learning task using manually-crafted convex loss functions. While these methods are based on theoretical insights, they are inherently constrained by human creativity, so the large search space of possible loss functions remains under explored. We address this by performing LLM-driven objective discovery to automatically discover new state-of-the-art preference optimization algorithms without (expert) human intervention. Specifically, we iteratively prompt an LLM to propose and implement new preference optimization loss functions based on previously-evaluated performance metrics. This process leads to the discovery of previously-unknown and performant preference optimization algorithms. The best performing of these we call Discovered Preference Optimization (DiscoPOP), a novel algorithm that adaptively blends logistic and exponential losses. Experiments demonstrate the state-of-the-art performance of DiscoPOP and its successful transfer to held-out tasks.",MachineLearning,44,20,1718245116.0,1deo4pd,hardmaru,https://www.reddit.com/r/MachineLearning/comments/1deo4pd/r_can_llms_invent_better_ways_to_train_llms/,Research
[R] Why using the Gumbel-Softmax is better than just using Softmax ?,"Hello,
Many papers tend to use the Gumbel-Softmax function to generate a probability distribution , and then simple a binary mask for this distribution. My quedtion is why is Gumbel-Softmax better than Softmax. As for me the trick is to keep the gradient from the differentiable vector while using the binary mask.
Thanks !",MachineLearning,47,13,1717002505.0,1d3hadp,Training-Adeptness57,https://www.reddit.com/r/MachineLearning/comments/1d3hadp/r_why_using_the_gumbelsoftmax_is_better_than_just/,Research
"[P] State-of-the-art, open source, Computer Vision models that are not ultra resource intensive?","What are some leading-edge CV models (object detection, segmentation etc) that can fit on a relatively mid-tier GPU such as an A4000 or thereabouts. I'm specifically interested in inference on hardware, training is less important.

Something more interesting and performant than say a ResNet or YOLO, doesn't have to be a CNN!

Thanks in advance, just hit me with your ideas

Edit: I neglected to mention that I'm interested in FPGA inference deployment in addition, this is clearly more of a limiting factor than GPU.

Edit: My testing indicates the inference module is generally very lightweight for the majority of current CV models, I'm going to research ways to increase resource utilisation through compiler directives, scheduling and graph optimisations - Thanks!",MachineLearning,48,19,1716559311.0,1czlgss,None,https://www.reddit.com/r/MachineLearning/comments/1czlgss/p_stateoftheart_open_source_computer_vision/,Project
[R] Entropy-Guided Critical Neuron Pruning for Efficient Spiking Neural Networks,"This paper introduces a pruning method for Spiking Neural Networks (SNNs) based on neuroscience principles of criticality. The key insight is using neuronal avalanche analysis to identify neurons that have the most significant impact on network dynamics, similar to how critical neurons function in biological brains.

Key technical points:
* Monitors spike propagation patterns to identify critical neurons
* Introduces adaptive pruning schedule based on network stability metrics
* Achieves 90% compression while maintaining accuracy on MNIST/CIFAR-10
* Works across different SNN architectures (feed-forward, CNN)
* Uses stability measures to prevent catastrophic forgetting during pruning

Main results:
* Outperforms existing pruning methods on accuracy retention
* Shows better energy efficiency compared to unpruned networks
* Maintains temporal dynamics important for SNN operation
* Demonstrates scalability across different network sizes
* Validates biological inspiration through avalanche analysis

I think this approach could be particularly important for deploying SNNs in resource-constrained environments like edge devices. The adaptive pruning schedule seems especially promising since it automatically adjusts based on network behavior rather than requiring manual tuning.

I think there are some open questions about computational overhead of the avalanche analysis that need to be addressed for very large networks. However, the biological principles behind the method suggest it could generalize well to other architectures and tasks.

TLDR: Novel pruning method for SNNs based on neuroscience principles of criticality. Uses neuronal avalanche analysis to identify important neurons and achieves 90% compression while maintaining accuracy. Introduces adaptive pruning schedule that adjusts based on network stability.

[Full summary is here](https://aimodels.fyi/papers/arxiv/brain-inspired-efficient-pruning-exploiting-criticality-spiking). Paper [here](https://arxiv.org/abs/2311.16141).",MachineLearning,45,5,1732283114.0,1gx86i0,Successful-Western27,https://www.reddit.com/r/MachineLearning/comments/1gx86i0/r_entropyguided_critical_neuron_pruning_for/,Research
"[R] Undetectable Backdoors in ML Models: Novel Techniques Using Digital Signatures and Random Features, with Implications for Adversarial Robustness","I found an important analysis of backdoor attacks that demonstrates how a malicious service provider can insert undetectable backdoors into machine learning models.

The key contribution is showing how to construct backdoors that are provably undetectable even under white-box analysis, while allowing arbitrary manipulation of model outputs through subtle input perturbations.

Technical details:
* Two frameworks for planting undetectable backdoors:
  * Digital signature scheme-based backdoors that are computationally infeasible to detect with black-box access
  * Random Fourier Features/Random ReLU based backdoors that withstand white-box inspection
* Backdoored models are indistinguishable from clean models even with:
  * Full access to model architecture and parameters
  * Complete training dataset
  * Ability to analyze model behavior

Results:
* Backdoored models maintain same generalization error as original models
* Service provider can modify classification of any input with slight perturbations
* Construction works with any underlying model architecture
* Backdoors cannot be detected by any computationally-bounded observer

The implications are significant for ML security and outsourced training. The work shows fundamental limitations in certifying adversarial robustness - a backdoored model can be indistinguishable from a robust one while having adversarial examples for every input.

**TLDR:** Paper proves it's possible to insert undetectable backdoors into ML models that allow arbitrary manipulation of outputs while being provably impossible to detect.

[Full summary is here](https://aimodels.fyi/papers/arxiv/planting-undetectable-backdoors-machine-learning-models). Paper [here](https://arxiv.org/abs/2204.06974).",MachineLearning,46,5,1731590341.0,1gr4ksm,Successful-Western27,https://www.reddit.com/r/MachineLearning/comments/1gr4ksm/r_undetectable_backdoors_in_ml_models_novel/,Research
[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",MachineLearning,45,38,1728785721.0,1g2fmfw,AutoModerator,https://www.reddit.com/r/MachineLearning/comments/1g2fmfw/d_selfpromotion_thread/,Discussion
[D] Is there an appropriate community for technical discussions of general intelligence development?,"Acknowledgment that is post is skirting the line of not discussing AGI, and mods can delete it. I know that posts related to AGI should be directed to r/singularity , but that reddit seems to mostly be filled with non-technical posts hyping and philosophizing news articles. I think there is a lot of valid discussion in the field of ML to be had regarding technical approaches, issues, and research to creating generalized intelligence, such as spiking networks, evolutionary algorithms, memory augmented networks, RL etc. I don't think just scaling current approaches (LLMs) will get us there for technical reasons and we are rather far out, but I don't want this post to be about discussing that. Rather, are there recommendations for communities or other groups that focus on the technical work, research, and practical discussion of working towards AGI?",MachineLearning,47,38,1723051670.0,1emhuwa,Revolutionary-Fig660,https://www.reddit.com/r/MachineLearning/comments/1emhuwa/d_is_there_an_appropriate_community_for_technical/,Discussion
[R] Discussion of ReFT Paper with lead author Zhengxuan Wu,"Hey all,

We were lucky enough to have the lead author of the ReFT paper in our Friday paper dive this week and thought I'd share the discussion and our notes!

[https://www.oxen.ai/blog/arxiv-dives-how-reft-works](https://www.oxen.ai/blog/arxiv-dives-how-reft-works)

TLDR \~ ReFT is a fine-tuning technique that is 15x-60x more parameter efficient than LoRA. It is super speedy to train. About 18 minutes for 1k examples on an A100. I successfully fine-tuned a ReFT on Llama 2 7B in less than 1 minute of an A10 with \~100 examples.

  
It works by operating on the representations in the residual stream instead of the K-V matrices. They add extra learned parameters they call ""interventions"" to specific token indices and layers making it efficient and easy to steer the representations. ReFTs are also nice because they are composable. For example, you could train one for instruction following, one for German, then apply them both to get and instruction following model in German.

The author gives super practical tips and lessons they learned while iterating in the lab. The whole discussion is on YouTube as well.

Hope you enjoy!  
",MachineLearning,46,4,1721581162.0,1e8qwnl,FallMindless3563,https://www.reddit.com/r/MachineLearning/comments/1e8qwnl/r_discussion_of_reft_paper_with_lead_author/,Research
[P] Instruction Finetuning From Scratch Implementation ,,MachineLearning,43,9,1718543683.0,1dh7cmv,seraschka,https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/ch07.ipynb,Project
[D] What do you think of NoPE (on small models at least)?,"Hi!

I tried Karpathy's ""[Let's reproduce GPT-2 (124M)](https://www.youtube.com/watch?v=l8pRSuU81PU)"" (and [repo](https://github.com/karpathy/build-nanogpt)) code but with NoPE, just removing the positional encoding.

EDIT: NoPE stands for No Positional Encoding, I don't know if it was used before but I first saw it in the paper ""[The Impact of Positional Encoding on Length Generalization in Transformers](https://arxiv.org/abs/2305.19466)"" which argues that there is no need to add positional encoding and that the network will learn them. They also argue that it outperforms them. 

EDIT2: Unfortunately in my experience I found that ALiBi did better than NoPE, it had the same low perplexity values for all sequence lengths as the lowest perplexity of NoPE (on the training sequence length, 1024)

From what I'm getting, it's not bad, but it's generalization capabilities are a bit exagerated in my opinion.

I got the following figures:

https://preview.redd.it/clq8zwl2ve6d1.png?width=1480&format=png&auto=webp&s=c8c846df1ada58c0bed156e24595478087e77b75

https://preview.redd.it/0hvgd273ve6d1.png?width=1498&format=png&auto=webp&s=f111ef399c34f7c01530064cb3b87d71ac2db8a8

It's on the same hardware, training and validation sets, training configuration etc. Unfortunately I can't compare with the model with learned PE (these experiments are too costly for me).

There are many issues with the figures:

* Not the same colors for the same legend in the two figures. I couldn't correct that my instance was already shut down :(
* The jumps in the sequence lengths are too big (always double).
* Something happened during training at around 15xxx step. When I came back I found that the norm was too high (didnt log/plot that), even at the last step so it got a hard time to recover from that. I'm not sure about the norm during the stable phase but I guess it decreased below 1.

I lack the most important point which is a fair comparison with the learned PE model but I'm already running two experiments with ALiBi and FIRE and this is becoming way too costly. There is no real gain in terms of throughput with NoPE (in this case at least) so the only gain we can observe *might be* a bit of generalization.

What do you guys think? Anyone has more experience with this?",MachineLearning,46,14,1718316002.0,1dfay95,ReinforcedKnowledge,https://www.reddit.com/r/MachineLearning/comments/1dfay95/d_what_do_you_think_of_nope_on_small_models_at/,Discussion
[N] YaFSDP vs. FSDP for LLM training: real improvement?,"At Yandex, we’ve developed an enhanced version of FSDP, called YaFSDP, which shows an impressive speedup of up to 26% (compared to FSDP) in LLM training time and huge savings in GPU resources. For instance, in a pre-training scenario involving a model with 70 billion parameters, using YaFSDP can save the resources of approximately 150 GPUs, which translates to roughly $0.5 to $1.5 million (depending on the virtual GPU provider or platform) in potential monthly savings. 

YaFSDP is open-sourced, so we really can’t wait to hear your feedback and see how you implement it in your work!

[https://github.com/yandex/YaFSDP](https://github.com/yandex/YaFSDP) ",MachineLearning,46,7,1718106115.0,1ddc1uf,azalio,https://www.reddit.com/r/MachineLearning/comments/1ddc1uf/n_yafsdp_vs_fsdp_for_llm_training_real_improvement/,News
[D] ML Conferences and Organization Metrics,"I feel like many would consider NeurIPS, ICLR, ICML, etc as important venues in the field of ML. Even outside of ML, NeurIPS and ICLR have [the #9 and #10 highest H-index of any venues](https://scholar.google.com/citations?view_op=top_venues&hl=en). However, now I am looking at tenure-track positions globally, and it seems like a different story. It seems like such publications are worthless for the purposes of immigration or academic tenure, because they are not traditional journals. You'll notice they are missing from the SCImago, a ranking which many organizations use as a proxy for publication quality, and consequently tenure or immigration decisions.

  
I am curious as to what academic ML researchers do under these circumstances. Do you stop submitting to NeurIPS and aim to publish in ACM ""Foundation and Trends in Machine Learning"", a journal which [ranks #2 on SCImago instead](https://www.scimagojr.com/journalrank.php)? Or the ever-growing list of IEEE journals on machine learning?",MachineLearning,45,20,1717151693.0,1d4shqn,smorad,https://www.reddit.com/r/MachineLearning/comments/1d4shqn/d_ml_conferences_and_organization_metrics/,Discussion
"[R] BiomedParse is a new biomedical foundation AI model for holistic image analysis that can jointly conduct recognition, detection, and segmentation for 64 major object types across 9 imaging modalities in medicine, outperforming prior state-of-the-art methods.","https://microsoft.github.io/BiomedParse/


https://www.microsoft.com/en-us/research/blog/biomedparse-a-foundation-model-for-smarter-all-in-one-biomedical-image-analysis/


https://youtu.be/WUPUypgmB-s
",MachineLearning,45,1,1732061785.0,1gvccdy,Happysedits,https://www.reddit.com/r/MachineLearning/comments/1gvccdy/r_biomedparse_is_a_new_biomedical_foundation_ai/,Research
[R] How do RoPE-based LLMs learn attention sinks (or encode absolute positions)?,"I recently revisited the ‘Attention Sink’ paper ([link](https://arxiv.org/pdf/2309.17453)) and started thinking about how LLMs manage attention sinks. 

The concept of an attention sink describes the phenomenon where LLMs allocate a disproportionately high attention score to the initial tokens, regardless of their semantic value.

Here’s the paradox: state-of-the-art open LLMs typically employ RoPE (Rotary Position Embeddings) for their positional encoding. Since RoPE only encodes relative positions, it’s puzzling how the model consistently identifies and assigns high attention to the absolute initial tokens. Any thoughts on how this behavior might emerge or be explained?",MachineLearning,45,11,1729539988.0,1g8yurr,StraightSpeech9295,https://www.reddit.com/r/MachineLearning/comments/1g8yurr/r_how_do_ropebased_llms_learn_attention_sinks_or/,Research
[R] Addition is All You Need for Energy-Efficient Language Models,"**TL;DR:** approximate floating-point multiplication via addition; get excellent accuracy/hardware efficiency tradeoff.

**Paper:** [https://arxiv.org/pdf/2410.00907](https://arxiv.org/pdf/2410.00907)

**Abstract:**

>Large neural networks spend most computation on floating point tensor multiplications. In this work, we find that a floating point multiplier can be approximated by one integer adder with high precision. We propose the linear-complexity multiplication L-Mul algorithm that approximates floating point number multiplication with integer addition operations. The new algorithm costs significantly less computation resource than 8-bit floating point multiplication but achieves higher precision. Compared to 8-bit floating point multiplications, the proposed method achieves higher precision but consumes significantly less bit-level computation. Since multiplying floating point numbers requires substantially higher energy compared to integer addition operations, applying the L-Mul operation in tensor processing hardware can potentially reduce 95% energy cost by element-wise floating point tensor multiplications and 80% energy cost of dot products. We calculated the theoretical error expectation of L-Mul, and evaluated the algorithm on a wide range of textual, visual, and symbolic tasks, including natural language understanding, structural reasoning, mathematics, and commonsense question answering. Our numerical analysis experiments agree with the theoretical error estimation, which indicates that L-Mul with 4-bit mantissa achieves comparable precision as float8\_e4m3 multiplications, and L-Mul with 3-bit mantissa outperforms float8\_e5m2. Evaluation results on popular benchmarks show that directly applying L-Mul to the attention mechanism is almost lossless. We further show that replacing all floating point multiplications with 3-bit mantissa L-Mul in a transformer model achieves equivalent precision as using float8\_e4m3 as accumulation precision in both fine-tuning and inference.

**The Gist:**

https://preview.redd.it/s2pqz5f6ajtd1.png?width=1119&format=png&auto=webp&s=00729823841c3f50fcc54a1e56657941972fd371

**The Gist, visually:**

https://preview.redd.it/jqt81xtmajtd1.png?width=1113&format=png&auto=webp&s=499225094c7eed7570e29b460e2ae5c859f5ae25

**Highlights:**

https://preview.redd.it/zkenk9ijdjtd1.png?width=1111&format=png&auto=webp&s=ac1260b4d5739fa7b32ae306cc7fd0d9de4b48ff

https://preview.redd.it/qca456akdjtd1.png?width=975&format=png&auto=webp&s=9e04ec40cc6f90fe7430291ce12aabaa3067e168

https://preview.redd.it/qi5z575ldjtd1.png?width=1123&format=png&auto=webp&s=451b4992195270f8f33345ed8d8fd5486627bc45

https://preview.redd.it/3cnq9u7mdjtd1.png?width=629&format=png&auto=webp&s=d1b9a15b60a6b2791468e5de04ae78e51910b4f2

**~~Complaints~~** **Discussion:**

The section dedicated to all-layers L-mul LLM (as opposed to using L-mul within attention block) is quite sketchy, just 2 paragraphs and a tiny table. The only experiment is fine-tuning a Gemma-2-2b-it on GSM8k and evaluating on the same benchmark.

I mean, guys, if you have the audacity to name you paper ""Addition is all you need"", >!virtually guaranteeing the scorn of the whole r/MachineLearning community for using such a deadbeat meme template,!< you might as well show a substantiated proof that L-mul is indeed a comrehensive replacement for vanilla fp multiplications in transformers.

The issue is serious: the efficient implenentation of the method requires highly specialized accelerator hardware. If L-mul fails to replace vanilla fp multiplications in a general way, the need for specialized accelerators becomes way less clear. And without specialized hardware implementations you cannot capture the full benefit of the proposed simplification.",MachineLearning,46,3,1728396195.0,1fz0jza,StartledWatermelon,https://www.reddit.com/r/MachineLearning/comments/1fz0jza/r_addition_is_all_you_need_for_energyefficient/,Research
[P] Breaking down PyTorch functions helped me with understanding what happens under the hood,"Hi guys,

I used to find it tough to understand what’s going on under the hood of the PyTorch library. Breaking down how things work inside was always a challenge for me, so I’ve put together a simple explanation of some key functionalities.

Here I focus on:

* loss.backward()
* torch.no\_grad()
* requires\_grad=True

I know there’s a lot more to explore, and I will cover other functions later on.

Maybe some of you guys could tell me:

* If you have other “black box” functions in mind you struggle with
* Whether you understood my explanation well
* Any feedback on the video (I am grateful for positive and negative feedback)

Thanks a lot!",MachineLearning,46,21,1726463608.0,1fhwwli,vtimevlessv,https://www.reddit.com/r/MachineLearning/comments/1fhwwli/p_breaking_down_pytorch_functions_helped_me_with/,Project
[N] From sci-fi to state law: California’s plan to prevent AI catastrophe,"Ars Technica: [From sci-fi to state law: California’s plan to prevent AI catastrophe](https://arstechnica.com/information-technology/2024/07/from-sci-fi-to-state-law-californias-plan-to-prevent-ai-catastrophe/)

> California's ""Safe and Secure Innovation for Frontier Artificial Intelligence Models Act"" (a.k.a. SB-1047) has led to a flurry of headlines and debate concerning the overall ""safety"" of large artificial intelligence models. But critics are concerned that the bill's overblown focus on existential threats by future AI models could severely limit research and development for more prosaic, non-threatening AI uses today. SB-1047, introduced by State Senator Scott Wiener, passed the California Senate in May with a 32-1 vote and seems well positioned for a final vote in the State Assembly in August. 

An especially notable feature of the bill:

> In his Understanding AI newsletter, Ars contributor Timothy Lee lays out how SB-1047's language could severely hamper the spread of so-called ""open weight"" AI models. That's because the bill would make model creators liable for ""derivative"" models built off of their original training.",MachineLearning,41,15,1722294088.0,1efdpmw,bregav,https://www.reddit.com/r/MachineLearning/comments/1efdpmw/n_from_scifi_to_state_law_californias_plan_to/,News
[D] An Intuitive Explanation of Sparse Autoencoders for LLM Interpretability,"Sparse Autoencoders (SAEs) are one of the most promising and popular methods for LLM interpretability and explainability. I wrote a simple, intuitive introduction to SAEs, complete with diagrams and reference PyTorch code.

Among other things, I cover why we use SAEs, how they are used for model interventions (such as the [Golden Gate Bridge Claude](https://www.anthropic.com/news/golden-gate-claude)), and challenges with using SAEs. One of the most prominent challenges is the lack of good metrics, as there isn't a clear underlying measurable ground truth in natural language text.

Link to the explanation here: [https://adamkarvonen.github.io/machine\_learning/2024/06/11/sae-intuitions.html](https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html)",MachineLearning,44,5,1722202720.0,1eeihdl,seraine,https://www.reddit.com/r/MachineLearning/comments/1eeihdl/d_an_intuitive_explanation_of_sparse_autoencoders/,Discussion
[R] Discussion on the paper: Transcendence: Generative Models Can Outperform The Experts That Train Them,"Hi all,

Per the title: I'm creating this post to discuss the paper that was recently released and received a lot of attention so far. I just read the paper, and have some questions. In case it happened to also have read and liked the paper, let's chat!

[https://arxiv.org/abs/2406.11741](https://arxiv.org/abs/2406.11741)

1. Is the setup clear to you? Do the authors test experimentally theorem 3 or theorem 4 as well?
2. how many experts/ players are there in the training dataset? If they test theorem 4, do they train each member of the ensemble on games of a specific player each?
3. how do they encourage the condition of the disjoint sets in theorem 3?
4. Eq. 4 has a typo (the two terms are identical)?",MachineLearning,42,38,1721140041.0,1e4q1a8,South-Conference-395,https://www.reddit.com/r/MachineLearning/comments/1e4q1a8/r_discussion_on_the_paper_transcendence/,Research
"Implementing ""Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet"" paper for open source models.[D]","I recently came across an interesting paper titled ""Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet"" which explores using sparse autoencoders to extract interpretable features from the activations of a large language model. The methodology seems promising for gaining insights into the model's internal representations and behaviors.

It got me thinking about the feasibility of implementing similar interpretability techniques for open-source language models. Can we steer LLMs and their behaviour without having to extensively finetune.

I wanted to reach out to this community to discuss a few things:

1. Has anyone already implemented or experimented with similar interpretability techniques on open-source language models? Can we make something similar to that of golden gate claude??
2. Do you think it's feasible to adapt and scale these techniques to work with Llama,phi,mistral etc. These are much smaller in parameters size, when compared to sonnet.
3. I'm interested in collaborating with others who are passionate about this area of research. If you're working on interpretability for open-source models or have ideas for novel approaches, I would be excited to team up and explore this further. We could collaborate on implementing techniques, sharing resources, or brainstorming new ideas.

If you're interested in collaborating or have any ideas to share, please feel free to share.

",MachineLearning,44,15,1717300245.0,1d64lx8,No-Point1424,https://www.reddit.com/r/MachineLearning/comments/1d64lx8/implementing_scaling_monosemanticity_extracting/,Discussion
[D] Should the embedding matrix and final pre-softmax matrix be shared in transformers?,"Hi all,

When comparing various LLMs, one can see that some of them use the same matrix for the token embeddings and the transformation matrix in the end before the softmax is taken to get the predicted token probabilities. I found this paper from 2016 [Using the Output Embedding to Improve Language Models](https://arxiv.org/pdf/1608.05859) which suggests this is superior and also the [Attention Is All You Need](https://arxiv.org/pdf/1706.03762) paper references it and does this weight sharing. Same for other models such as GPT2 and Gemma.

That makes me wonder why the LLaMa models don't do this weight sharing. Is it worth it in terms of model capacity to have separate matrices there? Do models like Gemma necessarily have to use weight sharing because they use a huge vocabulary? I'd be interested in the trade-offs here and what's the current consensus for this topic, if there is any.",MachineLearning,42,10,1716901133.0,1d2iurw,CloudyCloud256,https://www.reddit.com/r/MachineLearning/comments/1d2iurw/d_should_the_embedding_matrix_and_final/,Discussion
[R] Faster inference: torch.compile vs TensorRT,"torch.compile outperforms TensorRT in terms of ease of use and performance in our tests on models like LLama-7b, LLama-3-8b, mistral-v0.1, phi-3, and phi-2. Unless you need TensorRT-specific features or work exclusively within NVIDIA's ecosystem, torch.compile is the better choice for optimizing PyTorch models.

[https://www.collabora.com/news-and-blog/blog/2024/12/19/faster-inference-torch.compile-vs-tensorrt/](https://www.collabora.com/news-and-blog/blog/2024/12/19/faster-inference-torch.compile-vs-tensorrt/)",MachineLearning,47,11,1734708730.0,1himai0,mfilion,https://www.reddit.com/r/MachineLearning/comments/1himai0/r_faster_inference_torchcompile_vs_tensorrt/,Research
[D] As a CS masters student/researcher should one be very deliberate in picking a lab’s domain?,"I (very fortunately) got an opportunity in a great lab in an R1 school, Prof has a >40 h-index, great record, but mainly published in lower tier conferences, though do some AAAI. It applies AI in a field that aligns with my experience, and we are expected to publish, which is perfect. However I’m more keen to explore more foundational AI research (where I have minimal experience in apart from courses I took).

In CS, ML it seems most people are only prioritising NIPS/ICLR/ICML especially since I’m interested in potentially pursuing a PhD. I’m in a bit of a dilemma, if I should seize the opportunity or keep looking for a more aligned lab (though other profs may not be looking for more students).

My gut tells me I should ignore conference rankings and do this, since they have some, chain of though, knowledge representation, cognitive system components. They expect multi semester commitment and of course once I commit I will see it through. My dilemma is that I’m moving more and more towards more practical applications in AI, which is pretty domain specific and am worried I won’t be able to pivot in the future. 

I’m aware how this can sound very silly, but if you can look past that, could I please get some advice and thoughts about what you’d do in the shoes of a budding academic, thank you!",MachineLearning,42,31,1732496209.0,1gz6mj1,giuuilfobfyvihksmk,https://www.reddit.com/r/MachineLearning/comments/1gz6mj1/d_as_a_cs_masters_studentresearcher_should_one_be/,Discussion
[D] What is the current state-of-the-art for discrete diffusion models? ,"Hi everyone,

I am currently working with Discrete Diffusion models for a new research project. In this project, I am applying Discrete Diffusion to a field where it has yet to be applied. However, I am quite new to diffusion itself, and I am overwhelmed by the number of papers published on the topic. In my current implementation, I focussed on an older [paper](https://arxiv.org/abs/2102.05379) since they described their approach quite well, and I wanted to test my idea first to see if it had some merit, which, according to initial results, it has.

Currently, I am looking at updating my method with more recent additions to this field, but as I said earlier, I am a bit overwhelmed by the amount. So my question to you is, what are good recent papers that looked into Discrete Diffusion that either explain essential concepts, such as survey papers, or that introduce new state-of-art methods that are not only applicable to a specific field, such as NLP or Vision?

Thank you in advance for your help.",MachineLearning,43,5,1732448112.0,1gyp1br,Derpirium,https://www.reddit.com/r/MachineLearning/comments/1gyp1br/d_what_is_the_current_stateoftheart_for_discrete/,Discussion
[P] Collection of SOTA TTS models,"As part of an ongoing project, I released what I think is the biggest collection of open-source voice-cloning TTS models here: [https://github.com/ttsds/datasets](https://github.com/ttsds/datasets)

I think it's very interesting how we haven't really reached a consensus on the rough ""best"" architecture for TTS yet, although I personally think audio token LLM-like approaches (with text prompts for style) will be the way forward.

https://preview.redd.it/2yru8a4oiu1e1.png?width=1249&format=png&auto=webp&s=73d48db7ce384e556e963385898c7f901d58c495

I'm currently evaluating the models across domains, will  be a more substantial post here when that's done :)

Edit: Also some trends (none of them surprising) that can be observed - we seem to be moving away from predicting prosodic correlates and training on only LibriVox data. Grapheme2Phoneme seems to be here to stay though (for now?)

Edit2: An older version of the benchmark with fewer models and only audiobook speech is available here: [https://huggingface.co/spaces/ttsds/benchmark](https://huggingface.co/spaces/ttsds/benchmark)",MachineLearning,45,6,1732016727.0,1guv9jl,cdminix,https://www.reddit.com/r/MachineLearning/comments/1guv9jl/p_collection_of_sota_tts_models/,Project
"[R] Classic GNNs (GCNs, GraphSAGEs, GATs) are Strong Baselines on Node Classification","We’re excited to share our recent paper ""[\[NeurIPS 2024\] Classic GNNs are Strong Baselines: Reassessing GNNs for Node Classification](https://arxiv.org/pdf/2406.08993).""

In this study, we conduct a thorough review of classic GNNs for node classification tasks. Our findings suggest that the superior performance often reported by state-of-the-art graph learning models may be due to suboptimal hyperparameter configurations in classic GNNs. By fine-tuning these hyperparameters, we show that classic GNNs outperform the latest models on 17 out of 18 widely used node classification datasets.

Code: [https://github.com/LUOyk1999/tunedGNN](https://t.co/QeNSn2D9CN)  
Arxiv: [https://arxiv.org/abs/2406.08993](https://t.co/MD4mVTnHk8)

If you find our work interesting, we’d greatly appreciate a ⭐️ on GitHub!",MachineLearning,41,3,1731213139.0,1gnsn54,luoyuankai,https://www.reddit.com/r/MachineLearning/comments/1gnsn54/r_classic_gnns_gcns_graphsages_gats_are_strong/,Research
[D] Demystifying distributed checkpointing,,MachineLearning,41,0,1730060757.0,1gdkdka,joygao,https://expertofobsolescence.substack.com/p/demystifying-distributed-checkpointing,Discussion
[Project] World's first autonomous AI-discovered 0-day vulnerabilities,"I'm sure a lot of people have found 0-day vulnerabilities by pasting code snippets into ChatGPT. The problem has always been scanning an entire project for 0-days. Some papers have shown it's possible by feeding their agents known vulnerable code, but as far as I know, none of those papers ever got any CVEs or found real 0-days. Vulnhuntr was released this weekend with more than a dozen 0-days discovered in open source projects of 10k+ GitHub stars:

[https://github.com/protectai/vulnhuntr](https://github.com/protectai/vulnhuntr)",MachineLearning,42,13,1729685243.0,1ga8wxn,FlyingTriangle,https://www.reddit.com/r/MachineLearning/comments/1ga8wxn/project_worlds_first_autonomous_aidiscovered_0day/,Project
[D] What are the pros and cons of using a VAE to provide a latent space for generative modelling? (especially for images or video),"I am of the opinion that the variational autoencoders (VAEs) are a hack to make certain kinds of  generative models (latent diffusion, latent consistency models, latent flow models) work with current hardware limitations.

But I also understand the point of some of my colleagues who say the compressed representation provided by VAEs force the generative model to be efficient, focusing on the factors that matter for correctly modelling the data distribution.

Current crop of state-of-the-art video generative models are almost all using some sort of compressed representation (generally a VAE). So, they work. But are they really necessary?

What is your take on this? Are VAEs a crutch? Or an essential part of generative models?",MachineLearning,42,35,1728569457.0,1g0jpzq,pm_me_your_pay_slips,https://www.reddit.com/r/MachineLearning/comments/1g0jpzq/d_what_are_the_pros_and_cons_of_using_a_vae_to/,Discussion
[R] I feel under-confident about the baselines I implemented. What do I do?,"I needed to implement 3 baseline RL algorithms, that have certain theoretical regret bounds. The original papers haven't provided any code of their own/and haven't done any simulations in their work. I don't feel confident about my implementations, particularly hyperparameter tuning since the environment we use is different. 

I tried my best to get the baselines to perform their best, by rigorously searching different params. It feels unethical to show our algorithm performs better, when theoretically, we are supposed to get comparable results. Their performance is quite dependent on hyperparams. What do I do?",MachineLearning,43,12,1727651593.0,1fshhor,Replay0307,https://www.reddit.com/r/MachineLearning/comments/1fshhor/r_i_feel_underconfident_about_the_baselines_i/,Research
[R] Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model,"Transfusion unifies text and image generation in a single model, rivaling specialized architectures.",MachineLearning,43,2,1724387839.0,1ez422y,AhmedMostafa16,https://arxiv.org/abs/2408.11039v1,Research
[R] CFG++ : A simple fix for addressing the flaws of CFG in diffusion models,"The classifier-free guidance (CFG)  is widely used for text-guidance in diffusion models, but  notorious for its challenges, such as difficulty in DDIM inversion and ambiguity in selecting a large guidance scale.

This paper demonstrates that these limitations of CFG stem from inherent design flaws in the original CFG, and introduce CFG++, a simple yet powerful fix  in the \**re\**nosing process. This adjustment facilitates smaller guidance scales, significantly improved invertibility, and much better alignment between images and text.

Project page: [https://cfgpp-diffusion.github.io/](https://cfgpp-diffusion.github.io/)

Github: [https://github.com/CFGpp-diffusion/CFGpp](https://github.com/CFGpp-diffusion/CFGpp)

Paper: [https://arxiv.org/abs/2406.08070](https://arxiv.org/abs/2406.08070)

https://preview.redd.it/lph7aufcvn6d1.png?width=854&format=png&auto=webp&s=5e4bfeb1af9563bb30b29848386f02b11f13fbdc

  
",MachineLearning,43,4,1718424973.0,1dg9mvc,Fit_Entrepreneur_588,https://www.reddit.com/r/MachineLearning/comments/1dg9mvc/r_cfg_a_simple_fix_for_addressing_the_flaws_of/,Research
XGBoost: Preffered Method of Feature Selection? [D],"Method 1 - Shap: Drop features with mean absolute shap value below a certain value

Method 2 - Feature Importance: Drop features with feature importance values below a certain value

Method 3 - R squared: Drop each feature individually from the model and calculate the resulting R2 score for each seperate model. Features which don't add significantly to the R2 score should be dropped

Method 4 - Keep all features and let XGBoost sort it out

What are you opinions on the relative efficacy of these methods and any other methods you like to you use, specifically for XGBoost?",MachineLearning,45,23,1716823387.0,1d1u0yd,Gef_1_Man_Army,https://www.reddit.com/r/MachineLearning/comments/1d1u0yd/xgboost_preffered_method_of_feature_selection_d/,Discussion
[D] Paperswithcode relevant? ,"I feel like paperswithcode became less relevant for tracking progress in ML in general for me.

But it’s hard to say, in my field (tabular ML/DL) there are not many established academic benchmarks (no need for something like papers with code yet)

In NLP and foundation model space leaderboards in hf spaces became a thing (mostly in NLP).

Overall, paperswithcode just feels less maintained and less useful.

Do you use paperswithcode often? What do you use it for? What’s your field where it is useful?",MachineLearning,45,21,1716493608.0,1cz1t4x,_puhsu,https://www.reddit.com/r/MachineLearning/comments/1cz1t4x/d_paperswithcode_relevant/,Discussion
"[D] In Byte Latent Transformer, how is the decoded patch boundary determined?","In Meta’s recent paper Byte Latent Transformer, I understand that the local encoder model uses the patch segmentation method (e.g. the entropy based method) to cut patches first and then for each patch, cross attention will attend to the bytes in that batch (since the patch boundaries are already determined). However, how does decoding work in this case? Is it that when each byte is being decoded, it is assumed to be in the latest patch, and if the new output byte is detected as a new patch boundary (e.g. using the entropy based method), it cuts a new patch and future bytes now belong to this patch? If this is the case, won’t the starting byte of each output patch be effectively decoded using the previous patch? Or is it that, when the new boundary is found, this byte is discarded, a new patch is started, and its starting byte is decoded again using this new patch? I am not sure if the author explicitly mentioned this in the paper.",MachineLearning,43,27,1735060779.0,1hli20i,TommyX12,https://www.reddit.com/r/MachineLearning/comments/1hli20i/d_in_byte_latent_transformer_how_is_the_decoded/,Discussion
[R] Amazon Researchers Find LLMs do not always follow User Requests and Propose a Self-Correction Pipeline,"Came across this interesting paper being presented next week at EMNLP 2024: *LLM Self-Correction with DECRIM: DECOMPOSE, CRITIQUE, AND REFINE for Enhanced Following of Instructions with Multiple Constraints*.

This study dives into an important question: **Do LLMs really do what we ask them to?** We often rely on LLMs for tasks with specific instructions, but when these instructions get complex and multi-constrained, like requesting specific tones or avoiding certain words, do LLMs actually follow through? This paper suggests that the answer might be more complicated than we think.

The authors created a new benchmark, RealInstruct, which uses real-world user instructions rather than synthetic prompts. **They estimated that at least 30% of real user requests contain multiple constraints that LLMs must follow**. In their results **even advanced models like GPT-4 fail to meet at least one requirement over 21% of the instructions tested**. So, while LLMs perform well in simple cases, their performance drops when handling more intricate, multi-step requests.

To address these gaps, the authors developed a self-correction pipeline called DECRIM, where the model breaks down each instruction, checks its response against each requirement, and iteratively refines it as needed. Through DECRIM, open-source models like Mistral saw notable improvements, even surpassing GPT-4 on the benchmarks. **Initial tests showed that LLMs couldn’t self-correct reliably alone**, however with weak but minimally reliable auxiliary feedback, **they achieved up to an 8% boost**. **With high-quality “ideal” feedback, DECRIM brought Mistral’s performance up by 34%, surpassing GPT-4 on both RealInstruct and IFEval benchmarks.**

I think this paper fits in a new trend on LLMs, these System 2 Reasoning models like GPT-o1 that try to mimic some thinking / reflection before outputting their response. Anyway it is shocking that LLMs perform that bad in a task that seems simply the most important ones for the user, following what the users ask. Is this type of model making us closer to AGI? Or is this just proving that this magic AGI that some people talk about is actually much much far away yet? 

Paper: [https://arxiv.org/pdf/2410.06458](https://arxiv.org/pdf/2410.06458)

[Their post on Linkedin](https://www.linkedin.com/posts/thomasferraz_emnlp2024-ai-llms-activity-7259680754299731968-uLBk?utm_source=share&utm_medium=member_desktop)

https://preview.redd.it/techjo8pfazd1.png?width=2794&format=png&auto=webp&s=18155cdbf4ba164f48480d4583c3cfea1d40298e

",MachineLearning,45,3,1730902001.0,1gkzac4,Mundane_Sir_7505,https://www.reddit.com/r/MachineLearning/comments/1gkzac4/r_amazon_researchers_find_llms_do_not_always/,Research
The kernel trick (RKHS) applied to logic: Logical Properties and Quantiﬁers in a Semantic Space Framework,,MachineLearning,42,11,1728707598.0,1g1sfz2,musescore1983,https://www.academia.edu/124637124/Logical_Properties_and_Quantifiers_in_a_Semantic_Space_Framework_working_draft_,None
[R] Theoretical limitations of generalization bounds,"**tl;dr: there are fundamental limitations on how tight generalization bounds can be.**

Though there have been many newly proposed generalization bounds in recent years, a common theme is that they are numerically loose (or even vacuous) when evaluated in practical settings (i.e. realistically sized models, standard datasets). This severely limits their utility as performance guarantees and their impact on practical algorithmic design.

Is this observed gap between theory and practise merely an artefact of loose proof techniques, or are there also fundamental statistical limitations on how tight such bounds can be? *We find that, in many settings, the latter is the case!*

**Paper 1 (published in ICLR ’24)** [**https://arxiv.org/abs/2309.13658**](https://arxiv.org/abs/2309.13658) **:**

* Bounds that are not tailored to specific algorithms are necessarily loose for many algorithm-distribution combinations.
* In rich enough learning settings, algorithm-dependent bounds are subject to an uncertainty principle: one can either learn the target distributions well, or verify the success of learning — never both!

**Paper 2 (recent preprint)** [**https://arxiv.org/abs/2410.01969**](https://arxiv.org/abs/2410.01969) **:**

* We show that algorithms that have certain inductive biases that cause them to be unstable do not admit tight generalization bounds.
* Next, we show that algorithms that are sufficiently stable do have tight generalization bounds.

We think that our findings could be of interest to many members of the community broadly interested in generalization.

Happy to discuss — questions, feedback, and criticism are all welcome :)",MachineLearning,45,14,1728123769.0,1fwnb1r,zweihander___,https://www.reddit.com/r/MachineLearning/comments/1fwnb1r/r_theoretical_limitations_of_generalization_bounds/,Research
[d] Practical example of ReFT: Representation Finetuning done on Llama3 in 14 minutes,"A couple weeks ago, Arxiv ReFT paper first author Zhengxuan Wu collab'd with [Oxen.AI](http://Oxen.AI) CEO Greg Schoeninger on a dive into ReFT: Representation Finetuning.

In tomorrow's (Friday's) AI Water Cooler, Oxen Intern Eric will present:  
*""How I fine-tuned Llama3 in 14 minutes w/ ReFT""*

TLDR of ReFT:  Instead of fine-tuning by parameters, **insert a representation into the hidden states** to guide the model.

Eric will show a practical implementation of the earlier Arxiv Dive.

Helpful deets:

* AI Water Coolers are less formal, UNrecorded spaces for deep tech conv.
* Friday August 09, at 10:00 AM Pacific
* Recurring cal invite: [https://oxen.ai/community](https://oxen.ai/community)
* YouTube [https://youtu.be/to2oKwnknUk?si=LmMMYxoryOn0UCwh](https://youtu.be/to2oKwnknUk?si=LmMMYxoryOn0UCwh)
* Arxiv Paper link: [https://arxiv.org/pdf/2404.03592](https://arxiv.org/pdf/2404.03592)",MachineLearning,42,3,1723158601.0,1enk8wm,ReluOrTanh,https://www.reddit.com/r/MachineLearning/comments/1enk8wm/d_practical_example_of_reft_representation/,Discussion
[D] why do majority of nlp models are decoder only models?,"I have noticed that a lot of major nlp models are decoder only models, can anyone tell the reason behind it?",MachineLearning,43,27,1722790642.0,1ejzz90,jiraiya1729,https://www.reddit.com/r/MachineLearning/comments/1ejzz90/d_why_do_majority_of_nlp_models_are_decoder_only/,Discussion
[P] From Unlabeled Data to Rich Segmentation: The Magic of Self-Supervised Models,"I've been experimenting with finetuning the DINOv2 ViT weights from Facebook Research for image segmentation. These DINOv2 encoder weights are pre-trained through self-supervised learning and can be easily finetuned using Low-Rank Adaptation (LoRA) and simple decoders like 1x1 convolutional decoders or Feature Pyramid Networks (FPN). I achieved solid validation IoU scores: \~62% on ADE20k and \~85% on Pascal VOC with 30-50 epochs of finetuning.

I also created a Jupyter Notebook with a detailed description of how these DINOv2 models achieve their semantic richness.

Github: [https://github.com/RobvanGastel/dinov2-finetune?tab=readme-ov-file](https://github.com/RobvanGastel/dinov2-finetune?tab=readme-ov-file)  
Colab: [https://colab.research.google.com/github/RobvanGastel/dinov2-finetune/blob/main/Explanation.ipynb](https://colab.research.google.com/github/RobvanGastel/dinov2-finetune/blob/main/Explanation.ipynb)

",MachineLearning,41,14,1720718249.0,1e0u9sx,Quiet_Grab1112,https://www.reddit.com/r/MachineLearning/comments/1e0u9sx/p_from_unlabeled_data_to_rich_segmentation_the/,Project
[Discussion] ECCV decisions out! (+Borderline paper support thread),"https://eccv2024.ecva.net/Conferences/2024/AcceptedPapers

We were accepted with initial reviews of WA/WA/WR and I nearly threw up when I saw my ID listed. It's been a nerve wracking couple months!

How did you all do?

And much love to all the borderline paper havers who are looking up their results! It's a completely random process for us at the borderlines!",MachineLearning,44,55,1719845319.0,1dsutwd,impatiens-capensis,https://www.reddit.com/r/MachineLearning/comments/1dsutwd/discussion_eccv_decisions_out_borderline_paper/,Discussion
[R] DiTTo-TTS: Efficient and Scalable Zero-Shot Text-to-Speech with Diffusion Transformer,,MachineLearning,42,7,1718682688.0,1dihfqu,keonlee9420,https://arxiv.org/abs/2406.11427,Research
[D] How Do You Efficiently Conduct Ablation Studies in Machine Learning?,"When conducting ablation studies for a model that can be pretrained and fine-tuned, do you perform a full grid search for each ablated version during both pretraining and fine-tuning? Or do you have strategies to make this process more efficient? Thank you for your insights.",MachineLearning,42,12,1716126872.0,1cvoten,Few-Pomegranate4369,https://www.reddit.com/r/MachineLearning/comments/1cvoten/d_how_do_you_efficiently_conduct_ablation_studies/,Discussion
[R] Why can Llama-3 work with 32K context if it only had 8K context length?,"Hello folks! See post here: [https://twitter.com/abacaj/status/1785147493728039111](https://twitter.com/abacaj/status/1785147493728039111)

I didn't understand what he meant by ""with zero-training (actually just a simple 2 line config) you can get 32k context out of llama-3 models""

Does someone know what this **dynamic scaling trick** is? Much appreciated! :)",MachineLearning,42,8,1714974189.0,1clbmz2,None,https://www.reddit.com/r/MachineLearning/comments/1clbmz2/r_why_can_llama3_work_with_32k_context_if_it_only/,Research
[P] Introducing LongTalk-CoT v0.1: A Very Long Chain-of-Thought Dataset for Reasoning Model Post-Training,"I’m excited to release [LongTalk-CoT v0.1](https://huggingface.co/datasets/kenhktsui/longtalk-cot-v0.1), a dataset designed for post training o1-like reasoning model. Each response is prompted using QwQ-32B-Preview, and specifically handcrafted system message that encourages **more vocalised thinking**, and **self reflection**.

* post-training dataset contains **97M tokens** (using meta-llama/Llama-3.1-8B-Instruct tokenizer).
* output token length is **5.29x longer** than HuggingFaceTB/smoltalk 🤔💭
* boosting performance in [**ProcessBench**](https://huggingface.co/papers/2412.06559)
* can be used for SFT and RL/ Preference Optimisation
* finetuned model able to solve Is 9.11 greater than 9.9 and How many letters R in the word strawberry!",MachineLearning,42,3,1735571464.0,1hpp8ph,transformer_ML,https://www.reddit.com/r/MachineLearning/comments/1hpp8ph/p_introducing_longtalkcot_v01_a_very_long/,Project
[R] Representation power of arbitrary depth neural networks,"Is there any theorem that discusses the representation power of neural networks with fixed hidden layer sizes but arbitrary depth?

I am especially interested in the following case:  
suppose I am using a neural network to construct a vector-valued function `f` that maps scalar `t` to 2-dim vector `v`. f: t-> v.

And this is done using only hidden layers of size 2.

I want to know if there is any theorem that guarantees that any function `f` of the above form can be approximated by a neural network given that it has sufficient depth.",MachineLearning,39,7,1735012696.0,1hl5918,atharvaaalok1,https://www.reddit.com/r/MachineLearning/comments/1hl5918/r_representation_power_of_arbitrary_depth_neural/,Research
[D] Training on Petabyte scale datasets,"Lets say we have a dataset that is much larger than we have disk storage. For example:

* Dataset: 1PB
* Our disk storage: 10TB
* GPU RAM: 8x80GB (not super relevant to this discussion)

What are the usual approaches to training on something like this? What I can think of intuitively is to do the following in parallel somehow:

\- prefetch block n, train on block n-1, delete block n-2 from disk

Lets say we use PyTorch, so we have a PyTorch Dataset that has all the paths to where the data is stored in the cloud. Do we need to write code for the prefetcher/deleter that downloads from the cloud and store on disk and have it run in a separate process, then have a DataLoader for training that just assumes that it can read from disk (because the prefetcher does its job correctly)? Having the DataLoader read from S3 would be bad for GPU utilization, right?

To take a step back, I'm assuming that this is ordinary and often occuring ""problem"" for every company that trains on large datasets, so I'm skeptical to writing all of this code by myself as I feel like there should be standard out of the box solutions for this, but can't really find anything that matches perfectly.",MachineLearning,44,30,1731090426.0,1gmpedb,lapurita,https://www.reddit.com/r/MachineLearning/comments/1gmpedb/d_training_on_petabyte_scale_datasets/,Discussion
[D] How could the new Claude Sonnet 3.5 provide precise coordinates?,"Not sure if this has been asked before, but recent release of Claude Sonnet has surprised me. A few months ago, I tried many LLMs to provide me the (x, y) coordinates on the screenshot using various methods like grid location, marked coordinates etc. but the accuracy was not sufficient. However; this new model can actually provide very accurate coordinates. Does anyone know/Can we guess the system they are using for something like this? Could they be using some other model like SeeClick?",MachineLearning,40,27,1729737104.0,1gasb94,super_deap,https://www.reddit.com/r/MachineLearning/comments/1gasb94/d_how_could_the_new_claude_sonnet_35_provide/,Discussion
[R] Dealing with paper reproductions ,"Hello, I’m currently a 1st year PhD student in computer vision, and I’ve been facing some challenges with paper reproduction during my group meetings. The issue I’m dealing with is that the papers I’m reproducing are often extensions of other papers, which in turn are built on even older work. When I present my results, my advisor often asks a lot of detailed questions, sometimes about the history or finer details of the model, and it’s easy for me to get confused.

I usually don’t have time to go back and fully understand the math or optimizations in older papers in a week (I take 3 courses with research), and it becomes overwhelming when I’m asked to explain them. Sometimes, I end up talking too much or too little and feel embarrassed afterward. The thing is, I’m really interested in the topic but just don’t have time to dive deep into every aspect while reproducing these models, although I looked into the fragments after the meeting. Has anyone else faced something similar?

1. How do you handle reproducing papers that have a long chain of extensions? For instance, training from scratch (situation when docker images are not available)
2. How do you deal with detailed technical questions in meetings/presentations when you only have a surface knowledge of the older work?
3. Any tips for balancing understanding with time management when it comes to reproducing results and fine-tuning models?

I appreciate your thoughts or any strategies you’ve found helpful in situations like this. Thanks in advance!",MachineLearning,40,9,1727823441.0,1fu1n9y,Cool-Economy3492,https://www.reddit.com/r/MachineLearning/comments/1fu1n9y/r_dealing_with_paper_reproductions/,Research
[R] An Empirical Study of Mamba-based Language Models (8B Mamba-2-Hybrid on 3.5T tokens data),"Link: [http://arxiv.org/abs/2406.07887](http://arxiv.org/abs/2406.07887)

>Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache. Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative. In a controlled setting (e.g., same data), however, studies so far have only presented small scale experiments comparing SSMs to Transformers. To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also compare these models to a hybrid architecture consisting of 43% Mamba-2, 7% attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks, we answer the question of whether Mamba models can match Transformers at larger training budgets. Our results show that while pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook) or long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points on average) and is predicted to be up to 8x faster when generating tokens at inference time. To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequences. On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average. To enable further study, we release the checkpoints as well as the code used to train our models as part of NVIDIA's Megatron-LM project.",MachineLearning,43,3,1720331551.0,1dx9ggp,ghosthamlet,https://www.reddit.com/r/MachineLearning/comments/1dx9ggp/r_an_empirical_study_of_mambabased_language/,Research
[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",MachineLearning,43,41,1720318510.0,1dx5tpo,AutoModerator,https://www.reddit.com/r/MachineLearning/comments/1dx5tpo/d_selfpromotion_thread/,Discussion
"[D] - AMD MI300X and Nvidia H100 benchmarking in FFT: VkFFT, cuFFT and rocFFT comparison","Hello, I am the creator of the [VkFFT](https://github.com/DTolm/VkFFT/) - GPU Fast Fourier Transform library for Vulkan/CUDA/HIP/OpenCL/Level Zero and Metal. There are not that many independent benchmarks comparing modern HPC solutions of Nvidia  (H100 SXM5) and AMD (MI300X), so as soon as these GPUs became available on demand I was interested in how well they can do Fast Fourier Transforms - and how vendor libraries, like cuFFT and rocFFT, perform compared to my implementation.

On-demand rent is quite pricey, so these initial results only include 1D batched power of 2 complex-to-complex FFTs in single and double precision. This benchmark is usually memory-bound on GPUs, meaning that most of the time is spent utilizing the VRAM bus and transferring data from the VRAM to the chip (batch size is chosen big enough to reduce cache reuse and utilize all compute units). I use estimated bandwidth as a benchmark metric, which is calculated as (2 x System size \[GB\]) / execution time \[s\]. A factor of two is there because we need to upload data and download it from the chip. So for memory-bound code, this value should be close to the memory bandwidth of the device.

https://preview.redd.it/ngv6qqxvbd7d1.png?width=4500&format=png&auto=webp&s=d4bdc8893462561f307e758cafb10e3f76636174

In single precision, both GPUs have similar results - around 3TB/s bandwidth for the single-upload FFT algorithm. After approximately 2\^14 (implementation dependent) all libraries switch to the two-upload (and two-download) FFT algorithm resulting in 2x memory transfers and, subsequently, 2x bandwidth drop. Switch to the 3-upload happens around 2\^24. Overall, both GPUs are not quite at their theoretical bandwidths (3.35TB/s for H100 and 5.3TB/s for MI300X), but it is common to have actual values lower than specification. For AMD MI300X there is also an inconsistency in results for small sizes, likely due to the need for more optimization for the new multiple-chip design and the presence of an L3 cache. The current VkFFT version (optimized for previous generation hardware) matches and often outperforms vendor solutions for the highly optimized case of powers of 2.  


https://preview.redd.it/9q0wt6m3cd7d1.png?width=4500&format=png&auto=webp&s=19644a0945cd9f4a6acd4172d956c467bca94856

Double precision results scale similarly to single precision. AMD MI300X achieves a higher base bandwidth here than in single-precision, I am not exactly sure why yet (maybe a 1:1 FP64:FP32 core ratio comes in handy).

VkFFT is also highly optimized for non-power-of-2 cases, so it should perform well with them on the new hardware. You can find the implemented algorithms description and the full performance comparison of the previous HPC GPUs generation in the VkFFT [paper](https://ieeexplore.ieee.org/document/10036080). I will tune the code for the new GPUs once I solve the issues with access costs for extensive testing.

Overall, MI300X is competitive with H100 and it looks like AMD improved on many issues of previous generations of CDNA (namely memory pin serialization for distant coalesced accesses). It seems that each compute unit is still weaker than the respective streaming multiprocessor - it has smaller and slower shared memory/L1 and L2 caches, however, it is offset by having the L3 cache and new multi-chip design (connecting 304 compute units), the impact of which is to be estimated. Thank you for reading, and if you have questions about VkFFT or the testing procedure - I will be happy to answer them.

",MachineLearning,38,3,1718744758.0,1dj1ixf,xdtolm,https://www.reddit.com/r/MachineLearning/comments/1dj1ixf/d_amd_mi300x_and_nvidia_h100_benchmarking_in_fft/,Discussion
[P] Simplified PyTorch Implementation of AlphaFold 3,,MachineLearning,41,16,1716158905.0,1cw0n8b,csozboz,https://github.com/ogchen/nanofold,Project
[P] I created a cloud provider for affordable & easy GPU access,"Hello r/MachineLearning!

I’m thrilled to introduce [Backprop GPU Cloud](https://backprop.co)—after three years of hosting GPU servers on public marketplaces I decided to build my own cloud to offer a better service.

I've focused on speed, price, and reliability:
- Instances are created in <60s with Jupyter pre-installed.
- The pricing is reasonable with no hidden fees on storage or bandwidth.
- The RTX 3090 instances are hosted in a tier III data center with 10 Gbps networking.
- You get a virtual machine with full root access and a dedicated IPv4 address.

If you're a student or a researcher, I'm happy to give you 10 hours of free credit. Just sign up and shoot me a message.

I'm looking to add more features and additional instance types. All feedback would help a ton!",MachineLearning,40,5,1714332069.0,1cfek96,ojasaar,https://www.reddit.com/r/MachineLearning/comments/1cfek96/p_i_created_a_cloud_provider_for_affordable_easy/,Project
[D] What are some popular open-ended problems in mechanistic interpretability of LLMs?,"Hi everyone, I am quite familiar with LLMs and its research. I am interested in mechanistic interpretability and am starting out to work on this field. Being new to mech interp, and planning to do my PhD in this field, what are some of the popular open ended problems in the field I should start exploring? Would love to hear insights from interpretability researchers here.",MachineLearning,44,12,1735246934.0,1hmxxwf,arinjay_11020,https://www.reddit.com/r/MachineLearning/comments/1hmxxwf/d_what_are_some_popular_openended_problems_in/,Discussion
[D] A collection of various LLM Sampling methods,"In the last couple months, I read about various algorithms to perform LLM sampling. I decided to build my own inference stack and implement those algorithms. 

Here is the Github repo - [https://github.com/shreyansh26/LLM-Sampling](https://github.com/shreyansh26/LLM-Sampling)

The repo includes implementations for Top-k, Top-p (nucleus), Min-p, Typical, Epsilon, Eta, Beam search, Chain-of-Thought (CoT) decoding, Constrained JSON decoding and Speculative decoding.

Personally, I found this to be a good learning experience. Sharing here in case it helps someone!",MachineLearning,43,7,1733650757.0,1h9fe8q,shreyansh26,https://www.reddit.com/r/MachineLearning/comments/1h9fe8q/d_a_collection_of_various_llm_sampling_methods/,Discussion
What's the best Open Source Image-Upscaling Model? [Discussion],"I'm using [Playground-v2.5-aesthetic](https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic) to make some images for YouTube thumbnails. I'm really happy with the results:

[1024x1024 base image of mars base.](https://preview.redd.it/uuo4sdgwp44e1.png?width=1024&format=png&auto=webp&s=84d61bf4d7fbf2457df1037e95603166390efa12)

But I would like the image to be 1920x1080 pixels, and my only options are 1024x1024, or 1280x720 pixels. At the moment, I can get to 1920x1080 with Photoshop's outpainting:

[1920x1080 outpainted image of mars base.](https://preview.redd.it/07tt5ix4q44e1.jpg?width=1920&format=pjpg&auto=webp&s=cdfb7cefa8d2bb4f187d2ab6b86aaba17596506a)

This is okay, but photoshops outpainting is manual and has a fairly significant quality drop. Ideally, I would generate an image in 1280x720 then upscale to 1920x1080 programmatically.

I've heard of the following models:

* Real-ERSGAN
* Waifu2
* SRGAN

But before I jump into any of them, what open-source model is generally considered best to achieve this? I have an RTX 3060 12GB of VRAM.",MachineLearning,41,23,1733012037.0,1h3qcon,FPGA_Superstar,https://www.reddit.com/r/MachineLearning/comments/1h3qcon/whats_the_best_open_source_imageupscaling_model/,Discussion
[R] Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution,"Text Diffusion Models now finally reached the text quality of GPT2. [https://arxiv.org/abs/2310.16834](https://arxiv.org/abs/2310.16834)  
(This paper won the ICML2024 best paper award!)

Do you think diffusion language models (diffusion LLMs) will catch up to autoregressive LLMs and potentially become the next ChatGPT? Could we soon see scaling laws for diffusion LLMs? These models have some key advantages over autoregressive LLMs, such as the ability to accept prompts anywhere—in the beginning, middle, end, or even split across the input. Additionally, they can, in principle, generate multiple tokens at once.

The paper is quite dense and math heavy, so I've made an animated explainer video, for anyone interested. [https://youtu.be/K\_9wQ6LZNpI](https://youtu.be/K_9wQ6LZNpI)

My take: I think this approach could theoretically scale, but there's a significant challenge: We've already invested heavily in hardware and software optimizations for GPTs / autoregressive transformers. Given the sunken cost fallacy, it's hard to imagine tech giants abandoning their current LLMs to start training diffusion LLMs, especially since it could take years for them to catch up to ChatGPT and similar models. Much like MAMBA, I fear discrete diffusion might also lose the hardware/software lottery.",MachineLearning,40,7,1724481159.0,1ezyunc,AICoffeeBreak,https://www.reddit.com/r/MachineLearning/comments/1ezyunc/r_discrete_diffusion_modeling_by_estimating_the/,Research
[R] LayerMerge: Neural Network Depth Compression through Layer Pruning and Merging (ICML 2024),"**Paper:** [https://arxiv.org/abs/2406.12837](https://arxiv.org/abs/2406.12837)

**Code:** [https://github.com/snu-mllab/LayerMerge](https://github.com/snu-mllab/LayerMerge)

**TL;DR:** LayerMerge reduces depth of the CNN and diffusion models by pruning and merging convolution and activation layers.

[Qualitative example of LayerMerge](https://preview.redd.it/oww7pgltjuid1.png?width=3034&format=png&auto=webp&s=99f614992132d12c53fe1d8cb1193f7e051a2f68)

**Overview:** LayerMerge is a novel method to make convolutional neural networks more efficient without losing performance. Traditional methods for reducing network depth usually follow one of two approaches:

*1. Pruning Convolution Layers:* Aggressively removes parameters, risking loss of important information.

*2. Pruning Activation Layers and Merging Layers:* Eliminates redundant activation layers and merges resulting consecutive convolution layers, potentially increasing kernel sizes and negating speed gains.

LayerMerge addresses these issues by *jointly pruning convolution layers and activation functions.* It optimizes which layers to remove, speeding up inference while minimizing performance loss. Since this selection process involves an exponential search space, we formulate a novel surrogate optimization problem and efficiently solve it via *dynamic programming*.

Our results show that LayerMerge outperforms current methods for reducing network depth in tasks including *image classification and generation.*

[Demo showing the effectiveness of LayerMerge with MobileNetV2-1.0 on ImageNet and with DDPM on CIFAR10.](https://preview.redd.it/dwyll66vjuid1.png?width=2868&format=png&auto=webp&s=39c4afd5c8d6398910ddd132b121d64fd281e54c)

  
",MachineLearning,42,1,1723735975.0,1esy876,jusjinuk,https://www.reddit.com/r/MachineLearning/comments/1esy876/r_layermerge_neural_network_depth_compression/,Research
[D] AI/ML in big tech vs biotech ,"I'm curious why a strong ML engineer would leave a big tech firm (like Google, Microsoft or OpenAI) and work for biotech company. What is the appeal to biotech versus all the cutting edge innovation happening in tech companies?",MachineLearning,38,34,1723007598.0,1em3ke2,Pleasant_Wish1799,https://www.reddit.com/r/MachineLearning/comments/1em3ke2/d_aiml_in_big_tech_vs_biotech/,Discussion
[P] How much VRAM I need to train llama 3 8B?,"Hello,

I assume very noob question, but can not find an answer.

I want to take llama 3 8b and enhance model with my custom data. 

I want to do both training and run model locally, on my Nvidia GPU.

I don't have GPU now, only mac m2 pro 16Gb,  and need to know what to purchase.

I wonder, what are the VRAM requirements? Would I be fine with 12 GB, or I need to get gpu with 16? Or only way is 24 GB 4090 like stuff?",MachineLearning,38,28,1717922000.0,1dbp2sz,webdunesurfer,https://www.reddit.com/r/MachineLearning/comments/1dbp2sz/p_how_much_vram_i_need_to_train_llama_3_8b/,Project
[P] A post on regularization properties of polynomial features in machine learning,"I wrote a post about the regularization properties of polynomial features in machine learning - things like bias-variance tradeoff, and controlling the shape of the fit curve.  This is the last post in a series about polynomial features. I certainly enjoyed learning everything I wrote about, and I hope it will be interesting and useful.

Series begins here: [https://alexshtf.github.io/2024/01/21/Bernstein.html](https://alexshtf.github.io/2024/01/21/Bernstein.html)

The latest post here: [https://alexshtf.github.io/2024/06/03/PolynomialBasesRegProps.html](https://alexshtf.github.io/2024/06/03/PolynomialBasesRegProps.html)",MachineLearning,40,3,1717442765.0,1d7d26z,alexsht1,https://www.reddit.com/r/MachineLearning/comments/1d7d26z/p_a_post_on_regularization_properties_of/,Project
[D] KAN == multi-layer GAM ?,"I just read the KAN paper, 

My understanding is that it provides a solution on how to stack multiple layers of GAMs (Generalized Additive Model): The Phi function is just the shape function of a GAM, and splines are well studied shape functions in GAMS.

So to me:

- MLP is a multi-layer linear regression  
- KAN is a multi-layer GAM

Still, a GAM has a link function than is not expressed in the KAN paper, but to me it looks like this is the real point of the paper. If we add an activation function to a KAN layer, then we fully have a multi-layer GAM.

This also means that we can consider MLP as a special case of a KAN because linear-regression is a special case of a GAM.

Does this sound correct?",MachineLearning,40,5,1717138932.0,1d4pjxp,mainro12,https://www.reddit.com/r/MachineLearning/comments/1d4pjxp/d_kan_multilayer_gam/,Discussion
[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video),"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties/results of KANs like continual learning, sparsification, symbolic regression etc.

Link here: [https://youtu.be/7zpz\_AlFW2w](https://youtu.be/7zpz_AlFW2w)",MachineLearning,40,25,1715711769.0,1crzj4o,AvvYaa,https://www.reddit.com/r/MachineLearning/comments/1crzj4o/d_kolmogorov_arnold_networks_a_visual_paper/,Discussion
[D] How does fast inference work with state of the art LLMs?,"I’ve read that inference speed for models like Llama-2 70B is ~10 t/s at best. So that left me wondering how the extremely large models like GPT-4 (1T params?) do their fast 20 t/s inference. With 10x the params, they gotta have at least 3x the layers(?) So that should make its inference much slower. Am I missing anything? What kind of further improvements might these companies be doing to power their fast APIs?

Edit: I must mention that you cannot parallelize across GPUs to help with latency of a single example when the data has to pass through model layers sequentially.

And with the large model sizes, model parallelism, with its inter-GPU communication should make it even slower…",MachineLearning,39,35,1715060200.0,1cm4h4i,Fit-Flow-4180,https://www.reddit.com/r/MachineLearning/comments/1cm4h4i/d_how_does_fast_inference_work_with_state_of_the/,Discussion
[D] What are some problems you guys are working on?,"Hey guys, I’m a graduate master’s student majoring in Machine Learning. Winter break is coming up, and I’m gonna be spending Christmas alone 😃. I’ve got some spare time and access to a few A100s, so I’m planning to work on a project.

I’m curious to know what kind of problems you guys are working on! Need someone to help out or wish someone could solve a problem you have? I maybeeee can spare my winter to work on it!

Please share any problem statements you’re working on or wish to tackle. Also, if you work in the industry and know what kinds of problems would help me stand out, that advice would be super appreciated too :)
",MachineLearning,39,43,1731377278.0,1gp9ydh,ziggyboom30,https://www.reddit.com/r/MachineLearning/comments/1gp9ydh/d_what_are_some_problems_you_guys_are_working_on/,Discussion
[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",MachineLearning,39,42,1729995312.0,1gd0v8r,AutoModerator,https://www.reddit.com/r/MachineLearning/comments/1gd0v8r/d_selfpromotion_thread/,Discussion
[D] Understanding 1.58-bit Large Language Models,"Hi all, I wrote an [article about ternary (trinary) models](https://medium.com/@arunnanda/understanding-1-58-bit-large-language-models-88373010974a) to summarize what I read on the topic. It's more like a high level literature review. The target audience is the developer who is curious about the research but doesn't have the time to dig into the papers themselves. It is a free article on Medium. 

  
Appreciate any feedback and inputs on it. ",MachineLearning,38,14,1726989564.0,1fmnkar,ahronorha,https://www.reddit.com/r/MachineLearning/comments/1fmnkar/d_understanding_158bit_large_language_models/,Discussion
[D] Implementing papers worth?,"Hello all,

I have a masters in robotics (had courses on ML, CV, DL and Mathematics) and lately i've been very interested in 3D Computer Vision so i looked into some projects. I found deepSDF. My goal is to implement it on C++, use CUDA & SIMD and test on a real camera for online SDF building.

Also been planning to implement 3D Gaussian Splatting as well.

But my friend says don't bother, because everyone can implement those papers so i need to write my own papers instead. Is he right? Am i losing time?",MachineLearning,39,21,1725904044.0,1fcvkjm,Huge-Leek844,https://www.reddit.com/r/MachineLearning/comments/1fcvkjm/d_implementing_papers_worth/,Discussion
[D] Why aren't reviewers required to respond to rebuttals?,"Hi,

I recently submitted to a conference, and was curious as to why reviewers aren't required to acknowledge  rebuttals. Obviously, reviewers tend to have a busy schedule, and a detailed response is often challenging. But I don't understand why it isn't required for reviewers to atleast address rebuttals (even with something as simple as ""Thanks for the reply!"" or ""I appreciate the additional info, and am updating my score to reflect it"")",MachineLearning,39,32,1724956747.0,1f49zq2,mathwoman,https://www.reddit.com/r/MachineLearning/comments/1f49zq2/d_why_arent_reviewers_required_to_respond_to/,Discussion
[D] Why we initialize the Neural Networks with random values in order to break the symmetry?,"I'm not that experienced in the realm of ANN yet, so I hope the question is not totally off-chart :)

I have come across the fact that neural networks are initialized with random values for their weights and biases to ensure that the values won't be initialized neither on the same or symmetrical values.

I completely understand why they cannot be the same - all but one node would be redundant.

The thing I cannot wrap my head around is why they must not be symmetrical. I have not found a single video about it on YouTube and GPT lowkey told me, when I kept asking why not, that if you have a range of relevant weights (let's say -10 to 10), it, in fact, is better to initialize them as far from each other as possible, rather than using one of the randomness algorithms.

The only problem GPT mentioned with this is the delivery of perfectly detached nodes.

Can anyone explain to me why then everyone uses random initialization?",MachineLearning,39,32,1724054683.0,1evwap1,kotvic_,https://www.reddit.com/r/MachineLearning/comments/1evwap1/d_why_we_initialize_the_neural_networks_with/,Discussion
[P] End-to-End Encrypted 23andMe Genetic Testing Application using Concrete ML and Fully Homomorphic Encryption.,"We've recently received some external contributions demonstrating how to use Zama libraries to create an end-to-end encrypted genetic testing application like 23andMe using fully homomorphic encryption (FHE). This blog post showcases the integration of advanced encryption techniques to ensure privacy while performing genetic analysis. This is a significant milestone, as it shows we can now predict encrypted DNA ancestry in about \~5 minutes using FHE.  
  
Read the full blog post here: [Build an End-to-End Encrypted 23andMe Genetic Testing Application Using Concrete-ML & Fully Homomorphic Encryption](https://www.zama.ai/post/build-an-end-to-end-encrypted-23andme-genetic-testing-application-using-concrete-ml-fully-homomorphic-encryption)  
Happy to answer any questions or assist anyone interested in building secure ML apps using our tools! ",MachineLearning,40,1,1722093524.0,1edir29,fd0r,https://www.reddit.com/r/MachineLearning/comments/1edir29/p_endtoend_encrypted_23andme_genetic_testing/,Project
[D] What is the most advanced TTS model now (2024)?,"If I want to train a TTS model for reading news, what should I do? What kind of training data do I need?

Thanks.",MachineLearning,42,27,1719827469.0,1dsp3vf,secsilm,https://www.reddit.com/r/MachineLearning/comments/1dsp3vf/d_what_is_the_most_advanced_tts_model_now_2024/,Discussion
[D] Data Scientist does the task without data,"Recently I was assigned a task to build a user purchase scoring system based on user interaction activities.

However, the funny thing is that I don't have data about user interactions with the product, so I surveyed the solutions of many parties and used my hypotheses to create the features which I thought will suitable to be able to build a prediction model. And of course when I presented it to the manager, the results were extremely bad. I sat down to discuss with him the definition of the features needed when creating the model and what made me quite angry was that he still don't know what kind of data is to build a scoring model. How will people deal with this situation?",MachineLearning,39,18,1716970867.0,1d374hh,unknow_from_vietnam,https://www.reddit.com/r/MachineLearning/comments/1d374hh/d_data_scientist_does_the_task_without_data/,Discussion
[D] Monthly Who's Hiring and Who wants to be Hired?,"**For Job Postings** please use this template

>Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]

**For Those looking for jobs** please use this template

>Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]

&#x200B;

Please remember that this community is geared towards those with experience.",MachineLearning,40,16,1735615814.0,1hq5o1z,AutoModerator,https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/,Discussion
[R] Automating the Search for Artificial Life with Foundation Models,"Happy to release this new work, [Automating the Search for Artificial Life with Foundation Models](https://arxiv.org/abs/2412.17799), right before the holiday season!

Blog: https://sakana.ai/asal/

Paper: https://arxiv.org/abs/2412.17799

Website version of paper: https://pub.sakana.ai/asal/

GitHub: https://github.com/SakanaAI/asal

**Abstract**

With the recent Nobel Prize awarded for radical advances in protein discovery, foundation models (FMs) for exploring large combinatorial spaces promise to revolutionize many scientific fields. Artificial Life (ALife) has not yet integrated FMs, thus presenting a major opportunity for the field to alleviate the historical burden of relying chiefly on manual design and trial-and-error to discover the configurations of lifelike simulations. This paper presents, for the first time, a successful realization of this opportunity using vision-language FMs. The proposed approach, called Automated Search for Artificial Life (ASAL), (1) finds simulations that produce target phenomena, (2) discovers simulations that generate temporally open-ended novelty, and (3) illuminates an entire space of interestingly diverse simulations. Because of the generality of FMs, ASAL works effectively across a diverse range of ALife substrates including Boids, Particle Life, Game of Life, Lenia, and Neural Cellular Automata. A major result highlighting the potential of this technique is the discovery of previously unseen Lenia and Boids lifeforms, as well as cellular automata that are open-ended like Conway's Game of Life. Additionally, the use of FMs allows for the quantification of previously qualitative phenomena in a human-aligned way. This new paradigm promises to accelerate ALife research beyond what is possible through human ingenuity alone.",MachineLearning,38,1,1735010627.0,1hl4o42,hardmaru,https://www.reddit.com/r/MachineLearning/comments/1hl4o42/r_automating_the_search_for_artificial_life_with/,Research
[D] Why are the Stella embedding models so much smaller than other models of similar quality?,"On the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard), `stella_en_v5` is currently ranked 3rd overall, while using *one fifth* the memory of all non-Stella models in the top 10.

`stella_en_400M_v5` is ranked 10th, while using *15-20 times less memory* than the models ranked near it. This appears to be relatively consistent across several subtasks of the benchmark (for English).

What is the secret sauce here? Alternatively, what is the catch? There is no paper yet. Anyone know details?",MachineLearning,37,7,1733889481.0,1hbkww5,-p-e-w-,https://www.reddit.com/r/MachineLearning/comments/1hbkww5/d_why_are_the_stella_embedding_models_so_much/,Discussion
[R] Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning (Research from Deepmind) ,"Abstract: A promising approach for improving reasoning in large language models is to use process reward models (PRMs). PRMs provide feedback at each step of a multi-step reasoning trace, potentially improving credit assignment over outcome reward models (ORMs) that only provide feedback at the final step. However, collecting dense, per-step human labels is not scalable, and training PRMs from automatically-labeled data has thus far led to limited gains. To improve a base policy by running search against a PRM or using it as dense rewards for reinforcement learning (RL), we ask: ""How should we design process rewards?"". Our key insight is that, to be effective, the process reward for a step should measure progress: a change in the likelihood of producing a correct response in the future, before and after taking the step, corresponding to the notion of step-level advantages in RL. Crucially, this progress should be measured under a prover policy distinct from the base policy. We theoretically characterize the set of good provers and our results show that optimizing process rewards from such provers improves exploration during test-time search and online RL. In fact, our characterization shows that weak prover policies can substantially improve a stronger base policy, which we also observe empirically. We validate our claims by training process advantage verifiers (PAVs) to predict progress under such provers, and show that compared to ORMs, test-time search against PAVs is >8% more accurate, and 1.5−5× more compute-efficient. Online RL with dense rewards from PAVs enables one of the first results with 5−6× gain in sample efficiency, and >6% gain in accuracy, over ORMs.",MachineLearning,38,4,1728715922.0,1g1uf90,DickMasterGeneral,https://arxiv.org/abs/2410.08146,Research
"[D] Have people stopped saying ""fine tuning"" in place of ""supervised fine tuning?"" Or is there some other fine tuning paradigm method out there.","I've always thought that fine tuning implied we have access to supervision. But these days it seems like ""SFT"" is what people say. Curious what the history is.",MachineLearning,38,39,1724106050.0,1ewezs4,Seankala,https://www.reddit.com/r/MachineLearning/comments/1ewezs4/d_have_people_stopped_saying_fine_tuning_in_place/,Discussion
[D] Models smarter than the original chat gpt can now be run on laptop hardware,"What a time to be alive!

https://i.redd.it/qlz4efsqzcgd1.gif

Back in 2022 I had assumed two things

1. ChatGPT was a unique product that wouldn’t be easily replicated
2. Language models of that quality would never be able to run locally

So happy to be wrong on both counts. Now you can download `llama3.1:8b` onto a laptop and chat with it with no limits, for free! I’m really impressed with the speed too. Never thought I would be such a fan of Mark Zuckerberg.",MachineLearning,40,15,1722651778.0,1eis40b,nanermaner,https://www.reddit.com/r/MachineLearning/comments/1eis40b/d_models_smarter_than_the_original_chat_gpt_can/,Discussion
[R] GraphReader: A Graph-based AI Agent System Designed to Handle Long Texts by Structuring them into a Graph and Employing an Agent to Explore this Graph Autonomously,,MachineLearning,37,8,1719690416.0,1drjcfz,valdanylchuk,/r/machinelearningnews/comments/1dpkz18/graphreader_a_graphbased_ai_agent_system_designed/,Research
[P] PixelProse 16M Dense Image Captions Dataset,"Hello everyone,

Hope everything is well with you. We would like to introduce a new project from our group here. Hope you like it.


We refresh the CC12M, RedCaps, and CommonPool with dense captions to produce a new 16M dataset using Gemini-1.0 Pro Vision, called PixelProse, consisting of over 16M pairs of image and dense caption. Hope it would be useful in your projects.

* arXiv: [https://arxiv.org/abs/2406.10328](https://arxiv.org/abs/2406.10328)
* huggingface repo: [https://huggingface.co/datasets/tomg-group-umd/pixelprose](https://huggingface.co/datasets/tomg-group-umd/pixelprose)


[Intro Figure: Dense synthetic image captions from PixelProse. Concrete phrases are highlighted in green, and negative descriptions are underlined in purple.](https://preview.redd.it/xll83cj34s7d1.png?width=3252&format=png&auto=webp&s=b09ee0046701f05308aaaffbf44dfc62391ea9ad)

  ",MachineLearning,35,4,1718912246.0,1dkkf9k,pidoyu,https://www.reddit.com/r/MachineLearning/comments/1dkkf9k/p_pixelprose_16m_dense_image_captions_dataset/,Project
[P] Mixed Precision Training from Scratch,"I reimplement the original mixed precision training paper from Nvidia ([https://arxiv.org/abs/1710.03740](https://arxiv.org/abs/1710.03740)) on a 2-layer MLP. I go all the way down to CUDA land to show TensorCore activations, which imo, is the real secret sauce of mixed precision training.

Code: [https://github.com/tspeterkim/mixed-precision-from-scratch](https://github.com/tspeterkim/mixed-precision-from-scratch)

Write-up: [https://tspeterkim.github.io/posts/mixed-precision-from-scratch](https://tspeterkim.github.io/posts/mixed-precision-from-scratch)",MachineLearning,38,1,1718584131.0,1dhlh0z,droidarmy95,https://www.reddit.com/r/MachineLearning/comments/1dhlh0z/p_mixed_precision_training_from_scratch/,Project
"[D] Question about You Only Cache Once: Decoder-Decoder Architectures for Language Models - 
https://arxiv.org/pdf/2405.05254v1","This is the first time I have tried to read through a paper. However, I have difficulties understanding this one and thought you guys would know the answer to my question because this new architecture seems like a big deal for LLMs as seen in figure 1.

[Figure 1](https://preview.redd.it/n6iitz36873d1.png?width=804&format=png&auto=webp&s=597102302acd26f27e28e99b366c13b7b135457a)

As I understand it, the main idea is splitting the network into two parts. The first L/2 layers are self-decoder layers which generate a global KV-Cache. The second L/2 layers are cross-decoder layers reusing the generated global KV-Cache.

Quote from their paper on how they save so much computation and memory ( I understand this part ):

>Specifically, because global KV caches are reused and efficient self-attention needs constant caches, the number of caches is O(N + CL), where N is the input length, C is a constant (e.g., sliding window size), and L is the number of layers. For long sequences, CL is much smaller than N, so about O(N) caches are required, i.e., you only cache once. In comparison, Transformer decoders have to store N × L keys and values during inference. So YOCO roughly saves L times GPU memory for caches compared to Transformer decoders.

Here is what I don't get. In a decoder-only network, the concepts of Queries, Keys, and Values function somewhat similarly to their use in a database, but with a focus on capturing relationships between words. In each layer of such a network, these components help refine the understanding of the text, adjusting the focus based on new insights as the processing moves from one layer to the next.

Each layer builds upon the previous ones by updating the queries, keys, and values, which in turn refine the network's interpretation and response generation.

**If all of the information of the individual KV-caches of a decoder only network is now compressed into a global KV-Cache, don't we lose valuable information and shouldn't we see worse performance?**  
  
Additionally, we only have half the layers to refine this interpretation, as the cross-decoder layers all reuse the same KV-cache.

[Figure 2](https://preview.redd.it/n2dx6hj9873d1.png?width=579&format=png&auto=webp&s=88ef6f8334ba3a13ce7ffdafeec333cae3bf869b)",MachineLearning,38,8,1716919107.0,1d2ptil,StraightChemistry629,https://www.reddit.com/r/MachineLearning/comments/1d2ptil/d_question_about_you_only_cache_once/,Discussion
[D] ML paper verb tense,"Why do most ML papers use all verb tenses in the present tense like MLA format while using a citation style or reference section as APA style?

In particular, even though academic societies such as ICML explicitly say that they follow the APA style, most of the papers' verb tenses do not seem to be followed by instructions in the APA guide to write the past, present, and future appropriately.",MachineLearning,37,18,1716708208.0,1d0w1g7,cosmoquester,https://www.reddit.com/r/MachineLearning/comments/1d0w1g7/d_ml_paper_verb_tense/,Discussion
[P] GPT-Burn: A simple & concise implementation of the GPT in pure Rust 🔥,,MachineLearning,36,0,1716020748.0,1cusmrp,ProfessionalDrummer7,https://github.com/felix-andreas/gpt-burn,Project
[D] What are the most common and significant challenges moving your LLM (application/system) to production?,"There are a lot of people building with LLMs at the moment, but not so many are transiting from prototypes and POCs into production. This is especially in the enterprise setting, but I believe this is similar for product companies and even some startups focused on LLM-based applications. In fact some surveys and research places the proportion as low as 5%. 

People who are working in this area, what are some of the most common and difficult challenges you face in trying to put things into production and how are you tackling them at the moment? 

",MachineLearning,39,17,1714291627.0,1cf178i,gamerx88,https://www.reddit.com/r/MachineLearning/comments/1cf178i/d_what_are_the_most_common_and_significant/,Discussion
[R] Understanding Transformer Limitations in Graph Search: A Mechanistic Analysis of Learning and Scaling Behavior,"This paper tackled a fundamental question about transformers' ability to learn search algorithms by studying how they handle graph connectivity problems. The authors developed a novel interpretation method to analyze how transformers process search operations layer by layer.

Key technical points:
- Used graph reachability as a test case with controlled complexity and unlimited training data
- Developed interpretation technique to understand how transformer layers compute reachable vertex sets
- Found transformers learn to expand search frontier exponentially with depth
- Demonstrated clear scaling limitations based on graph size
- Showed in-context learning (chain-of-thought) doesn't overcome these limitations

Main results:
- Small transformers can learn basic search when trained appropriately
- Each layer computes union of previously reachable vertices plus their neighbors
- Performance degrades sharply with increasing graph size
- Adding parameters doesn't solve the scaling problem
- Models struggle with graphs beyond their training distribution

I think this work reveals important architectural limitations in transformers that we need to address for applications requiring search capabilities. The scaling behavior suggests we may need fundamentally different approaches for larger search spaces rather than just bigger models.

I think the interpretation method they developed could be valuable for understanding how transformers process other types of structured data beyond just graphs. The clear empirical results on scaling limitations should inform architecture choices for applications involving search-like computations.

TLDR: Transformers can learn basic graph search operations but face fundamental limitations with scale. Adding more parameters doesn't help, suggesting we need new approaches for complex search problems.

[Full summary is here](https://aimodels.fyi/papers/arxiv/transformers-struggle-to-learn-to-search). Paper [here](https://arxiv.org/abs/2412.04703).",MachineLearning,35,1,1733837821.0,1hb1wjo,Successful-Western27,https://www.reddit.com/r/MachineLearning/comments/1hb1wjo/r_understanding_transformer_limitations_in_graph/,Research
"[D] How does VQ-VAE disentangle, if it does at all?","I currently use a BetaTC-VAE, which does an excellent job at disentangling, knowing that VAE can slightly disentangle since for the model it's easier to get a lower KL loss if the variables are dissentanlged, the beta term make this beta times more important, and total correlation and mutual information loss push for total disentanglement, but in VQ-VAE there is no (major) disentanglement, only a codebook, and discrete outputs. Could the discrete latent given by the codebook be disentangled? If not, is there any paper on disdentangling VQ-VAE? I have an environment where disentangled latent spaces provide better reconstruction than continous latent spaces ",MachineLearning,35,15,1732858390.0,1h2epzx,ZazaGaza213,https://www.reddit.com/r/MachineLearning/comments/1h2epzx/d_how_does_vqvae_disentangle_if_it_does_at_all/,Discussion
[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",MachineLearning,37,23,1732418110.0,1gyhfxm,AutoModerator,https://www.reddit.com/r/MachineLearning/comments/1gyhfxm/d_selfpromotion_thread/,Discussion
[D] Neural networks based on the spectral theorem for real symmetric matrices?,"This question was originally posted on MathOverflow, but I thought it might interest the community here as well:

**Question**:

I am exploring a neural network architecture inspired by physical interactions, where each neuron has associated ""mass"" and ""position"" vectors. The weight matrix between neurons is computed using a force-like inverse-square law interaction, reminiscent of the Coulomb interaction between charged particles. For two neurons with ""mass"" vectors `μ_i` and `μ_j` located at positions `x_i` and `x_j`, the weight `w_ij` is defined as:

```
w_ij = (μ_i · μ_j) / ||x_i - x_j||^2
```

This formulation is structurally similar to the **Coulomb matrix** used in quantum chemistry to represent atomic interactions in molecules, where the entries are defined as:

```
C_ij =
    (Z_i * Z_j) / ||x_i - x_j||    if i ≠ j
    0.5 * Z_i^2.4                  if i = j
```

where `Z_i` and `Z_j` are atomic charges.

Given this context, I am interested in the following theoretical question:

> **Under what circumstances can a general symmetric matrix `W` be represented in the form of a Coulomb-like matrix?**

That is, when does there exist a set of vectors `{ μ_i, x_i }` such that:

```
w_ij = (μ_i · μ_j) / ||x_i - x_j||^2    for all i, j
```

### Motivation:

Exploring the possibility of representing a symmetric weight matrix `W` as a Coulomb-like matrix could potentially confirm that neural networks using this ""force-based"" weight concept can learn any function representable by a traditional network using symmetric matrices. Since multilayer perceptrons with symmetric weight matrices are known to be universal function approximators, establishing a comparable representational capability in neural networks with force-based interactions could open new avenues for designing computationally efficient and scalable neural architectures.

The inquiry into whether any symmetric matrix can be represented as a Coulomb matrix underpins the theoretical validity of using such architectures in broader machine learning applications. Such a representation would not only underscore the universality of force-based neural networks but also provide a foundational argument for their use in scenarios where traditional neural architectures might be computationally prohibitive.

Any insights into conditions, dimensionality constraints, or special cases where such a representation is feasible would be greatly appreciated!

The idea to use vectors instead of real numbers for ""mass"" comes from physics:

Suppose that two bodies `1` and `2` with each mass `m_i` and electric charge `q_i` have between them two forces: Coulomb's Force and Newton's Gravity force:

```
F_C = (q_1 * q_2) / |x_1 - x_2|^2
F_N = (m_1 * m_2) / |x_1 - x_2|^2
```

But by Newton's principle, the forces can be added, thus:

```
F_12 = F_C + F_N = (m_1 * m_2 + q_1 * q_2) / |x_1 - x_2|^2
```

which can be written as:

```
F_12 = F_C + F_N = <μ_1, μ_2> / |x_1 - x_2|^2
```

where `μ_i = (m_i, q_i)` is a two-dimensional vector with components mass and charge.

I am not imposing restrictions on the diagonal of the symmetric matrix. The dimensions of the mass and position vectors can be different.

**Edit**: While modifying the original idea, after experiments with custom neural networks, to the following and while keeping the intention the same, I am asking if every symmetric matrix with `0` on the diagonal can be written as:

```
w_ij = <m_i, m_j> * ||x_i - x_j||^2
```

This simplifies the analysis I hope since then we do not divide through `0`.

**Second edit**: A somehow cheating solution which solves the problem above would be to use the spectral theorem, whereby a real symmetric matrix `w_ij` can be decomposed into:

```
w_ij = sum_{k=1}^n (q_{ik} * λ_k * q_{jk})
```

where `q_i = (q_{i1}, ..., q_{in})` is an `n`-dimensional vector and `λ_k` is a real number. The `λ_k` and `q_k` are the eigenvalues and eigenvectors of `W`. The `q_k` are pairwise orthogonal. Using this knowledge that *every* symmetric matrix can be decomposed this way, we might want to impose on the weights of the neural network the 'restriction' that they are generated this way for `d` being the dimension of the vectors `q_k` which could in theory be large as `n`, the number of neurons:

```
w_ij = sum_{k=1}^d (q_{ik} * λ_k * q_{jk})
```

Hence the neural network has a `d`-dimensional vector `λ` and each neuron `i` has a `d`-dimensional vector `q_i`. The weights between neuron `i` and `j` are computed as described above. The vectors `q_i` and `λ` are learned through gradient descent and backpropagation as is being done in Multilayer Perceptrons. I have not a proof that this setting should allow the network to learn any function, but my vague idea goes like this:

If an ordinary MLP can learn any function, then it means it can learn any sort of symmetric weights `w_ij`. By allowing the 'Spectral Neural Network' to be able to express any sort of symmetric weights `w_ij` through the spectral theorem, we could argue that the spectral neural network can adapt its parameters to learn any symmetric weights. But then it is an MLP which has learned some specific weights for a given specific function `f` and so it should be possible to learn any function with the spectral neural network.

Modified question: **Is it possible to make the idea of universal learning with spectral neural network more concrete, maybe a proof?**

Comment: Of course for `d ≥ n` the savings in memory are lost, but I can imagine that there are situations of problems where `d << n` and there we have not only savings in memory from `O(n^2)` to `O(n*d)` but also a dimensionality reduction, kind of. The training process could start with `d=1` and increase it fast or gradually specific to the problem. 

",MachineLearning,38,8,1730438994.0,1ggyfrd,musescore1983,https://www.reddit.com/r/MachineLearning/comments/1ggyfrd/d_neural_networks_based_on_the_spectral_theorem/,Discussion
[D] How are folks building conversational Retrieval Augmented Generation apps,"I've read through various resources such as:  
- [https://vectorize.io/how-i-finally-got-agentic-rag-to-work-right/](https://vectorize.io/how-i-finally-got-agentic-rag-to-work-right/)  
- [https://python.langchain.com/docs/tutorials/qa\_chat\_history/](https://python.langchain.com/docs/tutorials/qa_chat_history/)  
- [https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph\_agentic\_rag/](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/)  
- [https://docs.llamaindex.ai/en/stable/module\_guides/deploying/chat\_engines/](https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/)  
- [https://huggingface.co/datasets/nvidia/ChatRAG-Bench](https://huggingface.co/datasets/nvidia/ChatRAG-Bench) 

But these feel overly reductive, since they don't address complexities like:  
1) when to retrieve vs. just respond immediately to reduce latency  
2) rely on existing context previously retrieved in the conversation instead of retrieving again at the current turn  
3) partition LLM context between retrieved information and past conversation history.

I'm sure some teams already have good systems for this, would appreciate pointers!",MachineLearning,39,6,1727749065.0,1ftdby7,iidealized,https://www.reddit.com/r/MachineLearning/comments/1ftdby7/d_how_are_folks_building_conversational_retrieval/,Discussion
[D] Why are most Federated Learning methods so dependent on hyperparameters?,"I'm doing research in FL for some time now  and went through a few subfields. Whenever I start a new project and do some benchmarking of existing methods, it always takes an eternity to get the methods to work on standard datasets like cifar10 that weren't used in the original papers. Currently I am using a premade benchmarking tool (fl-bench) and still struggle to get fedavg to converge on even slightly non-i.i.d. datasets on cifar10. This makes working in the field super frustrating imo. Did you have similar experiences or is there something fundamental that I missed all this time?",MachineLearning,35,10,1726313344.0,1fgjzlt,NumerousSwordfish653,https://www.reddit.com/r/MachineLearning/comments/1fgjzlt/d_why_are_most_federated_learning_methods_so/,Discussion
Transformers learn in-context by gradient descent [R],"Can someone help me understand the reasoning in the paper [Transformers learn in-context by gradient descent](https://arxiv.org/pdf/2212.07677)? The authors first assume a ""reference"" linear model with some weight \\( W \\), and then show that the loss of this model after a gradient descent step is equal to the loss of the ""transformed data."" Then, in the main result (Proposition 1), the authors manually construct the weights of \\( K \\), \\( Q \\), and \\( V \\) such that a forward pass of a single-head attention layer maps all tokens to this ""transformed data.""

My question is: how does this construction ""prove"" that transformers **can** perform gradient descent in in-context learning (ICL)? Is the output of the forward pass (i.e., the ""transformed data"") considered a new prediction? I thought it should be like this: the new **prediction** matches the **prediction** given by the updated weight. I could not understand the logic here.

https://preview.redd.it/cztva19y05kd1.png?width=1046&format=png&auto=webp&s=0196944994516f480670ba9d29be91f8f55fc6f9

https://preview.redd.it/oihuv48405kd1.png?width=1728&format=png&auto=webp&s=8cfc35bf8aa433d53f9d7b5bc5faef3c3e4fba8a

",MachineLearning,35,5,1724306167.0,1eybxel,mziycfh,https://www.reddit.com/r/MachineLearning/comments/1eybxel/transformers_learn_incontext_by_gradient_descent_r/,Research
[D] what's the alternative to retrieval augmented generation?,It seems like RAG is the de-facto standard of question answering in the industry. What's the alternative?,MachineLearning,38,32,1722068518.0,1edbg0h,clocker2004,https://www.reddit.com/r/MachineLearning/comments/1edbg0h/d_whats_the_alternative_to_retrieval_augmented/,Discussion
[P] fast_mamba.np: pure and fast NumPy implementation of Mamba with 4x speedup,"[fast\_mamba.np](https://preview.redd.it/hymjfe99j57d1.png?width=400&format=png&auto=webp&s=ba30cbfdc61236b730d548d86038152da501f11d)

After looking at several repositories I found out that most of them do not implement the native caching of Mamba, in order to keep the code clean and simple. Caching usually complicates the code and that is why I implemented `fast_mamba.np` as a simple implementation of Mamba in pure Numpy with caching support. This implementation aims to be straightforward and efficient while accelerating by 4x on a local CPU compared to [mamba.np](https://github.com/idoh/mamba.np).

[https://github.com/idoh/fast\_mamba.np](https://github.com/idoh/fast_mamba.np)

    $ python fast_mamba.py ""I have a dream that""
    """"""
    I have a dream that I will be able to see the sunrise in the morning.
    
    Token count: 18, elapsed: 9.65s, 1.9 tokens/s
    """"""

I hope you find it useful :)",MachineLearning,39,5,1718638767.0,1di14et,id0h,https://www.reddit.com/r/MachineLearning/comments/1di14et/p_fast_mambanp_pure_and_fast_numpy_implementation/,Project
[P] An interesting way to minimize tilted losses,"Some time ago I read a paper about the so-called *tilted empirical risk minimization*, and later a JMLR paper from the same authors: [https://www.jmlr.org/papers/v24/21-1095.html](https://www.jmlr.org/papers/v24/21-1095.html)

Such a formulation allows us to train in a manner that is more 'fair' towards the difficult samples, or conversely, less sensitive to these difficult samples if they are actually outliers. But minimizing it is numerically challenging. So I decided to try and devise a remedy in a blog post. I think it's an interesting trick that is useful here, and I hope you'll find it nice as well:

[https://alexshtf.github.io/2024/06/14/Untilting.html](https://alexshtf.github.io/2024/06/14/Untilting.html)",MachineLearning,34,10,1718522822.0,1dh2aqt,alexsht1,https://www.reddit.com/r/MachineLearning/comments/1dh2aqt/p_an_interesting_way_to_minimize_tilted_losses/,Project
"[R] Enabling sparse, foundational LLMs for faster and more efficient models from Neural Magic and Cerebras","In a collaboration across Neural Magic, Cerebras, and IST Austria, we've pushed out, to the best of our knowledge, the first highly sparse, foundational LLMs with full recovery on several fine-tuning tasks, including chat, code generation, summarization, and more.

[Sparsity vs Baseline Accuracy Recovery for Popular Fine-tuning Tasks for Llama 2 7B](https://preview.redd.it/mexhej5i8s1d1.png?width=2936&format=png&auto=webp&s=3a126fe3ca7a36f25cb71cfb02924d2b26aa72f7)

Utilizing the models, we further demonstrate:

* Inference performance speedups from sparsity alone at 3x for CPUs and 1.7x for GPUs on Neural Magic's platform.
* Compounded gains with quantization for up to 8.6X faster inference performance.
* Close to theoretical gains for sparse training utilizing Cerebras's CS-3 AI accelerator.

[Prefill and Decode Llama 2 7B Performance at Various Sparsity Levels for FP32 and INT8 on an 8-core CPU.](https://preview.redd.it/jtxsxxio8s1d1.png?width=3636&format=png&auto=webp&s=ce9d228d9ff5891e043b83c27d955fb0645f3ecd)

  
Paper: [https://arxiv.org/abs/2405.03594](https://arxiv.org/abs/2405.03594)  
Models: [https://huggingface.co/collections/neuralmagic/sparse-foundational-llama-2-models-65f48cec6396309f02e74d21](https://huggingface.co/collections/neuralmagic/sparse-foundational-llama-2-models-65f48cec6396309f02e74d21)",MachineLearning,39,2,1716298594.0,1cx83cs,markurtz,https://www.reddit.com/r/MachineLearning/comments/1cx83cs/r_enabling_sparse_foundational_llms_for_faster/,Research
[D] Matrix Profile vs. Deep Learning for Multivariate Time Series ,"Hey everyone,

So I was reading a plethora of approaches, especially regarding research done in Multivariate Time Series and real-time Human Activity Recognition (HAR). Though, I recently stumbled upon [Eamonn Keogh’s](https://www.cs.ucr.edu/~eamonn/) amazing and comprehensive work on Matrix Profiles and ended up in a rabbit hole. 

But out of curiosity, in general how does the Matrix Profile compare with Deep Learning methods (e.g. MLP, LSTM, etc..) in the context of Multivariate Time Series and real-time streams of data? 

I would love to hear other people’s perspectives!",MachineLearning,37,13,1715232670.0,1cnpo6n,peachjpg111,https://www.reddit.com/r/MachineLearning/comments/1cnpo6n/d_matrix_profile_vs_deep_learning_for/,Discussion
[D] Benchmark creators should release their benchmark datasets in stages,"There's been a lot of discussion about benchmark contamination, where models are trained on the data they are ultimately evaluated on. For example, a [recent paper](https://twitter.com/hughbzhang/status/1785877026794356858) showed that models performed substantially better on the public GSM8K vs GSM1K, which was a benchmark recently created by Scale AI to match GSM8K on difficulty and other measures.

Because of these concerns about benchmark contamination, it is often hard to take a research lab's claims about model performance at face value. It's difficult to know whether a model gets good benchmark performance because it is generally capable or because its pre-training data was contaminated and it overfit on the benchmarks.

One solution to this problem is for benchmark creators to release their datasets in stages. For example, a benchmark creator could release 50% of their dataset upon release, and then release the remaining 50% in two stages, 25% one year later and 25% two years later. This would enable model evaluators to check for benchmark contamination by comparing performance on the subset of data released prior to the training cutoff vs. the subset released after the training cutoff. It would also give us a better understanding of how well models are actually performing.

One last point - this staged release process wouldn't be anywhere near as helpful for benchmarks created by scraping the web, as even the later-released data subsets could be found in the training data. But it should be useful for other kinds of benchmarks.",MachineLearning,38,5,1714671363.0,1cilnzv,kei147,https://www.reddit.com/r/MachineLearning/comments/1cilnzv/d_benchmark_creators_should_release_their/,Discussion
[P] We built a natural language search engine which lets you explorer over half a million artworks by describing what you want to see,,MachineLearning,35,11,1735407945.0,1hoavl0,stefanvdw,https://artexplorer.ai,Project
"[D] How do you keep track of experiments, history, results?","I saw people using some tools, but sometimes those doesn't really fit and i'm confused which ones to try.

Do you guys just save the config+results? But how about when the model code changes?

I am unsure how to go about this. Any tips?

I think i might need some paper/digital notes plus some way to backtrack.

  
EDIT: Lots of good comments ! Thank you! I'll keep this post up and just keep commenting. Others will surely find this helpful.",MachineLearning,33,36,1731384013.0,1gpc3cv,Pristine-Staff-5250,https://www.reddit.com/r/MachineLearning/comments/1gpc3cv/d_how_do_you_keep_track_of_experiments_history/,Discussion
[P] Built a roadmap site and got 450 users in 25 days and I am so happy!!!!!!,"hello everyone, I am a 3rd year cse student. I built this site called [https://www.mldl.study/](https://www.mldl.study/) last month. this site is for anyone who is ""new"" to machine learning and deep learning and is confused about where to start. I built this because I was confused about it too. It has got proper video lectures, articles, research papers, visualizations, kaggle competitions and basically everything you need to master ml and dl in proper order.



i just added google analytics 25 days back and I saw that I have got like 450 users and 135 returning users. I built this just to help my college friends but I am so glad that its helping others too. I just wanted to share this as I am so happy about this. This gives me confidence that I can build something more cooler and useful in future.



Thanks everyone. I got little push in my analytics from here only. THANKYOU!!



(I am also open to suggestions and all, what I can do to grow it even more)

https://preview.redd.it/s9v6omy5f10e1.png?width=1558&format=png&auto=webp&s=eeb9a22012e2e3806245e9267a1187bb91e75305

",MachineLearning,37,14,1731228501.0,1gnwfhn,None,https://www.reddit.com/r/MachineLearning/comments/1gnwfhn/p_built_a_roadmap_site_and_got_450_users_in_25/,Project
[D] We built a multi-cloud GPU container runtime,"Wanted to share our open source container runtime -- it's designed for running GPU workloads across clouds.

[https://github.com/beam-cloud/beta9](https://github.com/beam-cloud/beta9)

Unlike Kubernetes which is primarily designed for running one cluster in one cloud, Beta9 is designed for running workloads on many clusters in many different clouds. Want to run GPU workloads between AWS, GCP, and a 4090 rig in your home? Just run a simple shell script on each VM to connect it to a centralized control plane, and you’re ready to run workloads between all three environments.

It also handles distributed storage, so files, model weights, and container images are all cached on VMs close to your users to minimize latency.

We’ve been building ML infrastructure for awhile, but recently decided to launch this as an open source project. If you have any thoughts or feedback, I’d be grateful to hear what you think 🙏",MachineLearning,35,1,1729615526.0,1g9mrcj,velobro,https://www.reddit.com/r/MachineLearning/comments/1g9mrcj/d_we_built_a_multicloud_gpu_container_runtime/,Discussion
[D] Seeking advice from industry researchers who previously held roles in academia or completed a PhD,"1. What would you recommend someone moving from academia to join an industry research lab do in their first 30, 90, and 180 days to ensure they are making a good contribution to the company?
2. Are there habits or ways of thinking in academia which you need to actively move away from/manage in industry research environments?
3. In general, what skills are most commonly lacking or weak in employees coming from academia and/or which skills should an academic brush up/learn on before joining a company?
4. Any other tips/advice?",MachineLearning,37,11,1729089897.0,1g5130q,tfburns,https://www.reddit.com/r/MachineLearning/comments/1g5130q/d_seeking_advice_from_industry_researchers_who/,Discussion
[D] When is Lora not good enough?,"What are some examples of LLM fine-tuning tasks where LORA (or some of its variants) is not good enough and full fine-tuning is needed?

For example, here in all tested tasks, RoSA (LORA variant) is as good as full fine-tuning [https://arxiv.org/pdf/2401.04679](https://arxiv.org/pdf/2401.04679)",MachineLearning,36,5,1728135034.0,1fwqgfx,osamc,https://www.reddit.com/r/MachineLearning/comments/1fwqgfx/d_when_is_lora_not_good_enough/,Discussion
[P] Yet another transformer visualizer,"I made this for myself as I learned the decoder-only transformer architecture alongside Andrej Karpathy’s YT videos (particularly [""Let's build GPT: from scratch, in code, spelled out""](https://www.youtube.com/watch?v=kCc8FmEb1nY)). Hopefully it is helpful to a few people at least, but if you find anything incorrect, irksome, or unintuitive, feel free to call it out.   
  
Also, FYI, the design is not mobile friendly. Wide screens are recommended.

Link: [https://learn-good.github.io/llm\_viz/1\_decoder\_only\_transformer.html](https://learn-good.github.io/llm_viz/1_decoder_only_transformer.html)",MachineLearning,34,4,1727114340.0,1fnqwa0,arnokha,https://www.reddit.com/r/MachineLearning/comments/1fnqwa0/p_yet_another_transformer_visualizer/,Project
[D] How do companies like Glean or OpenAI store so much data in a vector DB for retrieval?,"I was going to Qdrants pricing and saw that if I wanted 32GB RAM with 4 VPCUs + 1TB (hypothetical space) it would cost me around $780/month.

  
How do companies like Glean or OpenAI make so much data for Enterprises searchable? These enterprises are already paying for storage, so they aren't thinking of RAG as paying for storage; confused about how large the amount of data scales when such services are so expensive.

Pretty sure even hosting your own wouldn't be cheap.

  
Any ideas?",MachineLearning,34,20,1720940282.0,1e2vxqw,dtek_01,https://www.reddit.com/r/MachineLearning/comments/1e2vxqw/d_how_do_companies_like_glean_or_openai_store_so/,Discussion
"[D] Hiring students/graduates, good or bad idea?","My startup is at a point where we'd like to start exploring some novel concepts using ML, specifically within the realm of audio. We're self funded so we have limited budget and can't afford some the ML people I find on job postings asking for $400k/yr 😳
But interestingly enough, all the ML open source projects I see that are truly interesting seem to be done by graduate students / people working on their PhD. Not by people with huge resumes working for massive companies. 

Is it unreasonable to try and find a passionate graduate student at a somewhat affordable hourly rate, in hopes that they could become part of the company, equity, etc? Or is that not usually a thing?",MachineLearning,32,36,1720883466.0,1e2cnw8,maxiedaniels,https://www.reddit.com/r/MachineLearning/comments/1e2cnw8/d_hiring_studentsgraduates_good_or_bad_idea/,Discussion
[D] Entity Extraction with LLMs,"A lot of us are using LLMs to extract entities from unstructured text. The biggest challenges with entity extraction with LLMs are:

1. **Duplicated entities**: For example, if you're extracting products from user support tickets, an LLM might extract ""product: shirt"". ""product: t-shirt"", ""product: short sleeve"", or ""product: shirts"" depending on how the input text is referencing `shirts`. This means you may need to rigorously iterate and monitor the LLM's behavior (e.g. making sure the LLM references an existing entity label).
2. **Adding new entities**: When an existing entity label doesn't match, an LLM is often great at coming up with new entity labels when necessary. However, this can become unmanageable especially if new entity labels are being introduced constantly (lack of consistency in granularity). And unfortunately, the larger the list of entity labels an LLM must index from, the less accurate the LLM will be.



So here are some tips to help you stay on top of extractions:

* **Set up alerts**: It's very important to stay on top of the outputs. If you're working with high-volume text daily, setting a SQL alert for any new DISTINCT labels is a good first step.
* **Provide context**: LLMs perform better with context. Add context to the task and to the entity labels.
* **Post-processing**: Create post-processing steps to handle overlapping entities and refine results.
* **Handle Ambiguity with Few Shot**: Identify some ambiguous or tricky examples, and add them to the prompt.
* **No answer is better than a wrong one**: Give LLMs an out. If there is no good entity, you will want to heavily encourage the LLM to not make up something (which they still do quite often).
* **Incorporate Business Feedback**: This is use-case dependent. Entity extractions can often be for ops efficiencies or user-facing features. If this is the case, it's important to confirm and align with those users on what a ""shirt"" is. The eng vs stakeholder perspectives are often different.



If you still aren't getting the accuracy you need, you can try fine-tuning. Classical ML and NER libraries are fairly unreliable but can be worth experimenting with, if you're desperate.



You can also try some external services that handle this for you. For example, we extract raw entities with a BERT-like model + manage and resolve them to canonical entity options with very high accuracy. As new entities occur (that don't match an existing entity), we add them to the entity list if they reach a high enough confidence level by our models.

  
Please comment below your experiences and tips on entity extraction! 

",MachineLearning,38,22,1720221256.0,1dwbcp5,Different-General700,https://www.reddit.com/r/MachineLearning/comments/1dwbcp5/d_entity_extraction_with_llms/,Discussion
[D] Nemotron-4 340b detailed analysis ,"I took a look at NVIDIA's 340B Nemotron LLM - some of my findings:

* **Squared ReLU** unlike Llama SwiGLU, Gemma GeGLU. Different to GLU variants found in [arxiv.org/pdf/2002.05202](http://arxiv.org/pdf/2002.05202) (GLU Variants Improve Transformer, Noam Shazeer)
   * ReGLU is \[ ReLU(X \* W\_gate) \* (X \* W\_up) \] \* W\_down
   * We need 2 ReLUs + tied weights \[ ReLU(X \* W\_up) \* ReLU(X \* W\_up) \] \* W\_down, so bit like GLU, but not the same
   * Why does Squared ReLU work? Primer paper: [https://arxiv.org/abs/2109.08668v2](https://arxiv.org/abs/2109.08668v2) discovered it. Also Shazeer's quote: ""We offer no explanation as to why these architectures seem to work; we attribute their success, as all else, to divine benevolence""
* Uses **rotary\_percentage of 50%**. Maybe related to Phi-2's partial\_rotary\_factor? - Phi-2 's rotary\_percentage is 40%, so it looks like for Nemotron, only 50% of the Q, K matrices apply RoPE, and the rest don't use RoPE.
   * See [https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi/modeling\_phi.py#L79](https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi/modeling_phi.py#L79)
* **Untied embeddings** like Llama. Gemma tied.
   * Tied also used in Apple's on device LLM to save VRAM.
   * Tied also sometimes does better on smaller models as per the Physics of LLM paper [https://arxiv.org/abs/2404.05405](https://arxiv.org/abs/2404.05405)
* **Normal layernorm** unlike Llama RMS LN. RMS Layernorm removes the bias and does not do mean removal
* No dropout, no bias like Llama, Gemma
* Batch size ramp up with 42% MFU. Float16 training with no sparsity.
* **4096 sequence length**. Quite short sadly.
* 8 trillion tokens - 3 flavours Base, Instruct, reward
* Does **SFT, DPO, RPO**
   * RPO - I think 2 steps? Reward-aware Preference Optimization
   * HelpSteer2 Dataset: [https://huggingface.co/datasets/nvidia/HelpSteer2](https://huggingface.co/datasets/nvidia/HelpSteer2)
* Beats GPT-4o in some benchmarks

Technical report: [https://research.nvidia.com/publication/2024-06\_nemotron-4-340b](https://research.nvidia.com/publication/2024-06_nemotron-4-340b)

HF Instruct: [https://huggingface.co/nvidia/Nemotron-4-340B-Instruct](https://huggingface.co/nvidia/Nemotron-4-340B-Instruct)

https://preview.redd.it/vhcke1177l6d1.png?width=980&format=png&auto=webp&s=2458d6c2bd9ed0db14e9f16c5be6e93340f6bfaf

",MachineLearning,38,15,1718392853.0,1dfywwi,danielhanchen,https://www.reddit.com/r/MachineLearning/comments/1dfywwi/d_nemotron4_340b_detailed_analysis/,Discussion
[D] Neurips 2024 submissions,"I just submitted an abstract to Neurips 2024. I was so impressed with my self for being two days early, and yet, my paper ID is over 7000. In the past I recall paper IDs were incremented as openreview received more submissions. Surely, this year it’s not the case! 7000 submissions already?!",MachineLearning,36,24,1715634440.0,1crahli,fixed-point-learning,https://www.reddit.com/r/MachineLearning/comments/1crahli/d_neurips_2024_submissions/,Discussion
[D] NVIDIA GPU Benchmarks & Comparison,"[https://tensordock.com/benchmarks](https://tensordock.com/benchmarks)

Spent the past few hours putting together some data on vLLM (for both Llama 7B and OPT-125M) and Resnet-50 training performance on the TensorDock cloud. 

vLLM data is 100% out of the box, with 2048 batch sizes from [this repository](https://github.com/vllm-project/vllm/tree/main/benchmarks). 

My learnings:

* H100 and A100 performance is unbeatable, but the price-to-performance of lower-end RTX cards is pretty darn good. Even the L40 and RTX 6000 Ada outperform the A100 at some tasks, as they are 1 generation newer than the A100. **If your application does not need 80GB of VRAM, it probably makes sense to not use an 80GB VRAM card**
* Standalone H100 performance isn't as strong as I would have imagined. H100 performance is bottlenecked by memory bandwidth for LLM inference, hence **H100s are only 1.8x faster than A100s for vLLM**. H100s really perform better when interconnected together, but I didn't benchmark that today. 
* **CPU matters more than I expected**. The OPT-125M vs Llama 7B performance comparison is pretty interesting... somehow all GPUs tend to perform similar on OPT-125M, and I assume that's because relatively more CPU time is used than GPU time, so the GPU performance difference matters less in the grand scheme of things.  
* The marketplace prices itself pretty well. If cohort GPUs into VRAM amount all GPUs with similar amounts of VRAM share a similar price-to-performance ratio.
* Self hosting can save you $$$ if you have sufficient batch sizes. If you built your own inference API, you could serve LLMs utilizing just 50% batches and save money compared to a pay-per-token API (we \[TensorDock\] cost less than $0.07 per million Llama 7 tokens if you use us at 100%)

\-- 

Let me know which GPU to benchmark next, and I'll add that! Or let me know some other workload to measure, and I'd be happy to add an new section for that too. 

P.S. We added some H100s at $1.80/hr for anyone lucky enough to grab them! ",MachineLearning,35,5,1714893933.0,1cklpyd,jonathan-lei,https://www.reddit.com/r/MachineLearning/comments/1cklpyd/d_nvidia_gpu_benchmarks_comparison/,Discussion
[R] Hyper-Connections,"**TL;DR** A more complex and more performant variant of residual stream.

**Paper:** [https://arxiv.org/pdf/2409.19606](https://arxiv.org/pdf/2409.19606)

**Abstract:**

>We present hyper-connections, a simple yet effective method that can serve as an alternative to residual connections. This approach specifically addresses common drawbacks observed in residual connection variants, such as the seesaw effect between gradient vanishing and representation collapse. Theoretically, hyper-connections allow the network to adjust the strength of connections between features at different depths and dynamically rearrange layers. We conduct experiments focusing on the pre-training of large language models, including dense and sparse models, where hyper-connections show significant performance improvements over residual connections. Additional experiments conducted on vision tasks also demonstrate similar improvements. We anticipate that this method will be broadly applicable and beneficial across a wide range of AI problems.

**Visual Abstract:**

[Trust me, it's less complicated than it seems at first glance](https://preview.redd.it/257g4nmnoz7e1.png?width=1345&format=png&auto=webp&s=2d423d68a06975ff7b8b027cb11b3f833282b443)

**Visual highlights:**

[The most impressive gains are achieved with MoE architecture, although Dense Transformers get a boost too](https://preview.redd.it/kq4ujazzpz7e1.png?width=1347&format=png&auto=webp&s=7e3a272ebb3b5ea18c46827efc13074426696c14)

[Hyper-connections mitigate representation collapse, to a degree](https://preview.redd.it/1f43ipl0qz7e1.png?width=437&format=png&auto=webp&s=c1aa03e93a806f199cc3a2d58643687992bd00eb)

[Expansion rate refers to splitting the residual stream into n independent components, each of which is dynamically gated. The input to each Transformer block is a simple sum of these components](https://preview.redd.it/7j5cu6miqz7e1.png?width=1129&format=png&auto=webp&s=75fe0b2a6edcc2477cf1e755d04c79f42825953c)

[SHC=static gating, DHC=dynamic gating](https://preview.redd.it/wxvs83w7rz7e1.png?width=1101&format=png&auto=webp&s=a64aefc5bf912ea95eb4ad2178dff3d73bb0600e)

[Computational overhead is negligible](https://preview.redd.it/oy4bptnsrz7e1.png?width=1105&format=png&auto=webp&s=aeccfc554f2ff5528bb0c2d6c25e7cbc73949452)

https://preview.redd.it/238ex3emtz7e1.png?width=973&format=png&auto=webp&s=ac12792a4d45f7bf6e91f767dd12f2134ec74083",MachineLearning,35,7,1734697092.0,1hiiktb,StartledWatermelon,https://www.reddit.com/r/MachineLearning/comments/1hiiktb/r_hyperconnections/,Research
[R] Aurora: A General-Purpose Foundation Model for Earth System Prediction,"The key contribution here is the development of Aurora, a foundation model trained on over 1M hours of atmospheric data that can perform multiple types of weather and climate predictions using a single model architecture. This represents a shift from building separate specialized models to having one model that learns general atmospheric physics.

Key technical points:
- Model architecture uses transformer blocks with attention mechanisms adapted for spatiotemporal data
- Trained on merged datasets from multiple sources including ERA5 reanalysis, satellite observations, and climate model outputs
- Can generate predictions for diverse tasks like air pollution, precipitation, and temperature forecasting
- Produces forecasts in under 1 minute compared to hours/days for traditional numerical models
- Outperforms both specialized ML models and physics-based numerical weather prediction on several benchmarks

Results:
- 15-20% improvement in 5-day global air pollution predictions vs current methods
- Better performance on 10-day weather forecasts compared to specialized models
- Maintains accuracy even for extreme weather events
- Shows continual improvement as training data increases
- Successfully handles multiple spatial and temporal resolutions

I think this work could significantly change how we approach environmental modeling. Instead of maintaining separate models for different prediction tasks, having a single foundation model that can handle multiple atmospheric predictions could make forecasting more efficient and accessible. The speed improvements (minutes vs hours) could enable new applications requiring rapid predictions.

I think the challenges ahead include:
- Validating performance across more diverse atmospheric phenomena
- Understanding model interpretability for critical forecasting
- Addressing computational costs of training and inference
- Ensuring reliability for operational forecasting systems

TLDR: Researchers developed Aurora, an atmospheric foundation model trained on massive weather/climate data that can handle multiple prediction tasks better than specialized models while being much faster. Shows foundation models could transform environmental forecasting.

[Full summary is here](https://aimodels.fyi/papers/arxiv/foundation-model-earth-system). Paper [here](https://arxiv.org/abs/2405.13063).",MachineLearning,34,2,1732542609.0,1gzj8rs,Successful-Western27,https://www.reddit.com/r/MachineLearning/comments/1gzj8rs/r_aurora_a_generalpurpose_foundation_model_for/,Research
[D] Can an AC override 3 rejects and accept a paper?,"I came across this paper: [Auto-Generating Weak Labels for Real & Synthetic Data to Improve Label-Scarce Medical Image Segmentation](https://openreview.net/forum?id=gHCo43zcDm) accepted at this year's MIDL (Medical Imaging with Deep Learning) conference. The reviewer ratings before/after the rebuttal are:

* 2: Weak reject / 2: Weak reject
* 2: Weak reject / 2: Weak reject
* 3: Borderline / 2: Weak reject

Despite having 3 reject decisions, the Area Chair ""recommended acceptance"". How common is it? And how much does having big names like [Curtis Langlotz](https://scholar.google.com/citations?user=WQkBYwQAAAAJ) and [Andrew Ng](https://scholar.google.com/citations?user=mG4imMEAAAAJ&hl=en) as co-authors on the paper, given that ACs can see author names?",MachineLearning,34,19,1730937270.0,1glczb9,thrownicecatch,https://www.reddit.com/r/MachineLearning/comments/1glczb9/d_can_an_ac_override_3_rejects_and_accept_a_paper/,Discussion
[R] Limitations in Mainstream LLM Tokenizers,"Mainstream LLM tokenizers cann't encode and decode to exact string. This means they aren't lossless. Some Llama, Mistral, and Phi tokenizers cannot encode string `' Who let the dog out?! !'` and then decode to the same string.

If you run code:
```python
from transformers import AutoTokenizer

models = [
    'meta-llama/Llama-2-7b',
    'meta-llama/Meta-Llama-3-8B',
    'meta-llama/Llama-3.1-8B',
    'mistralai/Mistral-7B-v0.3',
    'mistralai/Mixtral-8x7B-v0.1',
    'mistralai/Mixtral-8x22B-v0.1',
    'mistralai/Mistral-Nemo-Instruct-2407',
    'mistralai/Mistral-Small-Instruct-2409',
    'mistralai/Mistral-Large-Instruct-2407',
    'microsoft/phi-1',
    'microsoft/phi-1_5',
    'microsoft/phi-2',
    'microsoft/Phi-3-mini-4k-instruct',
    'microsoft/Phi-3.5-mini-instruct',
]

text = ' Who let the dog out?! !'

for n in models:
    tokenizer = AutoTokenizer.from_pretrained(n)
    text2 = tokenizer.decode(tokenizer.encode(text, add_special_tokens=False))
    
    if text2 == text:
        print('OK: ', n, repr(text2))
    else:
        print('ERR:', n, repr(text2))
```

You will get:
```
OK:  meta-llama/Llama-2-7b ' Who let the dog out?! !'
ERR: meta-llama/Meta-Llama-3-8B ' Who let the dog out?!!'
ERR: meta-llama/Llama-3.1-8B ' Who let the dog out?!!'
ERR: mistralai/Mistral-7B-v0.3 'Who let the dog out?! !'
OK:  mistralai/Mixtral-8x7B-v0.1 ' Who let the dog out?! !'
ERR: mistralai/Mixtral-8x22B-v0.1 'Who let the dog out?! !'
OK:  mistralai/Mistral-Nemo-Instruct-2407 ' Who let the dog out?! !'
OK:  mistralai/Mistral-Small-Instruct-2409 ' Who let the dog out?! !'
OK:  mistralai/Mistral-Large-Instruct-2407 ' Who let the dog out?! !'
ERR: microsoft/phi-1 ' Who let the dog out?!!'
ERR: microsoft/phi-1_5 ' Who let the dog out?!!'
ERR: microsoft/phi-2 ' Who let the dog out?!!'
OK:  microsoft/Phi-3-mini-4k-instruct ' Who let the dog out?! !'
OK:  microsoft/Phi-3.5-mini-instruct ' Who let the dog out?! !'
```

All marked with ERR cannot encode and then decode to the same string.",MachineLearning,35,13,1729240543.0,1g6dc8l,mtasic85,https://www.reddit.com/r/MachineLearning/comments/1g6dc8l/r_limitations_in_mainstream_llm_tokenizers/,Research
[D] What’s the Difference Between Increasing Batch Size and Packing Sequences with Attention Masking in LLM Training?,"I'm curious about the difference between the following two approaches when training large language models (LLMs) on fixed-length sequences:

    Using batch size = 4, where each sample has a sequence length of 1024 tokens, and they are treated independently.
    Packing 4 sequences together into one batch with a max sequence length of 4096 and applying an attention mask to ensure that no sequence attends to tokens from another sequence.

If the attention mask is correctly applied, ensuring no attention is paid to other sequences, is there a significant difference between these two approaches in terms of:

    Memory usage
    Computational cost
    Training dynamics

From what I understand, without the attention mask, packing would lead to a quadratic increase in computational cost due to the self-attention mechanism. But with masking, wouldn’t the computation and memory usage be almost the same as treating them as separate sequences in a batch? Or are there other factors I’m missing?",MachineLearning,35,16,1728222373.0,1fxguh6,JeanMichelRanu,https://www.reddit.com/r/MachineLearning/comments/1fxguh6/d_whats_the_difference_between_increasing_batch/,Discussion
"[D] Last Week in Medical AI: Top Research Papers/Models 🏅(September 14 - September 21, 2024)
","[Last Week in Medical AI: Top Research Papers\/Models 🏅\(September 14 - September 21, 2024\)](https://preview.redd.it/6b0dvmts9aqd1.jpg?width=1386&format=pjpg&auto=webp&s=b63ef279db604cf17988792741c4e625d17b9f04)

  
  
**Medical AI Paper of the Week**

* **How to Build the Virtual Cell with Artificial Intelligence: Priorities and Opportunities**
   * This paper proposes a vision for ""AI-powered Virtual Cells,"" aiming to create robust, data-driven representations of cells and cellular systems. It discusses the potential of AI to generate universal biological representations across scales and facilitate interpretable in-silico experiments using ""Virtual Instruments.""

**Medical LLM & Other Models**

* GP-GPT: LLMs for Gene-Phenotype Mapping
   * This paper introduces GP-GPT, the first specialized large language model for genetic-phenotype knowledge representation and genomics relation analysis. Trained on over 3 million terms from genomics, proteomics, and medical genetics datasets and publications.
* HuatuoGPT-II, 1-stage Training for Medical LLMs
   * This paper introduces HuatuoGPT-II, a new large language model (LLM) for Traditional Chinese Medicine, trained using a unified input-output pair format to address data heterogeneity challenges in domain adaptation.
* HuatuoGPT-Vision: Multimodal Medical LLMs
   * This paper introduces PubMedVision, a 1.3 million sample medical VQA dataset created by refining and denoising PubMed image-text pairs using MLLMs (GPT-4V).
* Apollo: A Lightweight Multilingual Medical LLM
   * This paper introduces ApolloCorpora, a multilingual medical dataset, and XMedBench, a benchmark for evaluating medical LLMs in six major languages. The authors develop and release Apollo models (0.5B-7B parameters)
* GMISeg: General Medical Image Segmentation

**Frameworks and Methodologies**

* CoD: Chain of Diagnosis for Medical Agents
* How to Build the Virtual Cell with AI
* Interpretable Visual Concept Discovery with SAM
* Aligning Human Knowledge for Explainable Med Image
* ReXErr: Synthetic Errors in Radiology Reports
* Veridical Data Science for Medical Foundation Models
* Fine Tuning LLMs for Medicine: The Role of DPO

**Clinical Trials**

* LLMs to Generate Clinical Trial Tables and Figures
* LLMs for Clinical Report Correction
* AlpaPICO: LLMs for Clinical Trial PICO Frames

**Medical LLM Applications**

* Microsoft's Learnings of Large-Scale Bot Deployment in Medical

....

Check the full thread in detail: [https://x.com/OpenlifesciAI/status/1837688406014300514](https://x.com/OpenlifesciAI/status/1837688406014300514)

Thank you for reading! If you know of any interesting papers that were missed, feel free to share them in the comments. If you have insights or breakthroughs in Medical AI you'd like to share in next week's edition, connect with us on Twt/x: [OpenlifesciAI](https://x.com/OpenlifesciAI)",MachineLearning,35,1,1726977103.0,1fmkhok,aadityaura,https://www.reddit.com/r/MachineLearning/comments/1fmkhok/d_last_week_in_medical_ai_top_research/,Discussion
[R] First Published ML Paper - From a quick glance does anything stand out in terms of peer review notes?,"# Long story short I've published my first paper through a conference proceeding, but my peer review was a little short. I am wondering if anyone here with experience in time series forecasting or XAI has any notes for me? would be kindly appreciated. No problems if not.

[https://dl.acm.org/doi/abs/10.1145/3674029.3674035](https://dl.acm.org/doi/abs/10.1145/3674029.3674035) (Is open access under ACM).",MachineLearning,38,11,1726655037.0,1fjpfvt,jnb_phd_ml_accy,https://www.reddit.com/r/MachineLearning/comments/1fjpfvt/r_first_published_ml_paper_from_a_quick_glance/,Research
[P] New open-source release: SOTA multimodal embedding models for fashion,"Hi All!

I am really excited to announce Marqo-FashionCLIP & Marqo-FashionSigLIP - two new state-of-the-art multimodal models for search and recommendations in the fashion domain. The models have surpassed current SOTA models FashionCLIP2.0, and OpenFashionCLIP on 7 fashion evaluation datasets including DeepFashion and Fashion200K, by up to 57%.

[Marqo-FashionCLIP](https://huggingface.co/Marqo/marqo-fashionCLIP) & [Marqo-FashionSigLIP](https://huggingface.co/Marqo/marqo-fashionSigLIP) are 150M parameter embedding models that:

* Outperform [FashionCLIP2.0](https://github.com/patrickjohncyh/fashion-clip), and [OpenFashionCLIP](https://github.com/aimagelab/open-fashion-clip) on all benchmarks (up to +57%).
* Are 10% faster for inference than FashionCLIP2.0, and OpenFashionCLIP.
* Use [Generalized Constrastive Learning (GCL)](https://github.com/marqo-ai/GCL) with SigLIP to optimize over seven fashion specific aspects including descriptions, titles, colors, details, categories, keywords and materials.
* Were benchmarked across 7 publicly available datasets and 3 tasks.

https://preview.redd.it/8kkyn2e61mid1.png?width=1459&format=png&auto=webp&s=e8bfa42faca752538b92e06e3dba3a7780007981

We are releasing Marqo-FashionCLIP and Marqo-FashionSigLIP under the Apache 2.0 license [here](https://huggingface.co/collections/Marqo/marqo-fashionclip-and-marqo-fashionsiglip-66b43f2d09a06ad2368d4af6).

# Benchmark Results

Here are the results across the 7 datasets. All values represent the relative improvement for precision/recall over the FashionCLIP2.0 baseline. You can find more details and the code to reproduce here [https://github.com/marqo-ai/marqo-FashionCLIP](https://github.com/marqo-ai/marqo-FashionCLIP). 

[Averaged recall\/precision @1 results across 7 datasets \(compared to FashionCLIP2.0 baseline\)](https://preview.redd.it/ofuenapm0mid1.png?width=1568&format=png&auto=webp&s=2c6641c8bc3aa41910ee694c05d8db5a62d60c2c)

  
Let me know any feedback or if there are other models you are interested in seeing being developed!

GitHub: [https://github.com/marqo-ai/marqo-FashionCLIP](https://github.com/marqo-ai/marqo-FashionCLIP)  
Blog: [https://www.marqo.ai/blog/search-model-for-fashion](https://www.marqo.ai/blog/search-model-for-fashion)

",MachineLearning,35,5,1723633213.0,1eryo73,Jesse_marqo,https://www.reddit.com/r/MachineLearning/comments/1eryo73/p_new_opensource_release_sota_multimodal/,Project
"[D] Introducing the Open Medical Reasoning Tasks Project : Open Source AI Is the Path Forward
","We're thrilled to announce the launch of the **Open Medical Reasoning Tasks** project by Open Life-Science AI. Inspired by the groundbreaking work of NousResearch we're embarking on an open, collaborative initiative to create a comprehensive list of medical reasoning tasks for Large Language Models (LLMs).

**Why This Matters**

As AI continues to transform healthcare, we need to ensure that open-source models can handle the complex reasoning tasks that medical professionals face daily. This project aims to create a robust set of benchmarks and training datasets specifically tailored for medical AI.

**Call to Action**

To all physicians, researchers, data scientists, and innovators pioneering the fusion of AI and healthcare.

**How You Can Contribute**

* **Share Your Expertise**: Help us define and refine medical reasoning tasks.
* **Join the Community**: Collaborate with medical professionals and AI researchers.
* **Shape the Future**: By contributing, you'll help create essential open medical seed tasks, robust synthetic datasets, and comprehensive benchmarks. This collaborative effort will pave the way for unbiased, highly capable open language models and AI systems in healthcare, ultimately enhancing patient care and medical research worldwide

**Current Progress**

* We've laid the groundwork with initial tasks, but we need YOUR specialized knowledge to expand and refine them. Check out our current task list here: [Open Medical Reasoning Tasks](https://github.com/openlifescience-ai/Open-Medical-Reasoning-Tasks/blob/main/tasks.md)
* Each task page contains its definition, medical examples, tags, and more.

**The Vision**

We believe that Open Source AI Is the Path Forward. By collaborating openly, we can create AI systems that truly understand and assist in medical reasoning, potentially revolutionizing patient care and medical research.

Let's shape the future of healthcare AI together!  
**#OpenMedicalAI #AIinHealthcare #MedicalInnovation**

https://preview.redd.it/w3eu4y241ygd1.png?width=1924&format=png&auto=webp&s=a2c85cf1dbd57d9fe4cce89071f79ac271e96819

",MachineLearning,35,9,1722906531.0,1el4sbc,aadityaura,https://www.reddit.com/r/MachineLearning/comments/1el4sbc/d_introducing_the_open_medical_reasoning_tasks/,Discussion
[P] TTSDS - Benchmarking recent TTS systems,"TL;DR - I made a benchmark for TTS, and you can see the results here: [https://huggingface.co/spaces/ttsds/benchmark](https://huggingface.co/spaces/ttsds/benchmark)

There are a lot of LLM benchmarks out there and while they're not perfect, they give at least an overview over which systems perform well at which tasks. There wasn't anything similar for Text-to-Speech systems, so I decided to address that with my latest project.

The idea was to find representations of speech that correspond to different factors: for example prosody, intelligibility, speaker, etc. - then compute a score based on the Wasserstein distances to real and noise data for the synthetic speech. I go more into detail on this in the paper (https://www.arxiv.org/abs/2407.12707), but I'm happy to answer any questions here as well.

I then aggregate those factors into one score that corresponds with the overall quality of the synthetic speech - and this score correlates well with human evluation scores from papers from 2008 all the way to the recently released [TTS Arena](https://huggingface.co/spaces/TTS-AGI/TTS-Arena) by huggingface.

Anyone can submit their own synthetic speech [here](https://huggingface.co/spaces/ttsds/benchmark). and I will be adding some more models as well over the coming weeks. The code to run the benchmark offline is [here](https://github.com/ttsds/ttsds).",MachineLearning,37,11,1721654977.0,1e9ec0m,cdminix,https://www.reddit.com/r/MachineLearning/comments/1e9ec0m/p_ttsds_benchmarking_recent_tts_systems/,Project
[D] Patenting in ML,"For those of you researching on the frontier of ML theory or applications in academia or industry, how often are you filing for patents before you publish or market the product? I know google and others have patented things that seem like algorithms, but are written into patents as an application or a computer system. How prevalent is it when you are breaking a SOTA advancement?

I see some other threads that suggest software patents aren't that enforceable: [\[D\] Can Google sue OpenAI for using the Transformer in their products? : ](https://www.reddit.com/r/MachineLearning/comments/10zzm18/d_can_google_sue_openai_for_using_the_transformer/)

While other comments say patents are more important than papers in industry: [https://www.reddit.com/r/MachineLearning/comments/mf24jr/comment/gsl3pux/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/MachineLearning/comments/mf24jr/comment/gsl3pux/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)

Anyways, I'm curious what people's experience has been.",MachineLearning,35,31,1719175605.0,1dmvu8t,SometimesObsessed,https://www.reddit.com/r/MachineLearning/comments/1dmvu8t/d_patenting_in_ml/,Discussion
[R] My learning notes for Auto-Encoding Variational Bayes (VAE),"Hi,

I am sharing my learning notes on the VAE paper https://maitbayev.github.io/posts/auto-encoding-variational-bayes/. It contains expanded proofs for the formulas from the paper.",MachineLearning,35,5,1735318390.0,1hnj63a,madiyar,https://www.reddit.com/r/MachineLearning/comments/1hnj63a/r_my_learning_notes_for_autoencoding_variational/,Research
[R] Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models,,MachineLearning,34,3,1732124871.0,1gvveu8,rcparts,https://arxiv.org/abs/2411.12580,Research
"[P] Attempting to replicate the ""Stretching Each Dollar"" diffusion paper, having issues","EDIT: I found the bug!

I was focused on making sure the masking stuff was correct, which it was, but i failed to see that after i unmask the patches (ie replace patches that the backbone missed with 0s), i reshape them back to the original shape, during which i pass them through a FFN output layer, which isnt linear so 0 inputs != 0 outputs. but the loss function expected 0 outputs at those places. So all i needed to do was make those bits 0 again, and now it works much much better  
-------  
  
I am attempting to replicate this paper: [https://arxiv.org/pdf/2407.15811](https://arxiv.org/pdf/2407.15811)

You can view my code here: [https://github.com/SwayStar123/microdiffusion/blob/main/microdiffusion.ipynb](https://github.com/SwayStar123/microdiffusion/blob/main/microdiffusion.ipynb)

I am overfitting to 9 images as a start to ensure sanity, but at lower masking ratios I cannot replicate the results in the paper

At masking ratio of 1.0, ie all patches are seen by the transformer backbone, it overfits to the 9 images very well

https://preview.redd.it/thteqn3rhlod1.png?width=1066&format=png&auto=webp&s=215fc88c74728f5f0dcfbd05a9b6a4db836b1f84

There are some mild distortions but perhaps some LR scheduling would help with that, main problem is as the masking ratio is reduced to 0.75, the output severely degrades:

https://preview.redd.it/ukcexjbyhlod1.png?width=1066&format=png&auto=webp&s=432187000cde0ba5b1b90813c6284c9f764a9979

At masking ratio 0.5, it is even worse:

https://preview.redd.it/00kzbpc0ilod1.png?width=1066&format=png&auto=webp&s=0717f8926582b2b5c694ebe5609b6f9fba8a088d

All of these are trained for the same number of steps, etc, all hyperparameters are identical apart from masking ratio

NOTE: I am using ""masking ratio"" to mean the percentage of patches that the transformer backbone sees, inverted from the papers perspective of it being the percentage of patches being hidden. I am near certain this is not the issue  
Im also using a x prediction target rather than noise prediction as in the paper, but this shouldnt really matter, and it works as can be seen at 1.0 masking ratio.

Increasing the number of patch mixing layers doesnt help, if anything it makes it worse

2 Patch mixing layers, 0.5 masking ratio:

https://preview.redd.it/punkf59uilod1.png?width=1066&format=png&auto=webp&s=04e0ea03d9ecd464f1bd007f7957c4a65c0ae9c2

4 patch mixing layers, 0.5 masking ratio:

https://preview.redd.it/9ihtiyvejlod1.png?width=1066&format=png&auto=webp&s=c78efb039b6ef630a3760dca60e2018d32e6c3b7

Maybe the patch mixer itself is wrong? Is using a TransformerEncoderLayer for the patch mixer a bad idea?",MachineLearning,35,10,1726245379.0,1ffz5xc,SwayStar123,https://www.reddit.com/r/MachineLearning/comments/1ffz5xc/p_attempting_to_replicate_the_stretching_each/,Project
[D] Is there an open truly multimodal LLM that isn't a toy model?,"Hi,

It's been a few months since gpt-4o came out and I have yet to find an equivalent open weights model. Gemini came out even before it and it had multimodal inputs. 

By equivalent, I mean a model that is early fusion and multimodal where vision and audio is tokenized and share the same embedding space as text tokens. I don't necessarily mean it has to have the same capabilities or accuracy. 

As far as I know Meta's chameleon is the closest match but it's bimodal (no audio support) and it can only generate text.

So my question is: is there a truly multimodal model that we can download and tun locally?
",MachineLearning,36,15,1725929507.0,1fd5hp7,Amgadoz,https://www.reddit.com/r/MachineLearning/comments/1fd5hp7/d_is_there_an_open_truly_multimodal_llm_that_isnt/,Discussion
[D] Efficient way to store large datasets,"I’m collecting trajectories for imitation learning (RL) and each trajectory is about 1500 time steps long, consists of 4 image streams of about 600x600 pixels. 
Obviously, the dataset size grows extremely quickly with the number of trajectories.

What are some good libraries for efficiently (in terms of disk space) storing such data? I tried h5py with level 9 gzip compression but the files are still way too large. Is there a better alternative? 

Saving and loading times do not really matter.

Most resources online are aimed at efficiently loading large datasets or handling them in memory which is not relevant for my question.

I already use uint8 as datatype for the rgb streams.

UPDATE: I ended up using lossy video compression via scikit-video. This results in a filesize of just 2MB instead of almost 2GB when storing raw frames in an array. A histogram of the reconstruction loss shows that most pixel differences are in the low single digit range which is not a problem in my case since I would apply domain randomisation through noise anyway.",MachineLearning,35,12,1725485047.0,1f951p8,CherubimHD,https://www.reddit.com/r/MachineLearning/comments/1f951p8/d_efficient_way_to_store_large_datasets/,Discussion
[R] Training LLMs to cite the pre-training data,"Our work got accepted at [COLM](https://colmweb.org/index.html) and thought It is worth sharing it here:  
 ""**Source-Aware Training Enables Knowledge Attribution in Language Models**""

TL;DR:

Normally, LLMs learn a lot of stuff during their training but don’t remember where they learned it from. The paper is about teaching LLMs to reference the sources of their knowledge from the pretraining data. This can make the models more transparent, easier to understand, and more reliable. We propose a two-step process: 1) Pretraining with document ID injection and 2) instruction tuning. The first stage teaches the model to link bits of knowledge to specific pretraining documents. The second stage teaches the model how to cite these documents when generating answers.

🔗 Paper : [https://arxiv.org/abs/2404.01019](https://arxiv.org/abs/2404.01019)

Code: [https://github.com/mukhal/intrinsic-source-citation](https://github.com/mukhal/intrinsic-source-citation)",MachineLearning,35,6,1721320982.0,1e6fxgj,moyle,https://www.reddit.com/r/MachineLearning/comments/1e6fxgj/r_training_llms_to_cite_the_pretraining_data/,Research
[D] Is OOD generalization still a future in the LLM era?,"I think OOD generalization is an important issue because it pulls in the distance from reality. But I am concerned that recent conferences like ICLR, ICML, NeurIPS etc. don't have many people working on this problem. And check out some OOD generalization benchmarks, many methods (Such as IRM, GroupDRO) are even weaker than ERM. So, I wonder if it's because of some difficulties in this field that people stopped studying it. Or is it because of some other reason.",MachineLearning,34,26,1718519028.0,1dh1eox,EducationalOwl6246,https://www.reddit.com/r/MachineLearning/comments/1dh1eox/d_is_ood_generalization_still_a_future_in_the_llm/,Discussion
[R] Tech report on FineWeb: decanting the web for the finest text data at scale,"The team behind the large FineWeb 15 trillions openly released web-scale dataset have just published an extensive blog post on the science of creating high quality web-scale datasets, detailing the steps and learnings that came in FineWeb, in a sort of [distill.pub](https://distill.pub) interactive article/blog fashion.

They also released FineWeb-Edu a filtered subset of Common Crawl with 1.3T tokens focusing on web pages with very high educational content and which seem to out-performs all openly release web-scale datasets on knowledge- and reasoning-intensive benchmarks like MMLU, ARC, and OpenBookQA

Interesting read: [https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)  
",MachineLearning,34,3,1717316259.0,1d68jjf,Thomjazz,https://www.reddit.com/r/MachineLearning/comments/1d68jjf/r_tech_report_on_fineweb_decanting_the_web_for/,Research
[R] Poisson Variational Autoencoder,"Preprint: [https://arxiv.org/abs/2405.14473](https://arxiv.org/abs/2405.14473)

X thread summary: [https://x.com/hadivafaii/status/1794467115510227442](https://x.com/hadivafaii/status/1794467115510227442)",MachineLearning,34,28,1716872218.0,1d2bhmw,vafaii,https://www.reddit.com/r/MachineLearning/comments/1d2bhmw/r_poisson_variational_autoencoder/,Research
[D] Real chances to be accepted in NeurIPS 2024 - Other conferences,"Hey!

This is my first time submitting to NeurIPS.

Does anyone know when the reviews are visible to the authors? August, or is it possible that earlier? If we have really bad reviews... The best thing is to exit the submission path, right? In that case, which alternatives do you recommend on those dates?

My topic is NN reliability, but I am always underconfident about my research and I always think that it is not enough, more if I think in a conference as Neurips. Do you think that everybody submits good papers or is there a large quantity of rubbish papers? I read a lot of bad opinions here about the reviewing process... So, I am a little afraid.

This year, there are 20000ish submissions. So, I don't know what to do, if continue the submission or submit to another conference. As the gap that I am filling is clear, I am sure that others are covering that gap and submitting it to NeurIPS. Is there any other conference that outputs the results first than NeurIPS? I am trying to think in a smart way. So hard to be a researcher...

Thank you!",MachineLearning,33,6,1715915200.0,1ctv9li,Sincerebri,https://www.reddit.com/r/MachineLearning/comments/1ctv9li/d_real_chances_to_be_accepted_in_neurips_2024/,Discussion
[D] Batch Normalization and effect on the gradients,"I have been reading this paper recently: [https://arxiv.org/pdf/1706.05350](https://arxiv.org/pdf/1706.05350), and I was trying to understand how the author derives the fact that the gradients scale inversely to the scale of the weights. This is the very first equation on page 3. If someone could give me a hint or an explanation I would really appreciate it.",MachineLearning,31,5,1735486162.0,1hoy9zk,Numerous_Talk7940,https://www.reddit.com/r/MachineLearning/comments/1hoy9zk/d_batch_normalization_and_effect_on_the_gradients/,Discussion
[R] How difficult is this dataset REALLY?,"New Paper Alert!

Class-wise Autoencoders Measure Classification Difficulty and Detect Label Mistakes

We like to think that the challenge in training a classifier is handled by hyperparameter tuning or model innovation, but there is rich inherent signal in the data and their embeddings.  Understanding how hard a machine learning problem is has been quite elusive.  Not any more.

Now you can compute the difficulty of a classification dataset without training a classifier, and requiring only 100 labels per class.  And, this difficulty estimate is surprisingly independent of the dataset size.

Traditionally, methods for dataset difficulty assessment have been time and/or compute-intensive, often requiring training one or multiple large downstream models. What's more, if you train a model with a certain architecture on your dataset and achieve a certain accuracy, there is no way to be sure that your architecture was perfectly suited to the task at hand — it could be that a different set of inductive biases would have led to a model that learned patterns in the data with far more ease.

Our method trains a lightweight autoencoder for each class and uses the ratios of reconstruction errors to estimate classification difficulty. Running this dataset difficulty estimation method on a 100k sample dataset takes just a few minutes, and doesn't require tuning or custom processing to run on new datasets!

How well does it work? We conducted a systematic study of 19 common visual datasets, comparing the estimated difficulty from our method to the SOTA classification accuracy. Aside from a single outlier, the correlation is 0.78. It even works on medical datasets!



Paper Link:  https://arxiv.org/abs/2412.02596

GitHub Repo Linked in Arxiv pdf",MachineLearning,32,20,1733846681.0,1hb54nd,ProfJasonCorso,https://www.reddit.com/r/MachineLearning/comments/1hb54nd/r_how_difficult_is_this_dataset_really/,Research
[D] [Project] JAX ML Framework; Write neural networks and more; shorter and faster; What are your thoughts?,"Made a JAX framework for machine learning because I wanted to code faster & shorter so I made zephyr. I hope it might be helpful to you guys too and wanted to hear some feedback.

Link in the comments.

Nothing wrong with current frameworks, this is just another way of doing things. 

NNs or ML algorithms to me, are just pure/mathematical functions and so I wanted that to reflect in my code. With other frameworks it comes in at least 2 steps: initialization in the constructor and a computation in the forward/call body. This seems fine at first but when models become larger, it's 2 places where I have to synchronize code. - If I change a computation, I might need to change a hyperparameter somewhere, or if I change a hyperparameter, I might need to change a computation - or if i have to re-read my code, i have to read in at least 2 places. I usually use a small window for an editor and so jumping between these could a hassle (putting them side by side is another solution).

Another thing I was experiencing was that if I was doing something that is not neural networks, for example if an algorithm was easier to do with a recursive call (but with different trainable weights for each call), that would be challenging in other frameworks. So while they generic computational graph-frameworks, some computations are hard to do. 

To me, computations was about passing data around and getting them to transform, so this \`act\` of transforming data should be that focus of the framework. That's what I did with zephyr. Mathematical functions are python functions, no need for initialization in a constructor. You use the functions(networks or layers, etc) when you need them. No need for constructors, allows recursions, allows you to focus on the transformations or operations. Zephyr handles weight creation and management for you - it is explicit tho unlike other frameworks; you carry around a \`params\` tree, and that should be no problem, since that's a core of the computation and shouldn't be hidden away.

In short, zephyr is  short but readable aimed at people developing research ideas about ML. The README has a few samples for neural networks. I hope you guys like it and try it.

",MachineLearning,32,5,1732332331.0,1gxqag6,Pristine-Staff-5250,https://www.reddit.com/r/MachineLearning/comments/1gxqag6/d_project_jax_ml_framework_write_neural_networks/,Discussion
[D] Do I need to connect more?,"I am currently finishing up my third year PhD and would most probably be graduating in another year.
So far I been pretty much working on my own. 

As a matter of fact, all my publications are first-author and most of the contributions to these papers are from PI or Co-PI who does very surface-level checks (grammars etc). I basically did not get involved with any work with other people and have been working solely on my own for these 3 years. Thus I don’t have much connections outside my lab group ( I actually don’t even know them well at all ). I see a lot of publications have quite a handful of authors, and most comprising of multiple organisations.

The thing is I was hoping to get an industrial job after graduation, would me being relatively unknown/unheard actually be a problem? 

I find it hard to find connections outside my lab. Most of my labmates don’t work in similar research areas. How do you guys actually connect or find collaborators outside of your lab zone? 

Would an overseas attachment help me? 
This is also one of the main reason why I am trying to find an internship (still very competitive), which is to collaborate or to be mentored by experienced people.
",MachineLearning,34,9,1732215072.0,1gwn0rn,AmbitiousSeesaw3330,https://www.reddit.com/r/MachineLearning/comments/1gwn0rn/d_do_i_need_to_connect_more/,Discussion
[D] To what cross-entropy loss value can LLMs converge?,"LLMs are usually evaluated on benchmarks that aim to measure broad abilities. However, most publishers of foundational models do not publish the actual cross-entropy loss value that the model achieves at the end of training. I couldn't find any sources on this, but I would like to know what loss value the LLMs can achieve on human language. Is there anyone who knows more about this? Might there be some lower bound?",MachineLearning,34,19,1730819960.0,1gk92rs,cbl007,https://www.reddit.com/r/MachineLearning/comments/1gk92rs/d_to_what_crossentropy_loss_value_can_llms/,Discussion
Latent Diffusion in pure-torch (no huggingface dependencies) [P],"Been fiddling with diffusion for the last year and I decided to release a package with my implementation from scratch of DDPM latent diffusion models. It includes implementations for both the denoising UNet and the VAE+GAN used to embed the image.

It's pure torch, as I find Huggingface diffuser's good for simple tasks but if you want to learn how the inners work or to hack the model a bit, it falls short as the codebase is humongous and not geared towards reusability of components (but I insist is a good library for its purposes). To install it simply run

`pip install tiny-diff`

I aimed to create a reusable implementation, without any ifs in the forward methods (squeezing polymorphism as much as I could so the forward is as clear as possible) and modular components (so if you don't want to use the whole model but parts of it you can grab what you want)

Repo Link: [https://github.com/AlejandroBaron/tiny-diff](https://github.com/AlejandroBaron/tiny-diff)",MachineLearning,35,7,1726904641.0,1flxs7d,AIlexB,https://www.reddit.com/r/MachineLearning/comments/1flxs7d/latent_diffusion_in_puretorch_no_huggingface/,Project
[D] An Intuitive Explanation of How LLMs Work,"Hi,

I have written a blog post explaining how LLMs work in a very intuitive way. 

We start from a high level of abstraction where LLMs are viewed as personal assistants, and then dive deeper as we go and cover concepts such as tokenization, sampling and embeddings.

I have added a few figures to illustrate some of the concepts in a visual way. I also addressed some of the limitations of current LLMs such as failing to count the Rs in ""strawberry"" and reversing the string ""copenhagen"".

I hope you find it helpful!

If you have any feedback or questions, please let me know.  
  
[https://medium.com/@amgad-hasan/explaining-how-llms-work-in-7-levels-of-abstraction-3179de558686](https://medium.com/@amgad-hasan/explaining-how-llms-work-in-7-levels-of-abstraction-3179de558686)

EDIT:
There is a substack link a comment below for those who don't like medium.",MachineLearning,33,21,1726688239.0,1fk1jhj,Amgadoz,https://www.reddit.com/r/MachineLearning/comments/1fk1jhj/d_an_intuitive_explanation_of_how_llms_work/,Discussion
[D] How does Llama3.1 support multiple languages with only 28k additional tokens in its vocabulary?,"From the Llama3.1 paper, the vocabulary consists of 128k tokens, with 100k dedicated to English and 28k allocated for non-English languages. Given that languages like Korean, Chinese, and Japanese do not use the 26-letter Latin alphabet and can have millions of unique characters, how is it possible to cover these languages with just 28k tokens? Is the tokenization based on Unicode?",MachineLearning,31,12,1722074388.0,1edct9i,Financial_Air5256,https://www.reddit.com/r/MachineLearning/comments/1edct9i/d_how_does_llama31_support_multiple_languages/,Discussion
"[D] Every annotator has a guidebook, but the reviewers don't","I submitted to the ACL rolling review in June and found the reviewers' evaluation scores very subjective.

Although the ACL committee has an instruction on some basic reviewing guidelines, there lacks of a preliminary test for the reviewers to explicitly show the evaluation standards. Maybe we should provide some paper-score examples to prompt the reviewers for more objective reviews? Or build a test before they make reviews to make sure they fully understand the meaning of soundness and overall assessment, rather than giving some random scores based on their personal interests.",MachineLearning,33,10,1722000048.0,1ecnxng,Spico197,https://www.reddit.com/r/MachineLearning/comments/1ecnxng/d_every_annotator_has_a_guidebook_but_the/,Discussion
[R] Why there are few high-quality works about federated learning with time series forecasting?,"I am starting an internship and my mentor asks me to conduct some research on federated learning for time series forcasting. Compared with other tasks like cv and nlp, I found there are very few high quality papers from good-ranking conferences/journals. Does anyone know why? What's the main challenge?",MachineLearning,33,24,1719261118.0,1dnnlpm,by0724,https://www.reddit.com/r/MachineLearning/comments/1dnnlpm/r_why_there_are_few_highquality_works_about/,Research
[D] which universities and research centers are focusing on adversarial machine learning (especially in Germany) ?,"Hey people, I'm curious about where core adversarial learning research is going on. Looking at the publications, I do see several papers from various organizations but rarely do I see an organization advertise that they are focusing on adversarial learning as a field. Especially so for the universities and institutes in Germany. ",MachineLearning,33,29,1719233780.0,1dnctew,i_sanitize_my_hands,https://www.reddit.com/r/MachineLearning/comments/1dnctew/d_which_universities_and_research_centers_are/,Discussion
[Discussion] Why next token prediction doesn't work for Recommender System? (or am I wrong?),"I'm working on a research project that aims at applying next-token-prediction models to build/improve recommender systems. As a feasibility assessment study, I built and trained a GPT model to predict the next product to buy using the Instacart dataset. To be more specific, I treated each product\_id as a ""word"", each order as a ""sentence"" and each user's transaction history as a ""document"".

However, after 4 hours of training on a T4 GPU, the mean average precision at 10 (MAP@10) on the eval set is still only 0.075. For comparison, MAP@10 for the baseline popular product (top 10 most popular products in the user's personal transaction history) is already 0.251.

Although I can see one or two ways to improve the model, the low performance compared to a very simple baseline is really discouraging and makes me think this approach is not feasible at all. I would like to discuss a few points:

1. Is it true that next-token prediction (specifically decoder-only transformer architectures) doesn't work for this problem at any scale?
- Support: There are a lot of differences between the text data and the e-commerce transaction data, so a method that works for text might not work for transactions, it's no surprise.
- Against: 4 hours of training is only 1.5 epochs, so the current model may be underfitted. Therefore, the low performance could just be a function of training time and it might improve if I train for 2 days

2. Any resources I should read into this? I'm aware that the approach I'm taking is similar to session-based recsys models, but I only found one paper HierTCN (You et al., WWW 2019). Would appreciate any additional suggestions to read into this.

3. Any suggestion to help train the model better/faster? The current configuration is:
- Data: vocab\_size = 50000 (50K products), 200K users (each epoch is 200K training examples)
- Model: n\_layer=9, d\_model=512, n\_head=16 (similar to Gopher 44M-parameter model), block\_size=1024
- Training: batch\_size = 4, learning\_rate = 5e-4, optimizer = AdamW

Thank you!",MachineLearning,31,28,1717370640.0,1d6qfbc,Pancake502,https://www.reddit.com/r/MachineLearning/comments/1d6qfbc/discussion_why_next_token_prediction_doesnt_work/,Discussion
[R] Why In-Context Learning Transformers are Tabular Data Classifiers,"We are introducing TabForestPFN, which is an in-context learning transformer that can predict tabular data classification tasks. In the past, tabular data classification was dominated by tree-based algorithms like XGBoost and CatBoost, but now we are finally closing this gap using pretrained transformers.

https://preview.redd.it/c3unlgi1cq2d1.png?width=2690&format=png&auto=webp&s=cd414509a31a189df288668e928d52e5723df3fc

In-context learning transformers were introduced to tabular data classification by Hollman et al. (ICLR, 2023) in [TabPFN](https://arxiv.org/pdf/2207.01848). This work is limited by the GPU memory, so it only considers datasets with fewer than a thousand observations. We improve their model by adding a fine-tuning stage, which circumvents the GPU memory limitation. Also, we introduce an additional synthetic data *forest* generator to further boost the performance. The result is [TabForestPFN](https://arxiv.org/pdf/2405.13396).

The focus of the TabForestPFN [paper](https://arxiv.org/pdf/2405.13396) is about *why* we can pretrain on tabular data. In language and vision, pretraining can learn grammar and textures, so pretraining makes sense. But in tabular data, the datasets in pretraining share no features or labels with the real-world datasets of interest, so what could it even learn? In the paper, we argue in-context learning transformers learn the ability to create complex decision boundaries. If you are interested in the reasoning, give it a read.

Code is available at [https://github.com/FelixdenBreejen/TabForestPFN](https://github.com/FelixdenBreejen/TabForestPFN)  
With the code, you can reproduce all our pretraining, experiments and analysis, and it also includes some basic examples for you to immediately use the classifier on your own datasets.

Below are the results of TabForestPFN on two tabular data classification benchmarks. I am the author, so if there are any questions, feel free to ask.

https://preview.redd.it/1tavhybhzq2d1.png?width=1174&format=png&auto=webp&s=214fb394a229544dfd8b44677d7880852c5d222f

https://preview.redd.it/9tg9z9tobq2d1.png?width=832&format=png&auto=webp&s=9fd3c732be8d5e18c8f56bb2b0e1c94796968056",MachineLearning,33,7,1716710955.0,1d0wnjf,FelixdenBreejen,https://www.reddit.com/r/MachineLearning/comments/1d0wnjf/r_why_incontext_learning_transformers_are_tabular/,Research
"[P] ReproModel: Open Source ML Research Toolbox. 
","Hi, I’m a PhD in Computer Science, and I’ve just developed what I think is a great leap for Machine Learning research. I open sourced the app for everyone to check out, and all feedback and contributions are welcome.

ReproModel is a no-code toolbox that enables scientists and researchers to test and reproduce ML models efficiently. A large subset of research time is wasted trying to test models from existing papers. To replicate or test results, you’d have to look into the provided code, and mimic all the config files and experiment conditions, i.e data loaders, preprocessing, optimizers etc. 

The toolbox takes all of that away through fetching config files from existing papers (will be available soon), loading models directly, and testing them on your data through simple checkboxes and dropdown menus. Customization is, of course, possible and encouraged.

You can find the repo here. Of course, more work has to be done, but I am approaching this step-by-step to ensure future compatibility and reusability of code.  
[https://github.com/ReproModel/repromodel](https://github.com/ReproModel/repromodel)

Appreciate your time, comments, and support with this!",MachineLearning,35,7,1716452313.0,1cynq6x,MintOwlTech,https://www.reddit.com/r/MachineLearning/comments/1cynq6x/p_repromodel_open_source_ml_research_toolbox/,Project
[P] jaxsplat: 3D Gaussian Splatting for JAX,"I created jaxsplat which provides CUDA-accelerated 3D Gaussian Splatting for JAX.
The original INRIA code and gsplat's implementation contain dynamically shaped arrays unsuitable for usage with JAX.
Instead, I modified gsplat's CUDA implementation to expose custom XLA CUDA calls while not leaking any dynamic shapes into JAX-side code.

Take a look if you're interested in exploring 3D Gaussian Splatting with JAX:

GitHub: https://github.com/yklcs/jaxsplat

Docs: https://jaxsplat.readthedocs.io",MachineLearning,31,5,1715829816.0,1ct3knc,RocketLL,https://www.reddit.com/r/MachineLearning/comments/1ct3knc/p_jaxsplat_3d_gaussian_splatting_for_jax/,Project
[R] I made an app to predict ICML paper acceptance from reviews,"[https://www.norange.io/projects/paper\_scorer/](https://www.norange.io/projects/paper_scorer/)

A couple of years ago, u/programmerChilli [analyzed](https://www.reddit.com/r/MachineLearning/comments/im65ia/r_i_made_a_website_for_predicting_whether_your/) ICLR 2019 reviews data and trained a model that rather accurately predicted acceptance results for NeurIPS.

I've decided to continue this analysis and trained a model (total \~6000 parameters) on newer NeurIPS reviews, which has twice as many reviews compared to ICLR 2019. Additionally, review scores system for NeurIPS has changed since 2019, and here is what I've learned:

1) Both conferences consistently reject nearly all submissions scoring <5 and accept those scoring >6. The most common score among accepted papers is 6. An average rating around 5.3 typically results in decisions that could go either way for both ICML and NeurIPS, suggesting that \~5.3 might be considered a soft threshold for acceptance.

2) Confidence scores are less impactful for borderline ratings such as 4 (borderline reject), 5 (borderline accept), and 6 (weak accept), but they can significantly affect the outcome for stronger reject or accept cases. 

For instance, with ratings of \[3, 5, 6\] and confidences of \[\*, 4, 4\], changing the ""Reject"" confidence from 5 to 1 shifts the probabilities from 26.2% - 31.3% - 52.4% - 54.5% - 60.4%, indicating that lower confidence in this case increases your chances.

Conversely, for ratings \[3, 5, 7\] with confidences \[4, 4, 4\], the acceptance probability is 31.3%, but it drops to 28.1% when the confidence changes to \[4, 4, 5\]. Although it might seem counterintuitive, a confidence score of 5 actually decreases your chances. One possible explanation is that many low-quality reviews rated 5 are often discounted by the Area Chairs (ACs).

Hope this will be useful, and thanks to u/programmerChilli for the inspiration!

I also discussed this topic in a series of [tweets](https://x.com/nikitadurasov/status/1782746231082488131).

",MachineLearning,33,0,1713961389.0,1cbwsr2,Lavishness-Mission,https://www.reddit.com/r/MachineLearning/comments/1cbwsr2/r_i_made_an_app_to_predict_icml_paper_acceptance/,Research
[D] Should I transfer to recommendation algorithms?,"I'm working on an ""LLM"" team right now or at least that's how it was advertised it's honestly just classification using LLMs not really interesting. I got an offer to join another team in my company that does recommendation. I thought recommendation is a very solid field to join, but very competitive. What are your guys' experience working in recommendation?",MachineLearning,31,38,1731634964.0,1grl7gk,DolantheMFWizard,https://www.reddit.com/r/MachineLearning/comments/1grl7gk/d_should_i_transfer_to_recommendation_algorithms/,Discussion
[D] PhD or worklife?,"I’ll be done with my masters in Human Centered AI this February, and I had honestly looked forward to be able to relax during my evenings without having to worry about school, while also being quite sad by the thought of no longer going to UNI as I’ve loved every single moment of it, both with friends and through learning. 

I’ve just been offered a PhD stipend by my masters thesis supervisor, this came completely out of the blue for me - as I didn’t realize I was anywhere near good enough for a phd. I love learning, the topic sounds super interesting, and I already am kind of “tired” of having to do regular small data science tasks for the rest of my life in a smallish company, like the one I work at currently.

However, my question is this? How much work is a PhD really? I love learning, but I got very surprised by this opportunity, so I’m not quite sure what to think of it yet",MachineLearning,34,23,1730969617.0,1glm6j9,Hmm_okay_jeps,https://www.reddit.com/r/MachineLearning/comments/1glm6j9/d_phd_or_worklife/,Discussion
"[P] I made a tool for building and training neural networks visually, operation by operation ","Hey! I mostly made this as a tool to learn how to implement backpropagation and get some intuition on how it works, so I figure it might be useful for someone else! I also wrote up an article in the readme on how backpropagation and model training works: [https://github.com/PavleMiha/mlgarden](https://github.com/PavleMiha/mlgarden)

Does this seem useful to you? Is this something you'd play around with? I can't really figure out what to do with it, so I'm curious to hear the community's thoughts!",MachineLearning,34,11,1730911856.0,1gl30b0,Massena,https://www.reddit.com/r/MachineLearning/comments/1gl30b0/p_i_made_a_tool_for_building_and_training_neural/,Project
"Building a Model Recommendation System: Tell Us What You’re Building, and We’ll Recommend the Best AI Models for It! [D]","**Hey Reddit!**

We’re working on something that we think could make model discovery a LOT easier for everyone: **a model recommendation system** where you can just **type what you're working on in plain English**, and it'll suggest the best AI models for your project. 🎉

# 💡 How it works:

The main idea is that you can **literally describe your project** in **natural language**, like:

* ""I need a model to generate summaries of medical research papers.""
* ""I'm building a chatbot for customer support.""
* ""I want a model that can analyze product reviews for sentiment.""

And based on that input, the system will recommend the best models for the job! **No deep diving into technical specs, no complex filters—just solid recommendations based on what you need.**

# 🌟 What else we’re building:

Alongside the model suggestions, we’re adding features to make the platform super user-friendly:

* **Detailed model insights**: You’ll still get all the technical info, like performance metrics, architecture, and popularity, to compare models.
* **Advanced search & filters**: If you’re more hands-on, you can filter models by task, framework, or tags.
* **Personalized suggestions**: The system will get smarter over time and offer more relevant suggestions based on your past usage.

# Why we need your feedback:

We want this platform to actually solve problems for people in the AI/ML space, and that’s where **you** come in! 🙌

1. **Does a tool like this sound helpful to you?**
2. **What features do you think are missing from model platforms like Hugging Face?**
3. **Are there any specific features you’d want to see, like performance comparisons or customization options?**
4. **How could we make the natural language input even more useful for recommending models?**

# TL;DR:

We’re building a tool where you can just **describe your project** in plain English, and it’ll **recommend the best AI models** for you. No need for complex searches—just type what you need! Looking for your feedback on what you'd want to see or any features you think are missing from current platforms like Hugging Face.

We'd love to hear your thoughts and ideas! What would make this platform super useful for you? Let us know what you think could improve the model discovery process, or what’s lacking in existing platforms!

Thanks in advance, Reddit! 😊",MachineLearning,32,23,1729663070.0,1ga3h0j,O2MINS,https://www.reddit.com/r/MachineLearning/comments/1ga3h0j/building_a_model_recommendation_system_tell_us/,Discussion
[R] Composite Learning Units: Generalized Learning Beyond Parameter Updates to Transform LLMs into Adaptive Reasoners,,MachineLearning,33,4,1728671867.0,1g1gtns,jalabulajangs,https://arxiv.org/abs/2410.08037v1,Research
[D] Mechanistic Interpretability Paper Discussion on Yannic Kilcher's discord,"Continuing on the Anthropic’s Transformer Circuit series and as a part of **daily paper discussions** on the [Yannic Kilcher](https://www.linkedin.com/in/ykilcher/) discord server, I will be volunteering to lead the analysis of the following mechanistic interpretability work 🧮 🔍

📜 **Toy Models of Superposition** authored by [Nelson Elhage](https://www.linkedin.com/in/nelhage/), [Tristan Hume](https://www.linkedin.com/in/tristan-hume-0a81b53b/), [Catherine Olsson](https://www.linkedin.com/in/catherineolsson/), [Nicholas Schiefer](https://www.linkedin.com/in/nicholasschiefer/), et al.  
🌐 [https://transformer-circuits.pub/2022/toy\_model/index.html](https://transformer-circuits.pub/2022/toy_model/index.html)

🕰 Friday, Sep 19, 2024 12:30 AM UTC // Friday, Sep 19, 2024 6.00 AM IST // Thursday, Sep 18, 2024 5:30 PM PT

Previous Mechanistic Interpretability papers in this series that we talked about:  
🔬 [Softmax Linear Units](https://transformer-circuits.pub/2022/solu/index.html)  
🔬 [In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)  
🔬 [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html)

Join in for the fun \~ [https://ykilcher.com/discord](https://ykilcher.com/discord)

[Toy Models of Superposition](https://preview.redd.it/axf0j2kyjspd1.png?width=1198&format=png&auto=webp&s=b860d73603eb0b5736a10994a81eaa80e95c5009)",MachineLearning,32,22,1726762804.0,1fkouvg,CATALUNA84,https://www.reddit.com/r/MachineLearning/comments/1fkouvg/d_mechanistic_interpretability_paper_discussion/,Discussion
[Discussion] Beat GPT-4o at Python by searching with 100 dumb LLaMAs,"> One thing that should be learned from the bitter lesson is the great
> power of general purpose methods, of methods that continue to scale
> with increased computation even as the available computation becomes
> very great. The two methods that seem to scale arbitrarily in this way
> are _search_ and _learning_.
>
> Richard Sutton, _The Bitter Lesson_

The eponymously distasteful take-away of Richard Sutton’s [essay](http://www.incompleteideas.net/IncIdeas/BitterLesson.html?ref=blog.heim.xyz) has often been misconstrued: because scale is all you need, they say, smaller models are doomed to irrelevance. The rapid increase in model size above one trillion parameters and the technological limitations of GPU memory together seemed to foreclose on economical frontier intelligence anywhere except at an oligopoly of intelligence-as-a-service providers. Open models and self-serve inference were in retreat.

But as the quote above indicates, there are in fact two arrows in the scaling quiver: learning and search. Learning, as we do it now with neural networks, scales with _memory_ at inference time — larger models perform better, ceteris paribus, because they can extract more data from their training set into more [circuits](https://transformer-circuits.pub/) and more [templates](https://arxiv.org/abs/2305.18654). Search scales smoothly with _compute_ at inference time — compute that can be spent on either producing higher quality candidates or on producing more candidates. In the ideal case, the scaling behavior can be predicted via so-called scaling laws.

Recent papers indicate that generative models like LLMs can be scaled up with search. The [Large Language Monkeys](https://arxiv.org/abs/2407.21787) paper, published on arXiv by Brown, Juravsky, and co-authors last week, includes several results in this vein and indicates that frontier-level intelligence in certain domains can be elicited from smaller models that can run on a single, past-generation GPU.
Further, they observed smooth, predictable improvement of performance with scale.

Put more simply: where before, it seemed frontier capabilities required [one horse-sized duck](https://knowyourmeme.com/memes/horse-sized-duck), it is clear we can now alternatively get them with one hundred duck-sized horses (or, rather, LLaMAs).

This weekend, we set out to replicate this finding.

### Scaling LLaMA 3.1 8B HumanEval on Modal

Running all of our experiments, including configuration and testing, cost well under $50.

You can find our code [here](https://gist.github.com/charlesfrye/27f25188dbbcfdf20a83c0230020fe05). You can run it yourself without exceeding the $30/month in credits included in [Modal’s free tier](/pricing).

### Metrics and data: HumanEval and pass@k

[**Continued in original post...**](https://modal.com/blog/llama-human-eval)",MachineLearning,32,9,1722966065.0,1elo2d1,thundergolfer,https://www.reddit.com/r/MachineLearning/comments/1elo2d1/discussion_beat_gpt4o_at_python_by_searching_with/,Discussion
"[D] For those of you doing ML in bio, what does ""good"" data look like for you? ","What does a ""good"" dataset look like by your standards? What discredits it? Where are you finding these datasets and how much time do you spend preprocessing them?",MachineLearning,33,15,1722799029.0,1ek3c6w,solutions_architect,https://www.reddit.com/r/MachineLearning/comments/1ek3c6w/d_for_those_of_you_doing_ml_in_bio_what_does_good/,Discussion
[P] scikit-activeml: An Active Learning Library in Python,">**TL;DR:** What are interesting features and current research trends in active learning that should be included in our active learning library [scikit-activeml](https://github.com/scikit-activeml/scikit-activeml)?

Hey guys,

We’ve been working on `scikit-activeml` for a few years, and we've just released version [0.5.0](https://pypi.org/project/scikit-activeml/) with many new features.

**What is** `scikit-activeml`?

`scikit-activeml` is a comprehensive Python library built on top of [scikit-learn](https://github.com/scikit-learn/scikit-learn). It provides an easy-to-use interface for active learning strategies, enabling efficient data labeling by selectively choosing the most ""informative"" samples.

**What are the key features of** `scikit-activeml`?

* Implementation and overview of many (state-of-the-art) [active learning strategies](https://scikit-activeml.github.io/scikit-activeml-docs/generated/strategy_overview.html) from research papers.
* Support of various learning paradigms, ranging from pool-based and stream-based active learning to classification and regression tasks, including strategies considering multiple erroneous annotators.
* Extensive [documentation](https://scikit-activeml.github.io/scikit-activeml-docs/) with many [visualizations](https://scikit-activeml.github.io/scikit-activeml-docs/generated/sphinx_gallery_examples/index.html) and [tutorials](https://scikit-activeml.github.io/scikit-activeml-docs/tutorials.html) on varying use cases, e.g., self-supervised learning features to boost active learning or a simple interface to label new datasets.
* Integration of other frameworks like [skorch](https://github.com/skorch-dev/skorch) for deep active learning and [river](https://github.com/online-ml/river) for stream-based active learning.

**Which features and trends in active learning should**`scikit-activeml`**support?**

We would like to discuss what you think are important features and research trends in active learning. Currently, we focus on the following aspects:

* **Meaningful Evaluation**: We perform a large-scale benchmark comparing active learning strategies across various data domains and tasks for different models and active learning setups. The results will be published on an interactive website where users can plot learning curves and download results. Additionally, we are working on integrating active learning tasks into [openml](https://www.openml.org/) to allow users to make their results easily public and comparable.
* **More Learning Paradigms**: We are currently focusing on regression and classification tasks, including scenarios with noisy labels from multiple error-prone annotators. Given the importance of applications with multiple target variables, we aim to explore multi-output active learning strategies for regression and classification (i.e., multi-label) in the future.
* **Deep Active Learning**: We have started incorporating deep active learning strategies such as DAL, CoreSet, BADGE, and TypiClust. We aim to continue these implementation efforts, particularly with self-supervised learning features.

Do you have any further ideas of features and trends we should consider as active learning researchers and practitioners?

**How to contribute to** `scikit-activeml`?

We are always looking for helpful contributions in various forms:

* Join our team of open-source developers.
* Point out bugs in the code and documentation.
* Request new features, e.g., novel active learning strategies (even your own ones).

Feel free to contact us in the comments, via [issues](https://github.com/scikit-activeml/scikit-activeml/issues), or by direct text message on Reddit.

  
GitHub: [https://github.com/scikit-activeml/scikit-activeml/tree/master](https://github.com/scikit-activeml/scikit-activeml/tree/master)

Documentation: [https://scikit-activeml.github.io/scikit-activeml-docs/](https://scikit-activeml.github.io/scikit-activeml-docs/)",MachineLearning,32,12,1721743224.0,1ea8kc8,ScienceAnnotator,https://www.reddit.com/r/MachineLearning/comments/1ea8kc8/p_scikitactiveml_an_active_learning_library_in/,Project
"[P] New collection of Llama, Mistral, Phi, Qwen, and Gemma models for function/tool calling","Introducing Rubra v0.1: a Collection of Open-Weight, Tool-Calling LLMs

Try it out [here](https://huggingface.co/spaces/sanjay920/rubra-v0.1-function-calling) in Hugging Face Spaces for free!

We also extended vLLM and llama.cpp so you can get started really easily. Check out our docs: [Rubra Documentation](https://docs.rubra.ai/)

| Model                                                     | Function Calling | MMLU (5-shot) | GPQA (0-shot) | GSM-8K (8-shot, CoT) | MATH (4-shot, CoT) | MT-bench |
|-----------------------------------------------------------|------------------|---------------|---------------|----------------------|--------------------|----------|
| [**Rubra Llama-3 70B Instruct**](https://huggingface.co/rubra-ai/Meta-Llama-3-70B-Instruct)       | 97.85%           | 75.90         | 33.93         | 82.26                | 34.24              | 8.36     |
| [**Rubra Llama-3 8B Instruct**](https://huggingface.co/rubra-ai/Meta-Llama-3-8B-Instruct)        | 89.28%           | 64.39         | 31.70         | 68.99                | 23.76              | 8.03     |
| [**Rubra Qwen2 7B Instruct**](https://huggingface.co/rubra-ai/Qwen2-7B-Instruct)                 | 85.71%           | 68.88         | 30.36         | 75.82                | 28.72              | 8.08     |
| [**Rubra Mistral 7B Instruct v0.3**](https://huggingface.co/rubra-ai/Mistral-7B-Instruct-v0.3)   | 73.57%           | 59.12         | 29.91         | 43.29                | 11.14              | 7.69     |
| [**Rubra Phi-3 Mini 128k Instruct**](https://huggingface.co/rubra-ai/Phi-3-mini-128k-instruct)   | 65.71%           | 66.66         | 29.24         | 74.09                | 26.84              | 7.45     |
| [**Rubra Mistral 7B Instruct v0.2**](https://huggingface.co/rubra-ai/Mistral-7B-Instruct-v0.2)   | 69.28%           | 58.90         | 29.91         | 34.12                | 8.36               | 7.36     |
| [**Rubra Gemma-1.1 2B Instruct**](https://huggingface.co/rubra-ai/gemma-1.1-2b-it)               | 45.00%           | 38.85         | 24.55         | 6.14                 | 2.38               | 5.75     |


### Why We Created These Models

Though the gap in capabilities has been closing between proprietary and open-source models, we saw function/tool calling still lagged behind in open source.

Until now, there have been limited options to get LLMs to output reliable function calls the same way you can get OpenAI and Anthropic to do so. Prompt engineering, output parsing, and JSON grammar is a hacky option. The other option has been models that do function calling, such as Berkeley Gorilla, NexusRaven, Hermes, Command-R+, but all of them are pinned to a model and some are not realistic in agentic use cases where you need long context and the ability to chat on top of function calling. Most recently, Mistral v0.3 has tool calling available in it, but in our tests, it doesn't meet expectations.

We also knew with our experience with [gptscript](https://github.com/gptscript-ai/gptscript), autogen, and other agent frameworks, that you may want a smaller or larger model depending on the use case. We didn't want to be pinned to one model, so we decided to further post-train all the ones we liked.

---

A couple of side notes:
- The Rubra Qwen2 model is capable of function calling in Chinese! It has limited function calling capability in the 28 other languages that Qwen2 supports.
- The [GGUF models](https://huggingface.co/collections/rubra-ai/rubra-v01-gguf-667f52cef892a8cb95bac7c8) have received ~100k downloads in the last 48 hours!
- We have already started to train a new Rubra Phi3 based on the June 2024 Phi-3-mini update that came out today. Stay tuned!",MachineLearning,32,0,1719973092.0,1du3b1e,sanjay920,https://www.reddit.com/r/MachineLearning/comments/1du3b1e/p_new_collection_of_llama_mistral_phi_qwen_and/,Project
[D] Current research in learning during inference?,"I'm curious about the latest research on models that can learn during inference, particularly autoregressive models. What are some of the key papers or approaches in this area? I'm especially interested in:

* Methods for updating weights during inference
* Applications to language models, time series forecasting, etc.

Any pointers to recent work or thoughts on promising directions would be greatly appreciated. Thanks!",MachineLearning,31,16,1719949352.0,1dturka,uoftsuxalot,https://www.reddit.com/r/MachineLearning/comments/1dturka/d_current_research_in_learning_during_inference/,Discussion
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,MachineLearning,33,48,1715794991.0,1csqur3,OraclePred,https://www.reddit.com/r/MachineLearning/comments/1csqur3/d_acl_2024_decisions/,Discussion
[D] ECCV-2024 reviews are out ,Title says it all.,MachineLearning,33,46,1715288460.0,1co7w0i,darkknight-6,https://www.reddit.com/r/MachineLearning/comments/1co7w0i/d_eccv2024_reviews_are_out/,Discussion
[Discussion] Seeking help to find the better GPU setup. Three H100 vs Five A100?,"Long story short, a company has a budget for buying GPUs expected to fine-tune LLMs(probably 70B ones), and I have to the research to find which GPU setup is the best with respect to their budget.

The budget can buy **three H100** GPUs or **five A100** GPUs. 

I tried my best but until now is not clear to me which of these setups is better. While five A100s have more VRAM, they say H100 are 2-8 times faster than A100s!

I'm seeking help. Any valuable insights will be appreciated.",MachineLearning,30,42,1714679391.0,1cioyn8,nlpbaz,https://www.reddit.com/r/MachineLearning/comments/1cioyn8/discussion_seeking_help_to_find_the_better_gpu/,Discussion
[R]Large language models may not be able to sample behavioral probability distributions,"Through our experiments, we found that LLM agents have a certain ability to understand probability distributions, the LLM agent's sampling ability for probability distributions is lacking and it is difficult to give a behavior sequence that conforms to a certain probability distribution through LLMs alone. 

We are looking forward to your thoughts, critiques, and discussions on this topic. Full Paper & Citation: You can access the full paper [https://arxiv.org/abs/2404.09043](https://arxiv.org/abs/2404.09043). Please cite our work if it contributes to your research.

https://preview.redd.it/ai7uks7nluwc1.png?width=935&format=png&auto=webp&s=891dd57ef50d1ee99b1a8b2372b9a460397754d6",MachineLearning,31,15,1714147898.0,1cdpdci,GYX-001,https://www.reddit.com/r/MachineLearning/comments/1cdpdci/rlarge_language_models_may_not_be_able_to_sample/,Research
[D] Advice on achieving >=80% accuracy on Imagnet in under 100 epochs on a single H100 GPU,"Hi everyone!

I am currently trying to train an EfficientNetV2-medium model on ImageNet1K from scratch on a single H100 GPU. Since the GPU is shared among my lab-mates, the per-epoch training time is exasperatingly slow. Thus, I have limited the number of training epochs to 100. Given these limitations, I want your advice on how I can possibly achieve a top1 score greater than 80%?

Based on my current configuration, I am able to achieve 78%. The following are my training details:

Batch Size: 256 Learning Rate: 0.1 Optimizer: SGD with Momentum of 0.9 Weight decay: 2x10^(-5) Learning Rate Scheduler: Cosine Augmentations: TrivialAugmentWide, RandomCrop(8) Mixed Precision Training (bf16) Deep Learning Library: Pytorch Lightning

I have also applied dynamic resizing wherein my train and test images start with a size of 128x128 and every 20 epochs, increase by 24 pixels until the 80th epoch, where the image size gets fixed to 224x224.

I would really appreciate if you could provide any insights on how I could increase my score and if it's possible to further reduce the training time.

Thank you!



========

**EDIT:**

**========**

Thanks to all of your suggestions I was finally able to cross the 80% mark in 10 epochs, specifically, I achieved a score of 84.3%!

As one of the Redditors correctly pointed out, I ended up spending way too much time in trying to figure out the right model and the right set of hyperparameters. I tried out ResNet50, RegNet, EfficientNet and SwinTransformerV2 with AdamW and SGD with different learning rates but none achieved a score greater than 79% under 100 epochs. So I finally went with transfer learning (fine-tuning wasn't really an option). I used the DinoV2 model (dinov2\_vitb14) and trained a single hidden layer MLP on top of it with the AdamW optimizer.

  
Here is the training recipe in case anyone wants to do the same:

    torch.set_float32_matmul_precision(""medium"")
    Optimizer: AdamW(lr=0.001)
    Batch Size: 256
    Number of epochs: 10
    Label Smoothing: 0.1
    Augmentations: TrivialAugmentWide + Resize(224) + CenterCrop(224)
    Scheduler: Cosine Annealing
    Architecture: dinov2_vitb14 -> concatenate last layer CLS, register tokens and patch tokens -> 512 (linear layer) -> 1000 linear layer
    
    Final Validation Accuracy (Top-1): 84.30
    Final Validation Accuracy (Top-5): 97.00



Thank you so much, everyone!! This would not have been possible without your advice!

Please feel free to add further suggestions or ask your doubts.",MachineLearning,31,47,1735502755.0,1hp4hph,atif_hassan,https://www.reddit.com/r/MachineLearning/comments/1hp4hph/d_advice_on_achieving_80_accuracy_on_imagnet_in/,Discussion
[P] Vision Parse: Parse PDF documents into Markdown formatted content using Vision LLMs,"Hey Redditors,

I'm excited to share Vision Parse - [https://github.com/iamarunbrahma/vision-parse](https://github.com/iamarunbrahma/vision-parse), an open-source Python library that uses Vision Language Models to convert PDF documents into perfectly formatted markdown content automatically.

* Converts each page in a PDF document into high-resolution images
* Detects texts, tables, links, and images from the high-resolution image using Vision LLMs and parses them in markdown format
* Handles multi-page PDF documents effortlessly
* And it's easy to get started with this library (just `pip install vision-parse`, and then a few lines of code to convert a document into markdown formatted content).

**Why I built this?**

* Traditional PDF to markdown conversion tools often struggle with complex layouts, semi-structured and unstructured tables and formatting. Hence, relying on Vision LLMs to extract content in markdown from images (Here, I am converting each PDF page into an image).
* Document structure would get distorted with traditional OCR's and PDF to markdown conversion tools. Hence, using Generative AI models would help us in getting better understanding of the structure and preserve it.

You can find documentation to get started with this library here: [https://github.com/iamarunbrahma/vision-parse/blob/main/README.md](https://github.com/iamarunbrahma/vision-parse/blob/main/README.md)

View this [GitHub Project - Vision Parse](https://github.com/iamarunbrahma/vision-parse) and please provide me your feedback or any suggestions.",MachineLearning,30,18,1734420037.0,1hg5d3p,heliosarun,https://www.reddit.com/r/MachineLearning/comments/1hg5d3p/p_vision_parse_parse_pdf_documents_into_markdown/,Project
[D] Monthly Who's Hiring and Who wants to be Hired?,"**For Job Postings** please use this template

>Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]

**For Those looking for jobs** please use this template

>Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]

&#x200B;

Please remember that this community is geared towards those with experience.",MachineLearning,33,4,1733023815.0,1h3u444,AutoModerator,https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/,Discussion
[D] How do you manage to retain information and ideas from the research papers that you read way back earlier?,"I'm working on the NLP and graph learning field for the past 8 months and I've read quite a good amount of papers but I feel like I don't retain lot of the information from the earlier papers unless I explicitly integrate it in my work. How do you guys manage to retain information?

Also, as this field is progressing rapidly, how do you keep track of the papers coming out all the time. It seems tiring enough already.",MachineLearning,31,33,1730985157.0,1glq3yd,Remote_Status_1612,https://www.reddit.com/r/MachineLearning/comments/1glq3yd/d_how_do_you_manage_to_retain_information_and/,Discussion
How to run science projects,"I’ve put together my experience for running ML & science projects based on 9+ years at a FAANG company. It covers the usual stuff like figuring out vague business problems, finding the right stakeholders, setting up metrics, and getting things done. I also share some personal stories about what’s worked (and what hasn’t), especially when stakeholders aren't on the same page. If you’ve done similar work or have different approaches, I’d love to hear what you think!

[https://dzidas.com/ml/2024/10/22/implementing-data-science-projects/](https://dzidas.com/ml/2024/10/22/implementing-data-science-projects/)",MachineLearning,29,7,1729609384.0,1g9kakg,kafka399,https://www.reddit.com/r/MachineLearning/comments/1g9kakg/how_to_run_science_projects/,None
"[P] NHiTs: Deep Learning + Signal Processing for Time-Series Forecasting
","NHITs is a SOTA DL for time-series forecasting because:

* Accepts past observations, future known inputs, and static exogenous variables.
* Uses multi-rate signal sampling strategy to capture complex frequency patterns — essential for areas like financial forecasting.
* Point and probabilistic forecasting.

You can find a detailed analysis of the model [here](https://aihorizonforecast.substack.com/p/forecasting-with-nhits-uniting-deep): ",MachineLearning,31,10,1729351189.0,1g7b6po,apaxapax,https://www.reddit.com/r/MachineLearning/comments/1g7b6po/p_nhits_deep_learning_signal_processing_for/,Project
[R] A Unified Benchmark for Federated Unsupervised Anomaly Detection in Tabular Data,"Paper: [https://arxiv.org/abs/2408.04442](https://arxiv.org/abs/2408.04442)  
  
We are happy to announce that our work got accepted to the IEEE International Conference on Federated Learning Technologies and Applications! In this work, we introduced **FedAD-Bench**, a benchmark designed to evaluate unsupervised anomaly detection algorithms within federated learning (FL) environments, focusing on tabular data. In this paper, we aimed to standardize the evaluation of anomaly detection in FL, ensuring reproducibility and fair comparisons across studies. And we invite you to further participate in this benchmark! :)

Takeaway for non-FL researchers: We found that FL can sometimes outperform centralized models due to its regularization effects, which help mitigate overfitting.",MachineLearning,32,2,1725230743.0,1f6rfdy,Maleficent_Stay_7737,https://www.reddit.com/r/MachineLearning/comments/1f6rfdy/r_a_unified_benchmark_for_federated_unsupervised/,Research
[P] Fish Speech TTS: clone OpenAI TTS in 30 minutes,"While we are still figuring out ways to improve the agent's emotional response to OpenAI GPT-4o, we have already made significant progress in aligning OpenAI's TTS performance. To begin this experiment, we collected 10 hours of OpenAI TTS data to perform supervised fine-tuning (SFT) on both the LLM (medium) and VITS models, which took approximately 30 minutes. After that, we used 15 seconds of audio as a prompt during inference.

Demos Available: [here](https://firefly-ai.notion.site/OpenAI-Examples-34975ae263a9496c84e89fb7b1ea25a4?pvs=4).

As you can see, the model's emotion, rhythm, accent, and timbre match the OpenAI speakers, though there is some degradation in audio quality, which we are working on. To avoid any legal issues, we are unable to release the fine-tuned model, but I believe everyone can tune [fish-speech](https://github.com/fishaudio/fish-speech) to this level within hours and for around $20.

Our experiment shows that with only 25 seconds of prompts (few-shot learning), without any fine-tuning, the model can mimic most behaviors except details like timbre and how it reads numbers. To the best of our knowledge, you can clone how someone speaks in English, Chinese, and Japanese with 30 minutes of data using this framework.

  
Repo: [https://github.com/fishaudio/fish-speech](https://github.com/fishaudio/fish-speech)",MachineLearning,32,4,1716372712.0,1cxwqb7,lengyue233,https://www.reddit.com/r/MachineLearning/comments/1cxwqb7/p_fish_speech_tts_clone_openai_tts_in_30_minutes/,Project
"[D] - Can multimodal models tell images apart from text? Like if a text token and an image token are close vectors, will the model be able to ""tell"" if it is reading or seeing? ","I ran into this doing some work with multimodal models. It seemed like they couldn't tell which part of the information was from the text vs the image portions of an input. 

Is there any research on this? ",MachineLearning,32,13,1716234294.0,1cwoh51,30299578815310,https://www.reddit.com/r/MachineLearning/comments/1cwoh51/d_can_multimodal_models_tell_images_apart_from/,Discussion
[P] N-way-attention,"I have been playing with the concept of attending to more than two tokens in transformer models. Instead of having one query and one key for example, having two keys and one query, and for every query sum over every pair of previous tokens.

It makes the algorithm even slower ( O(n\*\*3) instead of O(n\*\*2)), but I think it is a fun concept. Some results where surprising to me, like how good it is at finding the longest increasing subsequence.

I want it to share it:  
[https://github.com/Gusanidas/n-way-attention/tree/main](https://github.com/Gusanidas/n-way-attention/tree/main)

And to ask if anyone knows of papers that treat the concept, or mention it.",MachineLearning,34,17,1716112586.0,1cvkrpf,Gusanidas,https://www.reddit.com/r/MachineLearning/comments/1cvkrpf/p_nwayattention/,Project
[P] Needle in a Needlestack (NIAN),"**Code**: [https://github.com/llmonpy/needle-in-a-needlestack](https://github.com/llmonpy/needle-in-a-needlestack)

**Website**: [https://nian.llmonpy.ai/](https://nian.llmonpy.ai/)

**Description**:

>Needle in a haystack (NIAH) has been a wildly popular test for evaluating how effectively LLMs can pay attention to the content in their context window. As LLMs have improved NIAH has become too easy. **Needle in a Needlestack** (**NIAN**) is a new, more challenging benchmark. Even GPT-4-turbo struggles with this benchmark.

>NIAN creates a list of limericks from a large database of limericks and asks a question about a specific limerick that has been placed at a test location. Each test will typically use 5 to 10 test limericks placed at 5 to 10 locations in the prompt. Each test is repeated 2-10 times.",MachineLearning,30,6,1715854958.0,1ct9nth,None,https://www.reddit.com/r/MachineLearning/comments/1ct9nth/p_needle_in_a_needlestack_nian/,Project
[R] Fully neuromorphic vision and control for autonomous drone flight,"Arxiv: https://arxiv.org/abs/2303.08778 (15 Mar 2023)  
https://www.science.org/doi/10.1126/scirobotics.adi0591 (15 May 2024)

Also they uploaded a number of videos a few hours ago:

[Supplementary Video 1](https://www.youtube.com/watch?v=NQUv7l56r1o)  
[Supplementary Video 2](https://www.youtube.com/watch?v=0xQU7WMR1Ys)  
[Supplementary Video 3](https://www.youtube.com/watch?v=UfKr1N8mu4c)  
[Supplementary Video 4](https://www.youtube.com/watch?v=hp8Rudld3sI)

Abstract:

> Biological sensing and processing is asynchronous and sparse, leading to low-latency and energy-efficient perception and action. In robotics, neuromorphic hardware for event-based vision and spiking neural networks promises to exhibit similar characteristics. However, robotic implementations have been limited to basic tasks with low-dimensional sensory inputs and motor actions because of the restricted network size in current embedded neuromorphic processors and the difficulties of training spiking neural networks. Here, we present a fully neuromorphic vision-to-control pipeline for controlling a flying drone. Specifically, we trained a spiking neural network that accepts raw event-based camera data and outputs low-level control actions for performing autonomous vision-based flight. The vision part of the network, consisting of five layers and 28,800 neurons, maps incoming raw events to ego-motion estimates and was trained with self-supervised learning on real event data. The control part consists of a single decoding layer and was learned with an evolutionary algorithm in a drone simulator. Robotic experiments show a successful sim-to-real transfer of the fully learned neuromorphic pipeline. The drone could accurately control its ego-motion, allowing for hovering, landing, and maneuvering sideways—even while yawing at the same time. The neuromorphic pipeline runs on board on Intel’s Loihi neuromorphic processor with an execution frequency of 200 hertz, consuming 0.94 watt of idle power and a mere additional 7 to 12 milliwatts when running the network. These results illustrate the potential of neuromorphic sensing and processing for enabling insect-sized intelligent robots.

They have some other cool papers:

[Lightweight Event-based Optical Flow Estimation via Iterative Deblurring](https://arxiv.org/abs/2211.13726) and [Video](https://www.youtube.com/watch?v=1qA1hONS4Sw)",MachineLearning,33,3,1715835750.0,1ct590c,Sirisian,https://www.reddit.com/r/MachineLearning/comments/1ct590c/r_fully_neuromorphic_vision_and_control_for/,Research
[R] OREO: Offline RL for Multi-Step Reasoning in Large Language Models,"This paper introduces **OREO**, a novel offline RL approach that combines policy learning with value assessment to improve LLM multi-step reasoning. The key innovation is using soft Bellman equations alongside preference optimization to better distribute credit across reasoning steps.

Main technical points:
- Implements offline RL with preference learning and value function estimation
- Uses soft Bellman equations to learn optimal behaviors
- Trains both policy and value functions simultaneously
- Integrates with existing DPO (Direct Preference Optimization) methods
- Tested on GSM8K, MATH, and ALFWorld benchmarks

Results:
- Outperformed baseline methods on GSM8K math reasoning tasks
- Showed improved performance on MATH benchmark problems
- Demonstrated better reasoning capabilities in ALFWorld environment
- Achieved more effective credit assignment across reasoning steps
- Reduced computational overhead during inference

I think this work addresses a fundamental challenge in getting LLMs to perform complex reasoning. By better understanding which steps contribute most to successful outcomes, we can train more capable systems for tasks requiring precise logical thinking. The approach could be particularly valuable for applications in automated theorem proving, robotic planning, and other domains requiring structured multi-step reasoning.

I'm particularly interested in how this might scale to more open-ended reasoning tasks where the ""correct"" sequence of steps isn't as clearly defined as in mathematical problems. The computational efficiency during inference is also noteworthy, as it suggests practical deployability.

TLDR: New offline RL method combines policy learning and value assessment to improve LLM reasoning by better understanding which steps matter most for successful outcomes.

[Full summary is here](https://aimodels.fyi/papers/arxiv/offline-reinforcement-learning-llm-multi-step-reasoning). Paper [here](https://arxiv.org/abs/2412.16145).",MachineLearning,30,2,1735056452.0,1hlglku,Successful-Western27,https://www.reddit.com/r/MachineLearning/comments/1hlglku/r_oreo_offline_rl_for_multistep_reasoning_in/,Research
"[D] Is there such a thing as ""integrable programming""?","I come from a pure math background and have been getting up to speed at a new job in scientific AI/ML where I've been working a lot with JAX. JAX is great, love it, but I see a super common pattern where researchers will have a fully differentiable simulation and a couple of neural net architectures or something, but then there will be a bunch of relatively imprecise numerical estimations of integral values. Obviously I'm reading up on numerical methods and doing my best to restructure problems to solve more algebraically, but for my own curiosity, is there an equivalent of ""differentiable"" programming where you're handling ""integrable"" entities instead?

Obviously, this would be a much harder class of problems since you can integrate...well, everything. And that's how you end up solving PDEs on compact supports with weird ugly Holder bounds. But are there computational approaches (or hell, differentiable programming strategies I should be aware of) that move in this direction? Are there nice natural algebraic properties to be leveraged? Can you use a computational graph the same sort of way? How about, like, valid ways to extend to ""weakly-differentiable"" functional programming?

Hope this is relevant enough since it's learning JAX inspired...


edit:
Neat, thanks everyone. Long time lurker, super psyched to get some fruitful answers. I probalby should have specified that I'm not looking for fully generic solutions but most people took that leap anyways. As u/yldedly pointed me to, I think I'm basically asking for generalizations of the Risch algorithm for restricted multivariate function spaces...which [do seem to exist](https://arxiv.org/abs/1305.1481), but it's apparently an open but pretty unpopular area. Also Risch would be an absolute beast to implement...haven't actually looked at Risch-Norman but Maple has had it for a while. Bayesian quadrature and generally using Pyro is probably the most practical solution right now, thanks u/hugosc and u/daking999. I'll be waiting for the right problem to try out u/bregav 's suggestion of just autodiffing the antiderivative drictly (slick).",MachineLearning,32,18,1733694990.0,1h9ty31,redwingviking,https://www.reddit.com/r/MachineLearning/comments/1h9ty31/d_is_there_such_a_thing_as_integrable_programming/,Discussion
[R] BitNet a4.8: 4-bit Activations for 1-bit LLMs,"**Paper:** [https://arxiv.org/pdf/2411.04965](https://arxiv.org/pdf/2411.04965)

**Abstract:**

>Recent research on the 1-bit Large Language Models (LLMs), such as BitNet b1.58, presents a promising direction for reducing the inference cost of LLMs while maintaining their performance. In this work, we introduce BitNet a4.8, enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid quantization and sparsification strategy to mitigate the quantization errors introduced by the outlier channels. Specifically, we utilize 4-bit activations for inputs to the attention and feed-forward network layers, while sparsifying intermediate states followed with 8-bit quantization. Extensive experiments demonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58 with equivalent training costs, while being faster in inference with enabling 4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of parameters and supports 3-bit KV cache, further enhancing the efficiency of large-scale LLM deployment and inference.

**Visual Abstract:**

https://preview.redd.it/gpt38utvqn3e1.png?width=1011&format=png&auto=webp&s=1c9a09638675e7a9f89e3804c1df0229663d136a

**Evaluations:**

[HS=HellaSwag, PQ=PiQA, WGe=WinoGrande](https://preview.redd.it/4ppq57varn3e1.png?width=955&format=png&auto=webp&s=3c4152947edf4542d2a1ffa181bfa52a5369d916)

https://preview.redd.it/7qrw9jtqrn3e1.png?width=1033&format=png&auto=webp&s=ecfdcb655ae939de8f297e37ef111b8ccaa2b1c9

",MachineLearning,28,3,1732806678.0,1h1y0ig,StartledWatermelon,https://www.reddit.com/r/MachineLearning/comments/1h1y0ig/r_bitnet_a48_4bit_activations_for_1bit_llms/,Research
[D] ICASSP 2025 reviews are due today! ,A friendly banter to discuss the icassp reviews! Hoping for the best!,MachineLearning,30,56,1732113339.0,1gvqvgd,always_been_a_toy,https://www.reddit.com/r/MachineLearning/comments/1gvqvgd/d_icassp_2025_reviews_are_due_today/,Discussion
[D] OpenAI's CLIP alternative,"Hi, Are there any new recent SOTA model like CLIP? I want to do similarity search on images, but CLIP's performance is not very good for my project.

I currently use: CLIP-ViT-B-32-laion2B-s34B-b79K

Embeddings which also capture colour would be perfect. Thanks.",MachineLearning,28,14,1732093723.0,1gvlgxm,CaptTechno,https://www.reddit.com/r/MachineLearning/comments/1gvlgxm/d_openais_clip_alternative/,Discussion
"[D] Eric Schmidt says that scaling laws are not yet stopping AI, what do you guys think?","This is the article in question: https://www.windowscentral.com/software-apps/theres-no-evidence-scaling-laws-have-begun-to-stop-former-google-ceo-claims-ai-systems-will-be-100-times-more-powerful

(I am sure there are far better articles on this topic, but I read this one first)",MachineLearning,33,55,1731985310.0,1guncvr,Born_Replacement_687,https://www.reddit.com/r/MachineLearning/comments/1guncvr/d_eric_schmidt_says_that_scaling_laws_are_not_yet/,Discussion
[R] The KAN paper has this interesting way to turn a unsupervised problem to a supervised problem (permitting var of some samples),"In the KAN paper they have an interesting way to infer the mapping between variables by permitting the variables for some of the sample of the data to create positive and negative samples. A form of contrastive learning. They don't site this approach, are there more formulated ways of doing this time of analysis to study the relationship between variables in an unsupervised way. 

Section 4.2 - https://arxiv.org/abs/2404.19756",MachineLearning,29,12,1729742826.0,1gau1kt,Sandy_dude,https://www.reddit.com/r/MachineLearning/comments/1gau1kt/r_the_kan_paper_has_this_interesting_way_to_turn/,Research
[P] Machine Learning Job List for College Students,"Hey everyone, made a job list to help me find a machine learning internship, and now to find a new grad position. I thought it might be useful to others here, so I made it public. And to those that are outside of the US, I didn't forget about you

[https://github.com/speedyapply/2025-AI-College-Jobs](https://github.com/speedyapply/2025-AI-College-Jobs)",MachineLearning,30,3,1729013935.0,1g4dj6h,0xjoemama69420,https://www.reddit.com/r/MachineLearning/comments/1g4dj6h/p_machine_learning_job_list_for_college_students/,Project
[N] Kaido Orav and Byron Knoll's fx2-cmix Wins 7950€ Hutter Prize Award!,"Kaido Orav and Byron Knoll have improved 1.59% on the [Hutter Prize for Lossless Compression of Human Knowledge](http://prize.hutter1.net/) with their ""[fx2-cmix](https://github.com/kaitz/fx2-cmix)"" entry.Because the Hutter Prize restricts contestants to a single *general* purpose processor and it uses [the most *general* loss function](https://www.youtube.com/watch?v=AKMuA_TVz3A), the required algorithmic advances are *generally* applicable regardless of the industry's ""[Hardware Lottery](https://arxiv.org/abs/2009.06489)"" or loss function compromises.  In this respect it provides a unique and low risk incentive for scientific advancement in machine learning.  
  
Some of fx2-cmix's algorithmic advances over the prior Hutter Prize winning algorithm:  
  
**Mixer and Predictor**: Mixers now skip weight updates when errors are below a certain threshold, which enhances processing speed.  
  
***Single Pass*** **Wikipedia Transform**: This update reduces the time and disk usage needed for processing large datasets like Wikipedia by simplifying the transformation process from a previous multi-step approach to a single pass, thereby significantly speeding up preprocessing stages.  
  
**New Stemming and Context Methods**: Utilizing Natural Language Processing techniques like new word types in stemming processes to create more compact and relevant word streams. This not only improves the quality of the training data but also enhances compression, reducing the storage requirements.  
  
**Efficient Article Ordering**: By embedding entire articles in a large vector and using t-SNE to reduce them to a single dimension, the entire corpus can be rapidly reordered to further speed up training.  
  
Detailed descriptions of the advances along with Jupyter notebooks and additional documents are available in the [fx2-cmix README](https://github.com/kaitz/fx2-cmix/blob/main/README.md).  
",MachineLearning,31,1,1728683774.0,1g1l725,jabowery,https://www.reddit.com/r/MachineLearning/comments/1g1l725/n_kaido_orav_and_byron_knolls_fx2cmix_wins_7950/,News
[D] EMNLP 2024 Results / Notifications,"Results seem to be out for some tracks and can be viewed on Openreview. Emails will probably follow tomorrow. 

  
Congratulations in advance and see you all in Miami!",MachineLearning,32,23,1726767908.0,1fkqxhh,EDEN1998,https://www.reddit.com/r/MachineLearning/comments/1fkqxhh/d_emnlp_2024_results_notifications/,Discussion
[D] Interview experience at OpenAI,Anyone with recent interview experience with OpenAI? I found a really helpful thread on their interview process but that’s from 7 years ago. Wondering how the process is and how others experience has been. Would appreciate any insights,MachineLearning,26,11,1726693689.0,1fk3plt,plantparent2021,https://www.reddit.com/r/MachineLearning/comments/1fk3plt/d_interview_experience_at_openai/,Discussion
[N] New Changes to CVPR 2025,,MachineLearning,32,3,1726442402.0,1fhq7rb,stirling_approx,https://cvpr.thecvf.com/Conferences/2025/CVPRChanges,News
[D] Sentiment analysis state of the art,"What’s the current SOTA for sentiment analysis, now that we have LLMs much stronger than previous NLP methods? How do the encoder-only and encoder-decoder models fare against the massive decoder-only LLMs in this task?

I’m also curious about more advanced methods that return higher dimensional results than just the classic positive/neutral/negative answer.",MachineLearning,30,8,1726364654.0,1fh1j9x,RobbinDeBank,https://www.reddit.com/r/MachineLearning/comments/1fh1j9x/d_sentiment_analysis_state_of_the_art/,Discussion
[D] How to prevent SQL injection in LLM based Text to SQL project ?,"I am working in Data analysis project and it is build for executive level. With the growth on chat GPT based interaction, they want similar functionality but for financial data. For eg. They can ask ""What is the most profitable bank in this quarter?"" and they need the bank list and further some visualization. I was planning to train the LLM with the MySQL db structure , question and relevant query and the progress is well. But I think this method is prone to sql injection attacks. For eg, ""Remove everything from Profit table. "" prompt might generate SQL query to delete the table or truncate the table. I know, we can limit execution of some command which contain delete, truncate, but still I see various problems. Is there any solution ?",MachineLearning,30,25,1726145952.0,1ff1y95,More_Lawfulness_6862,https://www.reddit.com/r/MachineLearning/comments/1ff1y95/d_how_to_prevent_sql_injection_in_llm_based_text/,Discussion
[D] potential dual submissions? 2 similar ICLR 24' papers from Google Research,"I was trying to catch up with the latest generative modeling papers and I spotted these two from ICLR 2024:

Paper 1: FINITE SCALAR QUANTIZATION: VQ-VAE MADE SIMPLE

* first submitted to arXiv on 27 Sep 2023
* Author list: Fabian Mentzer, **David Minnen**, Eirikur Agustsson, Michael Tschannen

>Abstract: We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values, leading to an (implicit) codebook given by the product of these sets…. we employ FSQ with MaskGIT for image generation …

Paper 2:

LANGUAGE MODEL BEATS DIFFUSION — TOKENIZER IS KEY TO VISUAL GENERATION

* First submitted to arXiv on 9 Oct 2023
* Author list: Lijun Yu, José Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk Sohn, **David Minnen**, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A. Ross, Lu Jiang

>Abstract: … In this paper, we introduce MAGVIT-v2,....

Its Abstract doesn’t mention what kind of quantizer it is. However, in the method, it places **Lookup-Free Quantization (LFQ)** in the first subsection. And ICLR reviewers pointed out LFQ as the core contribution.

The 2 papers have 1 author in common and they don’t cite each other. Both papers were submitted and accepted to ICLR 2024.

# Comparison

These two papers both proposed new quantization methods to replace the VQ-VAE method. 

First let’s recap what is VQ-VAE:

1. Given an input image, a spatial feature map is extracted: F \\in R\^{HxWxD}
2. The spatial feature map is viewed as a set of HW feature vectors, whose dimension is D
3. Each feature vector is quantized individually
4. There is a learnable codebook in the quantization process: C \\in R\^{NxD}
5. Each feature vector is replaced with its nearest neighbor in C
6. The codebook is jointly learned with the rest of the network (encoder, decoder)

In Paper 1:

https://preview.redd.it/n5orhlgm9fmd1.png?width=1400&format=png&auto=webp&s=7849b786a663180c2775ae511b23111c3f359454

The paper says:

“Each (scalar) entry in the representation z is independently quantized to the nearest integer by rounding.” 

https://preview.redd.it/ffy043gl9fmd1.png?width=1466&format=png&auto=webp&s=8e9ccf40c4b2a3632bca7c1d53c0f528333a41e9

What’s different from VQ-VAE is that, it treats the spatial feature maps as a set of HWD feature scalars, instead of HW vectors of dimension D. Each scalar is **independently** quantized to one value from  -floor(L/2) to floor(L/2). The codebook is not learnable - it’s a set of  integers predefined by hyperparameter L.

The code index is then a d-digit number in the L-base system.

In Paper 2:

https://preview.redd.it/agn4bc7k9fmd1.png?width=1382&format=png&auto=webp&s=4d74023f7a83ca0c1d819474f2b3c45bb4281b3f

It is also **independently** quantizing each dimension of a feature vector, whose dimension is denoted as log\_2 (K). They proposed to use a very small set of integer scalar codebook: {-1, 1}. Similarly, the code index is a log\_2 (K)-digit number in the 2-base system.

So in conclusion:

1. For both papers, the quantization method is the key contribution. Neither cites the other paper so I assume both claim it as its own.
2. Both methods are performing quantization for each dimension of feature vectors independently.
3. In both methods, the codebook is not learnable, and is set to fixed integers predefined by hyperparameters. 
   1. In paper 1, there is a hyperparameter integer L, and the codebook is a set of integers: from  -floor(L/2) to floor(L/2).
   2. In paper 2, the code book is {-1, 1}, which can be roughly seen as L=2
4. As for differences, they apply this quantization method to different tasks. Paper 2 did some experiments in video. However, the quantization method is the same.

The two papers feature the same quantization technique albeit a difference in a hyperparameter. The 2 papers have 1 common author so they should have known each other’s work. IMO, paper 2 seems like a more developed version of paper 1 (and achieved better empirical results). So the main difference seems to be experiments. 

If the 2 papers had been released by different authors, it could be coincidence — it’s common that people have similar ideas but some excel in execution. However, it’s not the case here.

What could be the story behind these? If the two groups of people in Google Research developed similar techniques independently and they both wanted to submit to ICLR, why would they have 1 common author? Did they do this to increase number of papers?

Do folks think these two papers are substantially similar enough to violate the dual submission rule? For some conferences like CVPR, 20% similar is considered substantial.",MachineLearning,29,3,1725294259.0,1f7axjm,One-Tax-2998,https://www.reddit.com/r/MachineLearning/comments/1f7axjm/d_potential_dual_submissions_2_similar_iclr_24/,Discussion
[D] Results for Google PhD Fellowship 2024,Has anyone heard anything from Google about results of the PhD Fellowship program? I thought they are going to notify people last July. ,MachineLearning,30,137,1725026572.0,1f4w1tr,EDEN1998,https://www.reddit.com/r/MachineLearning/comments/1f4w1tr/d_results_for_google_phd_fellowship_2024/,Discussion
[R] Wavelet Convolution for Large Receptive Fields,"**TL;DR**: We use wavelets to increase convolution's receptive field and low-frequency response.

Paper: [https://arxiv.org/abs/2407.05848](https://arxiv.org/abs/2407.05848)  
Code: [https://github.com/BGU-CS-VIL/WTConv](https://github.com/BGU-CS-VIL/WTConv)

>In recent years, there have been attempts to increase the kernel size of Convolutional Neural Nets (CNNs) to mimic the global receptive field of Vision Transformers' (ViTs) self-attention blocks. That approach, however, quickly hit an upper bound and saturated way before achieving a global receptive field. In this work, we demonstrate that by leveraging the Wavelet Transform (WT), it is, in fact, possible to obtain very large receptive fields without suffering from over-parameterization, e.g., for a *k*×*k* receptive field, the number of trainable parameters in the proposed method grows only logarithmically with *k*. The proposed layer, named WTConv, can be used as a drop-in replacement in existing architectures, results in an effective multi-frequency response, and scales gracefully with the size of the receptive field. We demonstrate the effectiveness of the WTConv layer within ConvNeXt and MobileNetV2 architectures for image classification, as well as backbones for downstream tasks, and show it yields additional properties such as robustness to image corruption and an increased response to shapes over textures.

https://preview.redd.it/nrvpb9cvz7cd1.jpg?width=1246&format=pjpg&auto=webp&s=5f9a02fa6c32d9c121d2519fe137b289756b9701",MachineLearning,30,8,1720847760.0,1e22i7j,shahaff32,https://www.reddit.com/r/MachineLearning/comments/1e22i7j/r_wavelet_convolution_for_large_receptive_fields/,Research
[R] Large language models are much more linear than everyone thought,"Authors have revealed a novel linear characteristic exclusive to transformer decoders, including models such as GPT, LLaMA, OPT, BLOOM, and others. They analyzed embedding transformations between sequential layers, uncovering a near-perfect linear relationship (Procrustes similarity score of 0.99). However, linearity decreases when the residual component is removed due to a consistently low output norm of the transformer layer.

https://preview.redd.it/t5kq9g598v9d1.png?width=2420&format=png&auto=webp&s=20da53fce41f75242244d78eea590c6fa52b88c9

The experiments showed that removing or linearly approximating some of the most linear blocks of transformers does not significantly affect the loss or model performance. Moreover, in the pretraining experiments on smaller models, authors introduced a cosine-similarity-based regularization, aimed at reducing layer linearity. This regularization improves performance metrics on benchmarks like Tiny Stories and SuperGLUE and successfully decreases the models' linearity. The study challenges the existing understanding of transformer architectures, suggesting that their operation may be more linear than previously assumed, as well as to eliminate 10-15% of layers without losing quality.

The research has been accepted at the ACL 2024 conference, additional details are provided in the [preprint](https://arxiv.org/abs/2405.12250).

",MachineLearning,31,10,1719823248.0,1dso3pg,AIRI_Institute,https://www.reddit.com/r/MachineLearning/comments/1dso3pg/r_large_language_models_are_much_more_linear_than/,Research
[R] Testing LoRA initialisations,"Hi all, over the past few days, I've been testing out few different initalization methods for LoRA. As you know by default, we initialise ΔW = AB as A\~kaiming\_uniform and B as zeros. But I wanted to try out other initialisation strategies that lead to ΔW = 0 but possibly using minimal zero paramters. 

Here are the approaces I tried:

* Reversing initialisations: Initialise A to zero and B to kaiming uniform
* Purely orthogonal initialisations: Create two non zero matrices that are orthogonal to each other. For this I had two strategies. 
   * Take a random set of orthogonal vectors (by performing orthogonal decomposition of random matrix), split the up into 2 sets. 
   * Split up rows of identity matrix into two sets. (say even rows in set 1 and odd rows in set2 for eg)
   * Init A with linear combinations of elements in first set and B with linear combinations of elements in set 2

I trained the same on different models like llama-2-7B, llama-3-8B, mistral-7B-v0.3 and llama-2-13B. The datasets I used are MetaMathQA and MagicCoder-evol. What I found is that orthogonal initialisation performs better than the standard initialisation. I was just comparing the eval losses of each of the runs. 

[Eval losses on different initialisation strategies.](https://preview.redd.it/81m0rp8o265d1.png?width=2142&format=png&auto=webp&s=b0b6a7efa5c9bff7105f14c844ee5feedb35e8d0&height=1448)

So this felt quite interesting to me. It was sort of along my expected lines that initialising with lesser number of zeros should be good. 

One other thing I noticed was, gradients of lora\_B were consistently more spread out than that of lora\_A. I initially thought it is due to initialisation, those that are init to zero are updated with bigger numbers. But the same holds true for different initialisations. Which is quite surprising. Maybe it is the order of operations that is leading to this? IDK...

I detailed everything in the blogpost  [https://datta0.github.io/blogs/know-your-lora/](https://datta0.github.io/blogs/know-your-lora/)   
Feel free to read and let me know if you have any thouhgts/comments.

Cheers.",MachineLearning,31,11,1717773774.0,1dadfcv,im_datta0,https://www.reddit.com/r/MachineLearning/comments/1dadfcv/r_testing_lora_initialisations/,Research
[R] Grounding DINO 1.5 Release: the most capable open-set detection model,"We introduce Grounding DINO 1.5, which is our most powerful open-world object detection model series. Building on the solid foundation of its predecessor, Grounding DINO, this enhanced model increases both the model size and its training dataset, enhancing its ability to understand and detect visual objects more accurately.

Github Link: [https://github.com/IDEA-Research/Grounding-DINO-1.5-API](https://github.com/IDEA-Research/Grounding-DINO-1.5-API)

Online Demo: [https://deepdataspace.com/home](https://deepdataspace.com/home)

Huggingface Demo: [https://huggingface.co/spaces/Mountchicken/Grounding-DINO-1.5](https://huggingface.co/spaces/Mountchicken/Grounding-DINO-1.5)

arXiv Link: [https://arxiv.org/abs/2405.10300](https://arxiv.org/abs/2405.10300)

# Zero-Shot Performance Compare with Grounding DINO

https://preview.redd.it/7sc0fq58k71d1.png?width=1149&format=png&auto=webp&s=3312392b5d6b820fa446f8cc2600c52ce33a6079

detailed performance:

https://preview.redd.it/kcpd3awek71d1.png?width=1280&format=png&auto=webp&s=498acee90fa95c73a87f8e9ee92a0f27430b2c51

# Fine-tuning Performance on Downstream Tasks

compare with Grounding DINO

https://preview.redd.it/d0spf0jjk71d1.png?width=1020&format=png&auto=webp&s=46073d82f6d9468852a5595b93df99b0148208f2

detailed performance

https://preview.redd.it/5m85gsylk71d1.png?width=1756&format=png&auto=webp&s=554d0b72621d79482c1ba11ba2821b51d1c65459

# Visualizations

https://preview.redd.it/lob31unpk71d1.png?width=1052&format=png&auto=webp&s=02fbffb5f23d6d8cb6b3d5cbe4e5a2146365d977

https://preview.redd.it/f1oufwnpk71d1.png?width=1039&format=png&auto=webp&s=42b906c6517181612583f89d0e69b73f3f9738d8

https://preview.redd.it/xxp98kopk71d1.png?width=1066&format=png&auto=webp&s=8dd7e47a1da43a62a41c2b78489b54816e208d7d

https://preview.redd.it/0l6te2opk71d1.png?width=1011&format=png&auto=webp&s=81c5a777a9728449b452c1cef4017fcc2e6f13e3

https://preview.redd.it/4fwp33spk71d1.png?width=2365&format=png&auto=webp&s=009c5f063fb2f0688e334d0f815e528926ce9430

https://preview.redd.it/nsgmwdspk71d1.png?width=2365&format=png&auto=webp&s=3e7125d6fa8799ad75e1db869538781c59689daf

https://preview.redd.it/o01ycarpk71d1.png?width=2367&format=png&auto=webp&s=fdffd0b2ddbff78b1b46286256e0178993304e0a

https://preview.redd.it/absfvhopk71d1.png?width=1105&format=png&auto=webp&s=d760d0082b7c271837b844c70973068db00a4e7b

https://preview.redd.it/8apy7ropk71d1.png?width=1118&format=png&auto=webp&s=8686af325641535bcd4baf12152918f5b7e60bcc

https://preview.redd.it/p65qh6qpk71d1.png?width=1106&format=png&auto=webp&s=2d9a62edf6dd6f086cd0396eca7df0c43356b6f0

https://preview.redd.it/mavta4qpk71d1.png?width=1038&format=png&auto=webp&s=d031236a761720de80e4d5e6468a41dc9f988f6a

https://preview.redd.it/th55xqppk71d1.png?width=1087&format=png&auto=webp&s=0266ebb44a9c24b5e611ce1fdbb2bcd9abf51725

https://preview.redd.it/0373ueqpk71d1.png?width=999&format=png&auto=webp&s=f1379231677d157a706ffd2a5bdf451ca61517bb



",MachineLearning,27,19,1716048342.0,1cv0m9x,Technical-Vast1314,https://www.reddit.com/r/MachineLearning/comments/1cv0m9x/r_grounding_dino_15_release_the_most_capable/,Research
[R] New Teleoperation Tool with VisionPro,,MachineLearning,31,8,1714369253.0,1cfrgx9,XiaolongWang,https://i.redd.it/t821064xvcxc1.gif,Research
[R] The Well: a Large-Scale Collection of Diverse Physics Simulations for Machine Learning,"**Dataset**: [https://github.com/PolymathicAI/the\_well](https://github.com/PolymathicAI/the_well)

**Paper**: [https://arxiv.org/pdf/2412.00568](https://arxiv.org/pdf/2412.00568)

**Abstract:**

>Machine learning based surrogate models offer researchers powerful tools for accelerating simulation-based workflows. However, as standard datasets in this space often cover small classes of physical behavior, it can be difficult to evaluate the efficacy of new approaches. To address this gap, we introduce the Well: a large-scale collection of datasets containing numerical simulations of a wide variety of spatiotemporal physical systems. The Well draws from domain experts and numerical software developers to provide 15TB of data across 16 datasets covering diverse domains such as biological systems, fluid dynamics, acoustic scattering, as well as magneto-hydrodynamic simulations of extra-galactic fluids or supernova explosions. These datasets can be used individually or as part of a broader benchmark suite. To facilitate usage of the Well, we provide a unified PyTorch interface for training and evaluating models. We demonstrate the function of this library by introducing example baselines that highlight the new challenges posed by the complex dynamics of the Well. The code and data is available at [this https URL](https://github.com/PolymathicAI/the_well).

",MachineLearning,31,0,1733827787.0,1haz4nw,StartledWatermelon,https://www.reddit.com/r/MachineLearning/comments/1haz4nw/r_the_well_a_largescale_collection_of_diverse/,Research
Causal Discovery Competition Winning Paper Discussion [D],"I’ve recently come across this post: https://thetourney.github.io/adia-report/ which describes the winning method for a casual discovery competition. It’s not really my field but I do have a reasonable understanding of GNNs and Causal Inference. Anyway, from the report I don’t understand precisely what the winning team was doing. Can anyone either link to a full paper or have a good intuitive and potentially step by step explanation of what they are doing?",MachineLearning,29,17,1732749763.0,1h1i0ji,www3cam,https://www.reddit.com/r/MachineLearning/comments/1h1i0ji/causal_discovery_competition_winning_paper/,Discussion
"[D] This is my first blog on medium, and they are about, How Modern Binary Hopfield Networks are just Hamming Distance Auto completers in disguise",,MachineLearning,29,0,1732382530.0,1gy4qpv,StoneSteel_1,https://medium.com/@kanishq.vijay/modern-hopfield-networks-are-just-fancy-hamming-distance-calculators-and-i-can-prove-it-e9f538ee908e,Discussion
[D] Layernorm is confusingly named?,"TIL, that in NLP, you do layer norm by taking the activations across each feature and normalizing across each feature:

Therefore, if you had a batch of two examples:

\[I, am, a, boy\]

\[She, is a girl\]

then you would create

\[H\_I, H\_am, H\_a, H\_boy\] -> \[u\_I, u\_am, u\_a, u\_boy\] and sigmas

\[H\_She, H\_is, H\_a, H\_girl\] -> \[u\_she, u\_is, u\_a, u\_girl\] and sigmas

So you end up with 8 us and sigma. This doesn't sound like you are normalizing the layer's activations, which leads me to say that layernorm is confusingly named. Am I missing something?",MachineLearning,29,9,1729349604.0,1g7am36,Complex-Media-8074,https://www.reddit.com/r/MachineLearning/comments/1g7am36/d_layernorm_is_confusingly_named/,Discussion
[P] A technical guide on how to upgrade your training code from single GPU to multiple GPUs,"Hey All,

We've been writing a technical guide on how to scale training code from single GPU all the way to multiple nodes.

It's centered around training LLMs, and goes over things like DDP, FSDP, diagnosing errors/logging, and way more.

Tried to make the code and explanations as clear and simple as possible, let us know if you find it helpful!

Contributions welcome and feel free to open issues with requests/bugs.

[https://github.com/LambdaLabsML/distributed-training-guide](https://github.com/LambdaLabsML/distributed-training-guide)",MachineLearning,29,5,1729105054.0,1g573wv,lambda-research,https://www.reddit.com/r/MachineLearning/comments/1g573wv/p_a_technical_guide_on_how_to_upgrade_your/,Project
[D] Machine learning for good,"Hello, I've been having a kind of existential crisis while being aty daily corporate job. I think all my knowledge is going to waste, because it's not used to help more people around me and I was wondering about how can we use our skills to make ML more available to small businesses.

Have you ever worked at a small business and apply some ML or at least some degree of data enginering to help them streamline processes?
How can we actually help to improve small businesses using technology?

Do you have some articles or books that talk about something like this, it'd be pretty nice to read them as well as knowing all of your opinions!",MachineLearning,27,17,1728612037.0,1g0z57p,None,https://www.reddit.com/r/MachineLearning/comments/1g0z57p/d_machine_learning_for_good/,Discussion
What do you think of T-FREE to reduce the embedding's vocab size [D],"Hey r/MachineLearning!

I've just published my second blog post analyzing an interesting new paper: [T-FREE: Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings](https://arxiv.org/abs/2406.19223). You can check out my full breakdown [here](https://f14-bertolotti.github.io/posts/06-09-24-tfree/index.html).

# The Encoding Phase

The authors present an interesting method to reduce the vocabulary size of the embedding matrix using a technique similar to locality sensitive hashing. Here's a breakdown of their process:

1. Apply a whitespace tokenizer to split sentences into words: `""hello world !"" -> [""hello"", ""world"", ""!""]`
2. Add special characters to word boundaries: `[""hello"", ""world"", ""!""] -> [""_hello_"", ""_world_"", ""_!_""]`
3. Split words into 3-grams: `[""_hello_"", ""_world_"", ""_!_""] -> [""_he"", ""hel"", ""ell"", ""llo"", ""lo_"", ""_wo"", ""wor"", ""orl"", ""rld"", ""ld_"", ""_!_""]`
4. Hash each 3-gram into multiple embedding matrix indices: `_hel -> [hash1(""_he"") % v, hash2(""_he"") % v, hash3(""_he"") % v]` (where `v` is the chosen vocabulary size)
5. Create word embeddings by summing all trigram embeddings within each word.

I've created a visual representation of this process: 

https://preview.redd.it/hmvh7lwy42od1.png?width=765&format=png&auto=webp&s=f14b4105c048b5ef019a3de06478aa5bb1beeb14

They also propose a decoding phase but it is a bit more convoluted. If you are interested you can check it on my \[post\](https://f14-bertolotti.github.io/posts/06-09-24-tfree/index.html) or on their \[paper\](https://arxiv.org/abs/2406.19223).

# Key Takeaways and Considerations

1. The paper presents a compelling idea and is generally well-written, though the decoding section could benefit from more detail.
2. The decoding phase applies two different normalizations (division by sum followed by softmax), which seems unconventional.
3. While marketed as tokenizer-free, the method still employs a whitespace tokenizer. ""Training-free tokenizer"" might be more appropriate.
4. An interesting experiment would be to use a standard decoding phase with the full word-embedding matrix. While computationally intensive, I think it could be an interesting experiment.

# Discussion

What are your thoughts on this approach? Do you see potential limitations?",MachineLearning,29,10,1726007143.0,1fduqr5,f14-bertolotti,https://www.reddit.com/r/MachineLearning/comments/1fduqr5/what_do_you_think_of_tfree_to_reduce_the/,Discussion
"""Writing in the Margins (WiM)"" - a better inference pattern for long context LLMs that solves the Lost-in-the-Middle problem",,MachineLearning,29,15,1724858888.0,1f3d3uo,samjulien,https://arxiv.org/abs/2408.14906,None
[Project]: Python Apps for AI Models: Your Feedback is Welcome!,"Hi,
I have been learning about a few popular AI models and have created a few Python apps related to them. Feel free to try them out, and I’d appreciate any feedback you have!

- [AutoSubs](https://github.com/prashkrans/auto_subs): Web app for embedding customizable subtitles in videos.
- [VideoSummarizer](https://github.com/prashkrans/yt_video_summarizer): Web app that summarizes YouTube videos with custom word limits options.
- [StableDiffusion](https://github.com/prashkrans/sd_1.5_python_app): Python app for text-to-image generation and inpainting using Stable Diffusion 1.5.
- [Image Matting](https://github.com/prashkrans/viTmatte_using_hf): Python app for background removal with enhanced accuracy using ViTMatte with trimap generation.
- [Lama Inpainting](https://github.com/prashkrans/mask_inpaint_upscale): Python app for object removal and inpainting with upscaling to maintain original resolution.
- [YT Video Downloader](https://github.com/prashkrans/yt_video_downloader): Web utility for downloading YouTube videos by URL.",MachineLearning,31,4,1724628584.0,1f1a1el,nashPrat,https://www.reddit.com/r/MachineLearning/comments/1f1a1el/project_python_apps_for_ai_models_your_feedback/,Project
[R] JPEG-LM: LLMs as Image Generators with Canonical Codec Representations,,MachineLearning,28,1,1724033195.0,1evqfwo,hardmaru,https://arxiv.org/abs/2408.08459,Research
"[R] preference learning: RLHF, best of n sampling, or direct preference optimization?","per the title: people with \*practical\* experience with all/some of these methods, which would you prefer and why?

are you aware of variational versions of these models and whether they help mitigate overoptimization?

thanks!",MachineLearning,29,8,1722866038.0,1ekodce,South-Conference-395,https://www.reddit.com/r/MachineLearning/comments/1ekodce/r_preference_learning_rlhf_best_of_n_sampling_or/,Research
[D] Paper that used adversarial noise to perform self-supervised learning and used 20 samples to beat a model trained on 20K samples.,"Sorry if this isn't the right sub. I recall reading a paper a few years back that added noise to an input image and performed metric learning with the original. I recall something about how the model was trained using 20 samples and performed on par with a model trained using 20K samples (or something like that).

Does anybody know the paper or similar papers that I'm referring to?

Thanks in advance.",MachineLearning,29,1,1721750338.0,1eabgnt,Seankala,https://www.reddit.com/r/MachineLearning/comments/1eabgnt/d_paper_that_used_adversarial_noise_to_perform/,Discussion
[P] ML system design: 450 case studies to learn from (Airtable database),"Hey everyone! Wanted to share the link to the database of 450 ML use cases from 100+ companies that detail ML and LLM system design. You can filter by industry or ML use case.

If anyone here approaches the task of designing an ML system, I hope you'll find it useful! 

Link to the database: [https://www.evidentlyai.com/ml-system-design](https://www.evidentlyai.com/ml-system-design) 

Disclaimer: I'm on the team behind [Evidently](https://github.com/evidentlyai/evidently), an open-source ML and LLM observability framework. We put together this database.",MachineLearning,28,2,1721322077.0,1e6gdsk,dmalyugina,https://www.reddit.com/r/MachineLearning/comments/1e6gdsk/p_ml_system_design_450_case_studies_to_learn_from/,Project
[R] Stanford Drone Dataset,I’m a PhD researcher working on object detection from aerial imagery. I have used the Stanford Drone Dataset (SSD) in the past but it appears as though the link on the official website (https://cvgl.stanford.edu/projects/uav_data/) is no longer working (and hasn’t for at least several weeks). This dataset is very valuable for nadir person detection. I was able to find compressed videos on kaggle but some of the videos seem to possibly be corrupted (annotations don’t align properly).  I have tried to email the person who maintains the official dataset but they are now the CEO of a startup and I couldn’t get through to him via their university email. He’s the guy that came up with the generalized IOU metric! Does anyone happen to have this dataset that they would be willing to share via Google drive or some other means? Or does anyone know of any alternative download for this? I would be so incredibly thankful if you did. Thank you for any info! ,MachineLearning,29,11,1717633073.0,1d954ji,LyveLyte,https://www.reddit.com/r/MachineLearning/comments/1d954ji/r_stanford_drone_dataset/,Research
[R] What is the state-of-art of model parallelism ? ,Is it easy to implement model parallelism with  common frameworks like PyTorch and Tensorflow? It depends on the model architecture? What are the most used approaches on model parallelism ?,MachineLearning,28,14,1716166275.0,1cw35by,Various_Protection71,https://www.reddit.com/r/MachineLearning/comments/1cw35by/r_what_is_the_stateofart_of_model_parallelism/,Research
[Research] A visual deep dive into Uber's machine learning solution for predicting ETAs.,"TL;DR: Uber follows a 2-layer approach. They combine traditional graph algorithms like Dijkstra with learned embeddings and a lightweight self-attention neural network to reliably predict estimated time of arrival or ETA.

[How Uber uses ML to ETAs (and solve a billion dollar problem)](https://open.substack.com/pub/codecompass00/p/uber-billion-dollar-problem-predicting-eta?r=rcorn&utm_campaign=post&utm_medium=web)

https://preview.redd.it/2ovttr82i9xc1.png?width=1358&format=png&auto=webp&s=51b12261bf98f529fd0e9b33daf6362b727f4580",MachineLearning,29,0,1714328311.0,1cfd15u,ml_a_day,https://www.reddit.com/r/MachineLearning/comments/1cfd15u/research_a_visual_deep_dive_into_ubers_machine/,Research
U-Net Vs Attention U-Net [D],"Hello folks,

Young researcher here, working on a in-house dataset to build a foundational model for a interesting use-case. But I have thesis to finsh, which will be just the tail of my current research.

For my thesis, we have decided to have a subsection for comparing how my segmentation results differ when used attention blocks are used within a U-Net. I've referred few papers on how this works and how can this be implemented.

Results are promising (att unet outperformimg unets, nothing suprising) but I see a concerning opposing point i.e. attention Unet having more number of parameters that the unet. Is there a way I can conduct this study where I compare results with and without attention? And there are no other additional factors influencing the results (layers, params, etc).

Does conducting ablation study makes sense in this case? I've not seen any other paper comparing similar use-case using this study.

Any papers I can look through, suggestions and tips are welcome.",MachineLearning,28,10,1733414627.0,1h7cjnd,ade17_in,https://www.reddit.com/r/MachineLearning/comments/1h7cjnd/unet_vs_attention_unet_d/,Discussion
[D] Anthropic AI fellow/residents- any new grads/entry-level people accepted?,"Hello. Are entry-level or new grads accepted into the Anthropic fellowship or resident programs? Past people who were accepted, what was your CV and experience like?",MachineLearning,25,13,1733088359.0,1h4e0ah,geekgeek2019,https://www.reddit.com/r/MachineLearning/comments/1h4e0ah/d_anthropic_ai_fellowresidents_any_new/,Discussion
[D] COLING 2025 Results are leaked,"Yall may login to softconf to check if you can submit the camera-ready paper or not.

Mine was 4/3/3 and luckily got accepted. My first paper!!!",MachineLearning,30,21,1731778540.0,1gssewj,Ambitious-Public-512,https://www.reddit.com/r/MachineLearning/comments/1gssewj/d_coling_2025_results_are_leaked/,Discussion
[D] Monthly Who's Hiring and Who wants to be Hired?,"**For Job Postings** please use this template

>Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]

**For Those looking for jobs** please use this template

>Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]

&#x200B;

Please remember that this community is geared towards those with experience.",MachineLearning,29,27,1727749817.0,1ftdkmb,AutoModerator,https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/,Discussion
"[R] Join Our Global Paper Reading Group for a Deep Dive into ""Plan Like a Graph (PLaG)"" - Enhancing LLMs in Asynchronous Plan Reasoning | ICML 2024 with the author Fangru Lin!","Hey everyone,

We’re excited to invite you to an upcoming session hosted by our global paper reading group**, Computer Vision Talks,** where we invite authors to discuss their research! This time, we’ll be exploring a fascinating topic that’s gaining traction in the AI community: **""Plan Like a Graph (PLaG): Enhancing LLMs in Asynchronous Plan Reasoning""**. 🚀🧠

**Paper Link**:  [https://arxiv.org/pdf/2402.02805](https://arxiv.org/pdf/2402.02805)

**Event Details:**

* **Date:** Saturday, August 24th, 2024
* **Time:** 10:00 AM EST
* **Join:** [Zoom Registration Link](https://uqz.zoom.us/meeting/register/tZMpd-qoqT4rHtTnpCqTO0P5fT0a5b_gHGss#/registration)

**What’s it About?**

The ""Plan Like a Graph (PLaG)"" method introduces an innovative approach to improving large language models (LLMs) by decomposing tasks into sub-tasks that form an execution graph. This method allows for both parallel and sequential task execution, significantly enhancing LLM performance without the need for fine-tuning. Our speaker, **Fangru Lin**, a DPhil NLP student from the University of Oxford, will guide us through how PLaG can be applied to models like GPT-4 and LLaMA-2, achieving state-of-the-art results even as task complexity increases.

**Why Attend?**

This session is perfect for anyone interested in advanced prompt engineering, LLMs, or tackling complex reasoning tasks. Whether you're new to the PLaG method or have been following its development, this talk will provide you with valuable insights and practical applications.

**About Our Speaker:**

**Fangru Lin** is a DPhil NLP student at Oxford and a Clarendon scholar. Her research is already sparking engaging discussions within the AI world, and we’re thrilled to have her share her insights with us.

**Join Us and Support:**

As a global paper reading group, we aim to bring together AI enthusiasts and professionals from around the world. If this topic interests you, please consider joining our talk and engaging with our posts on LinkedIn and YouTube to help spread the word:

* **LinkedIn:** [Computer Vision Talks Post](https://www.linkedin.com/posts/computer-vision-talks_graph-enhanced-large-language-models-in-asynchronous-activity-7231498394203009024-eiB9?utm_source=share&utm_medium=member_desktop)
* **YouTube:** [Computer Vision Talks Channel](https://www.youtube.com/@computervisiontalks4659)

We’d love to have you as part of our discussion and to hear your thoughts!",MachineLearning,28,2,1724186384.0,1ex6dl6,ShambhaviCodes,https://www.reddit.com/r/MachineLearning/comments/1ex6dl6/r_join_our_global_paper_reading_group_for_a_deep/,Research
[D] ECAI 2024 Reviews Discussion,"Discussion thread for ECAI 2024 reviews.

Reviews are out now!

Results are out now. A total of 2,344 submissions were received and 547 of them were accepted, corresponding to an acceptance rate of 23%.",MachineLearning,27,136,1718550628.0,1dh9n4e,Fun_Equal5145,https://www.reddit.com/r/MachineLearning/comments/1dh9n4e/d_ecai_2024_reviews_discussion/,Discussion
[D] Dealing with features having large scale. Eg.  from -1e2 to 1e4 ,"How would you deal with such features where the range is large? One option is log scale, but what if the features are negative valued?

How do you guys deal with such data for your Deep learning models?

Edit: Just to make it clear, the featurs have wide range. eg. -1e2 to +1e4. 

Also, how would you deal if such wide range exists in input features as well as learned features?

Have you been successful in using min max or standardization for cases?",MachineLearning,28,43,1718298524.0,1df4478,rmm_philosopher,https://www.reddit.com/r/MachineLearning/comments/1df4478/d_dealing_with_features_having_large_scale_eg/,Discussion
[R] Lipreading with LipNet: End-to-End Sentence-level Lipreading,"Hey there,

I recently implemented LipNet from scratch based on the paper End-to-End Sentence-level Lipreading. It predicts sentences by extracting features from the lip movement in the input frames. It is originally a 3DConv-GRU model which I've implemented with a 3DConv-LSTM (bi-directional) and a few other models with varying complexity, and have utilized He (Kaiming Normal) initialization for the weights.

I request you take look at the repository and provide any feedback, and consider a fork if you find it useful.

[GitHub/LipNet](https://github.com/mishra-18/lipnet-pytorch)

[Image edited from the official paper](https://preview.redd.it/ur6wsop2qo3d1.png?width=625&format=png&auto=webp&s=8282b11b16c6f7d7b8fa36ca2b3f79231fe1078c)

",MachineLearning,29,0,1717128036.0,1d4mpkw,Kian5658,https://www.reddit.com/r/MachineLearning/comments/1d4mpkw/r_lipreading_with_lipnet_endtoend_sentencelevel/,Research
[D] Phi-3 models compared side-by-side.,"https://preview.redd.it/8l04pnfhq62d1.png?width=661&format=png&auto=webp&s=7fe616ca8cd7da974070c86b6b47ffab3ab545e5

---------------------------------------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------------------------------------

https://preview.redd.it/hr7fr1uiq62d1.png?width=688&format=png&auto=webp&s=bd3de359bfe4c1ed82d092be92ae38c246bdfda2

---------------------------------------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------------------------------------

https://preview.redd.it/v6k3v39kq62d1.png?width=450&format=png&auto=webp&s=c0abb0e397a498ef7ccfb35b1b1cb598198f66ad



For anyone looking to compare the Phi-3 benchmarks in one place.

Interesting comparisons for: ANLI, Hellaswag, MedQA, TriviaQA, Language understanding, Factual Knowledge and Robustness.

Note: Phi-3 mini model table have labels in different order.",MachineLearning,26,18,1716473973.0,1cytxb5,dark_surfer,https://www.reddit.com/r/MachineLearning/comments/1cytxb5/d_phi3_models_compared_sidebyside/,Discussion
"Alice's Adventures in a Differentiable Wonderland -- Volume I, A Tour of the Land",,MachineLearning,28,4,1714530103.0,1chawnq,emiyake,https://arxiv.org/abs/2404.17625,None
"[P] Why does my LSTM always predict the ""Ġ"" char/ U-0120?
",Ġ denotes a space with BPE tokenization so im thinking its just cause there are so many of them. Should I remove all spaces and train my model on that?,MachineLearning,28,8,1735692856.0,1hqs7k9,SnazzySnail9,https://www.reddit.com/r/MachineLearning/comments/1hqs7k9/p_why_does_my_lstm_always_predict_the_ġ_char_u0120/,Project
[R] GLIDER: Grading LLM Interactions and Decisions using Explainable Ranking,,MachineLearning,28,3,1734645035.0,1hi443m,Megixist,https://arxiv.org/abs/2412.14140v1,Research
[R] Optimizing LLM merging to reduce performance tradeoffs,"We just released our new [work](https://huggingface.co/papers/2412.04144) on large-scale merging across checkpoints trained with different hyperparameters, data mixture, objectives, etc. to optimize tradeoffs. For instance, some models that perform well on code generation do worse on instruction following and vice versa. An interesting question to ask whether model merging can lead to a Pareto-optimal model in such setup.

  
**TL;DR:**

AS you're developing an LLM, it is common to obtain different checkpoints, where each excels at a single or a group of tasks/capabilities. Typically, you can keep tuning your hyperparameters until you obtain a Pareto-Optimal model, but the process can be super expensive. We show that you can collect all these models into a pool, and optimize merging parameters to reduce the task tradeoffs. We show that simple linear merging can yield Pareto-Optimal models in different tradeoff scenarios---better tradeoffs than individual models and strong merging baselines. Interestingly, our optimized merges outperform the original models in some cases. Our study uses large-scale models (Command R+ 104B from Cohere) and explores merging up to 16 checkpoints from SFT/RLHF training. 

https://preview.redd.it/3b2her78d57e1.png?width=2269&format=png&auto=webp&s=3974e5b7bbf2f1b5b602eb6aa3a4092978b286d5

  
Happy to hear your thoughts.

  
📄 Paper: [https://huggingface.co/papers/2412.04144](https://huggingface.co/papers/2412.04144)

🧵 Twitter/X: [thread](https://x.com/MKhalifaaaa/status/1866126245014200467?t=2MmryY1FWBz1Ddk_OQ392w&s=19)",MachineLearning,29,6,1734327232.0,1hfc8s5,moyle,https://www.reddit.com/r/MachineLearning/comments/1hfc8s5/r_optimizing_llm_merging_to_reduce_performance/,Research
[D] Cloud GPU Price Analysis - December 2024: A Comprehensive Market Review,"After analyzing current cloud GPU pricing across major providers, I've compiled insights that might help with infrastructure decisions. Some findings surprised me - particularly around hidden costs and spot pricing variations.

Current Market Rates (December 2024)

On-Demand Pricing:

\- RunPod H100 (80GB): $2.49/hr

\- RunPod A100 (80GB): $1.69-1.99/hr

\- [Vast.ai](http://Vast.ai) A100: $0.73-1.61/hr (marketplace model)

\- Lambda A100: $1.29/hr

Key Market Insights

1. Spot Instance Pricing

\- Can reduce costs by 30-70%

\- Availability varies significantly by region

\- Some providers offer spot instance guarantees

\- Price stability varies by provider

2. Hidden Cost Factors

\- Data transfer fees vary dramatically

\- Storage costs for large datasets

\- Network bandwidth tiers

\- Instance startup/shutdown minimums

3. Provider Differentiators

\- UI/UX and ease of use

\- Available regions/zones

\- Support quality

\- API functionality

Cost Optimization Strategies

1. Workload Planning

\- Match GPU to actual requirements

\- Consider splitting workloads across smaller instances

\- Use spot instances for interruptible tasks

\- Monitor utilization patterns

2. Data Management

\- Optimize dataset storage

\- Plan data transfer patterns

\- Use caching effectively

\- Consider compression strategies

I'll be tracking these prices and patterns monthly. Would be interested in:

1. Which providers you're using?
2. How do you optimize costs?
3. What metrics matter most in your GPU decisions?",MachineLearning,27,28,1733237698.0,1h5p7fr,Botinfoai,https://www.reddit.com/r/MachineLearning/comments/1h5p7fr/d_cloud_gpu_price_analysis_december_2024_a/,Discussion
[R] Sources: Reasons why KG outperformes RD in Retrievers?,"Are there any sources discussing WHY Retriever work better with KG in contrast to RD? I find it super intuitive to say its better because in knowledge graphs we have more semantic structure and relations are discovered effeciently. In my mind its ""of course the graph is richer/more dense"" but when collaborated on a paper, it struck me that I wasnt able to justify that claim. I found no source whatsoever that actually explained why that might be the case.

The only source i got was this one:  
[https://arxiv.org/abs/2311.07509](https://arxiv.org/abs/2311.07509)

also here in [](https://www.reddit.com/r/LocalLLaMA/) sub last year:  [https://www.reddit.com/r/LocalLLaMA/comments/17vy1bo/a\_benchmark\_to\_understand\_the\_role\_of\_knowledge/](https://www.reddit.com/r/LocalLLaMA/comments/17vy1bo/a_benchmark_to_understand_the_role_of_knowledge/)

So all we we're able to say was ""We justify our decision because KG works better then RD \[source to benchmark paper\]""

I would have loved to discuss why exactly KG are better suited and give arguments about information density, semantic strutuce or the better selection of related entities. But everything I found were only articles that threw around wild claims or pointed out easier/native implementation, which technically could also be achieved with RD.

Can anyone point me to sources? would love to read an in-depth discussion on the reasons of better performance.",MachineLearning,25,21,1733062637.0,1h447eu,PopPsychological4106,https://www.reddit.com/r/MachineLearning/comments/1h447eu/r_sources_reasons_why_kg_outperformes_rd_in/,Research
[R] Black holes and the loss landscape in machine learning,"Abstract:

>Understanding the loss landscape is an important problem in machine learning. One key feature of the loss function, common to many neural network architectures, is the presence of exponentially many low lying local minima. Physical systems with similar energy landscapes may provide useful insights. In this work, we point out that black holes naturally give rise to such landscapes, owing to the existence of black hole entropy. For definiteness, we consider 1/8 BPS black holes in =8 string theory. These provide an infinite family of potential landscapes arising in the microscopic descriptions of corresponding black holes. The counting of minima amounts to black hole microstate counting. Moreover, the exact numbers of the minima for these landscapes are a priori known from dualities in string theory. Some of the minima are connected by paths of low loss values, resembling mode connectivity. We estimate the number of runs needed to find all the solutions. Initial explorations suggest that Stochastic Gradient Descent can find a significant fraction of the minima.

Arxiv: [https://arxiv.org/abs/2306.14817](https://arxiv.org/abs/2306.14817)",MachineLearning,28,28,1732678009.0,1h0uwjd,Mindless-House-8783,https://www.reddit.com/r/MachineLearning/comments/1h0uwjd/r_black_holes_and_the_loss_landscape_in_machine/,Research
[Discussion] Scaling laws and graph neural networks,"I stumbled upon a paper that introduces the first ""graph foundation model"": [https://arxiv.org/pdf/2407.11907](https://arxiv.org/pdf/2407.11907)

They show that a GNN can scale with data and model size, generalize across different domains, and be efficiently fine-tuned on new datasets. 

This is interesting to me because even though LLMs are all the rage, text can be a weak data representation. Most knowledge has a graph structure. Code, research papers, even the human brain –– all graphs. And next-token prediction as an inductive bias doesn't capitalize on this. 

There's a huge data bottleneck here, of course. But maybe the next step here is using LLMs to convert huge swaths of text on the internet into graphs to train on. 

What do y'all think?",MachineLearning,26,12,1731584095.0,1gr2t6l,jsonathan,https://www.reddit.com/r/MachineLearning/comments/1gr2t6l/discussion_scaling_laws_and_graph_neural_networks/,Discussion
[Discussion] Proof of Reconstruction Loss Term in VQ-VAE Loss,"Hello everyone,

I was reading the paper ""Neural Discrete Representation Learning"" and I was puzzled when I looked at the first term in VQ-VAE Loss Equation

https://preview.redd.it/l1s9kur3sn0e1.png?width=1394&format=png&auto=webp&s=4d374dce319a7ac0bbf19089d4e06cabcaa2cd3d

I understand the role of the second and the third term. However, I am not able to derive the first term from either the MSE between the original and reconstructed image. I assumed it will be similar to the ELBO Loss in the VAE. The paper mentions why they have omitted the KL Divergence Term, but even then I don't understand how the expectation in the ELBO Loss turned out to be the first term.

Note: I am not coming from a stats background, so If the question is something fundamental, it would be helpful if you could tell me what it is. Also, If the question isn't clearly explained, I could explain it more in the discussionHello everyone,I was reading the paper ""Neural Discrete Representation Learning"" and I was puzzled when I looked at the first term in VQ-VAE Loss EquationI understand the role of the second and the third term. However, I am not able to derive the first term from either the MSE between the original and reconstructed image. I assumed it will be similar to the ELBO Loss in the VAE. The paper mentions why they have omitted the KL Divergence Term, but even then I don't understand how the expectation in the ELBO Loss turned out to be the first term.Note: I am not coming from a stats background, so If the question is something fundamental, it would be helpful if you could tell me what it is. Also, If the question isn't clearly explained, I could explain it more in the discussion

\[Discussion\]",MachineLearning,28,16,1731499665.0,1gqbeie,Snoo_65491,https://www.reddit.com/r/MachineLearning/comments/1gqbeie/discussion_proof_of_reconstruction_loss_term_in/,Discussion
[R] State-space models can learn in-context by gradient descent,,MachineLearning,26,4,1731005057.0,1glxr2v,anandtrex,https://arxiv.org/abs/2410.11687,Research
[R] MaskBit: Embedding-free Image Generation via Bit Tokens,"**Paper:** [https://arxiv.org/pdf/2409.16211](https://arxiv.org/pdf/2409.16211)

**Abstract:**

>Masked transformer models for class-conditional image generation have become a compelling alternative to diffusion models. Typically comprising two stages - an initial VQGAN model for transitioning between latent space and image space, and a subsequent Transformer model for image generation within latent space - these frameworks offer promising avenues for image synthesis. In this study, we present two primary contributions: Firstly, an empirical and systematic examination of VQGANs, leading to a modernized VQGAN. Secondly, a novel embedding-free generation network operating directly on bit tokens - a binary quantized representation of tokens with rich semantics. The first contribution furnishes a transparent, reproducible, and high-performing VQGAN model, enhancing accessibility and matching the performance of current state-of-the-art methods while revealing previously undisclosed details. The second contribution demonstrates that embedding-free image generation using bit tokens achieves a new state-of-the-art FID of 1.52 on the ImageNet 256x256 benchmark, with a compact generator model of mere 305M parameters.

**Visual Abstract:**

https://preview.redd.it/knmmcdsnu4td1.png?width=863&format=png&auto=webp&s=857739c60b5b8e45abe757ffd6d6f00a9821f12b

**Highlights:**

\[VQGAN enhancement\]

>We provide a detailed ablation of key components in the VQGAN design, and propose several changes to them, including model and discriminator architecture, perceptual loss, and training recipe. As a result, we significantly enhance the VQGAN model, reducing the reconstruction FID from 7.94 \[11\] to 1.66, marking an impressive improvement of 6.28.

>\[...\] The initial modifications to the Taming-VQGAN baseline are as follows: (1) removing attention blocks for a purely convolutional design, (2) adding symmetry to the generator and discriminator, and (3) updating the learning rate scheduler. Removing the attention layers, as adopted in recent methods \[4, 55, 56\], reduces computational complexity without sacrificing performance.

\[Bit tokens\]

>Our resulting method, employs a binary quantization process by projecting latent embeddings into K dimensions and then quantizing them based on their sign values. This process produces bit tokens, where each token is represented by K bits. We empirically observe that this representation captures high-level structured information, with bit tokens in close proximity being semantically similar. This insight leads us to propose a novel embedding-free generation model, MaskBit, which directly generates images using bit tokens, eliminating the need for learning new embeddings (from VQGAN token indices to new embedding values) as required in traditional VQGAN-based generators \[11, 4, 56\].

>\[...\] The Stage-II training follows the masked modeling framework \[9\], where a certain number of tokens are masked (i.e., replaced with a special mask token) before being fed into the transformer, which is trained to recover the masked tokens. This approach requires an additional entry in the embedding table to learn the embedding vector for the special mask token. However, this presents a challenge for an embedding-free setup, where images are generated directly using bit tokens without embedding lookup. Specifically, it raises the question of how to represent the masked bit tokens in the new framework. To address this challenge, we propose a straightforward yet effective solution: using zeros to represent the masked bit tokens. In particular, a bit token t is represented as t ∈ {−1, 1}^(K) (i.e., K-bits, with each bit being either −1 or 1), while we set all masked bits to zero. Consequently, these masked bit tokens do not contribute to the image representation.

>\[...\] With increasing number of bits, the categorical cross-entropy gets computed over an exponentially increasing distribution size. Given that bit tokens capture a channel-wise binary quantization, we explore masking “groups of bits”. Specifically, for each bit token t ∈ {−1, 1}^(K), we split it into N groups tn ∈ {−1, 1}^(K/N) , ∀n ∈ {1, · · · , N}, with each group contains K/N consecutive bits. During the masking process, each group of bits can be independently masked. Consequently, a bit token t may be partially masked, allowing the model to leverage unmasked groups to predict the masked bits, easing the training process. During the inference phase, the sampling procedure allows to sample some groups and use their values to guide the remaining samplings. However, this approach increases the number of bit token groups to be sampled, posing a challenge during inference due to the potential for poorly chosen samples. Empirically, we found that using two groups yields the best performance, striking a good balance.

>\[...\] Empirically, we find that using 14 bits works the best on ImageNet.

>\[...\] MaskBit follows the non-autoregressive sampling paradigm \[4, 55\], enabling flexibility in the number of sampling steps during inference (up to 256 steps in our ImageNet 256×256 experiments). Unlike autoregressive models \[11, 47\], this approach allows for fewer forward passes through the Stage-II generative model, reducing computational cost and inference time. However, increasing MaskBit’s sampling steps to match those of autoregressive models can also improve performance.

**Visual Highlights:**

https://preview.redd.it/6jd9e0vxv4td1.png?width=1089&format=png&auto=webp&s=90f91e776462b85de50491b06aa65cc854826ddb

https://preview.redd.it/hjhfdhszv4td1.png?width=845&format=png&auto=webp&s=4caeb2681966a0917ab299c5fa35cf8d67e05da2

https://preview.redd.it/88u6hu63w4td1.png?width=1109&format=png&auto=webp&s=3ccd4250ba1dc09490527da7c25d82f1eae61ce4

https://preview.redd.it/5ps9hxq4w4td1.png?width=987&format=png&auto=webp&s=4261df159d7c4570811497a126abe7bf305eeb7a

https://preview.redd.it/zhlkfnp6w4td1.png?width=989&format=png&auto=webp&s=c430a9df12e9d19aa727b762c30945116af5d283

",MachineLearning,28,4,1728219645.0,1fxfy91,StartledWatermelon,https://www.reddit.com/r/MachineLearning/comments/1fxfy91/r_maskbit_embeddingfree_image_generation_via_bit/,Research
[P] I tried to map the most recurrent and popular challenges in AI by analyzing hundreds of Reddit posts.,"Hey fellow AI enthusiasts and developers! I've been working on a project to analyze and visualize the most common technical challenges in AI development by looking at Reddit posts on dedicated subs.

# Project Goal

The main objective of this project is to identify and track the most prevalent and trending technical challenges, implementation problems, and conceptual hurdles related to AI development. By doing this, we can:

1. Help developers focus on the most relevant skills and knowledge areas
2. Guide educational content creators in addressing the most pressing issues
3. Provide insights for researchers on areas that need more attention or solutions

# How It Works

1. **Data Collection**: I fetched the hottest 200 posts from each of the followingAI-related subreddits: r/learnmachinelearning, r/ArtificialIntelligence, r/MachineLearning, r/artificial.
2. **Screening**: Posts are screened using an LLM to ensure they're about specific technical challenges rather than general discussions or news.
3. **Summarization and Tagging**: Each relevant post is summarized and tagged with up to three categories from a predefined list of 50 technical areas (e.g., LLM-ARCH for Large Language Model Architecture, CV-OBJ for Computer Vision Object Detection).
4. **Analysis**: The system analyzes the frequency of tags, along with the associated upvotes and comments for each category.
5. **Visualization**: The results are visualized through various charts and a heatmap, showing the most common challenges and their relative importance in the community.

# Results (here are the [figures](https://imgur.com/a/CHsQMgA)):

1. Top 15 Tags by Combined Score (frequency + upvotes + comments)
2. Normalized Tag Popularity Heatmap
3. Tag analysis table with individual scores

# Feedback

I'd love to get your thoughts on this project and how I can make it more useful for the AI development community. Specifically:

1. Are there any other data sources we should consider beyond Reddit?
2. What additional metrics or analyses would you find valuable?
3. How can I make the results more actionable for developers, educators, or researchers?
4. Are there any potential biases or limitations in this approach that we should address?
5. Would you be interested in a regularly updated dashboard of these trends?

Your insights and suggestions are greatly appreciated!

**TL;DR: AI Development Challenges Analyzer**

* Project analyzes Reddit posts to identify common AI development challenges
* Uses ML to screen, summarize, and tag posts from AI-related subreddits
* Visualizes results to show most discussed and engaging technical areas
* [View results here](https://imgur.com/a/CHsQMgA)
* Seeking feedback to improve the analysis",MachineLearning,27,21,1727697208.0,1fstn9m,Fixmyn26issue,https://www.reddit.com/r/MachineLearning/comments/1fstn9m/p_i_tried_to_map_the_most_recurrent_and_popular/,Project
[D] Call to intermediate RL people - videos/tutorials you wish existed?,"I'm thinking about writing some blog posts/tutorials, possibly also in video form. I'm an RL researcher/developer, so that's the main topic I'm aiming for.

I know there's a ton of RL tutorials. Unfortunately, they often cover the same topics over and over again.

The question is to all the intermediate (and maybe even below) RL practitioners - are there any specific topics that you wish had more resources about them? 

I have a bunch of ideas of my own, especially in my specific niche, but I also want to get a sense of what the audience thinks could be useful. So drop any topics for tutorials that you wish existed, but sadly don't!",MachineLearning,27,12,1723908079.0,1euk7jh,SmolLM,https://www.reddit.com/r/MachineLearning/comments/1euk7jh/d_call_to_intermediate_rl_people_videostutorials/,Discussion
[D] Is it possible to juggle doing research and working full time?,I am currently working as an AI engineer for a life sciences company. I am interested in deep learning research (computer vision in particular) but at the same time don't want to leave my job. Is it possible for me to work as a researcher with a professor remotely after my work hours? Willing to dedicate 20 hours/week for this. (2 hours/weekday and 10 hours/weekend),MachineLearning,28,22,1722277018.0,1ef6mnn,Hour_Amphibian9738,https://www.reddit.com/r/MachineLearning/comments/1ef6mnn/d_is_it_possible_to_juggle_doing_research_and/,Discussion
[D] Llama 3 vs llama 3.1 in Medical Domain: Llama 3 Outperforms 3.1 in Base Category,"Just analyzed Llama 3 and 3.1 models in medical tasks. Here are the key findings:

1. 🥇 Meta-Llama-3.1-70B-Instruct: Overall champion 🥈 Meta-Llama-3-70B-Instruct: Close runner-up
2. But here's the shocker: In base models (both 70B and 8B), Llama 3 often outperforms Llama 3.1! 🤯
3. Llama 3.1 70B instruct beats GPT-4 in a few tasks and is almost equal to GPT-4 in Open Medical-LLM Leaderboard

**70B Models:**

Instruct:

* Llama 3.1 generally outperforms Llama 3
* 3.1 excels in college biology, 3 in college medicine
* Both strong in medical genetics

Base:

* Surprisingly, Llama 3 outperforms 3.1 overall
* 3 dominates in college anatomy
* 3.1 superior in medical genetics and professional medicine

**8B Models:**

Instruct:

* Llama 3.1 leads in most categories
* 3.1 shines in college biology
* 3 maintains edge in medical genetics

Base:

* Llama 3 slightly outperforms 3.1 overall
* 3 better in anatomy
* 3.1 excels in medical genetics and PubMedQA

  
for a detailed comparison check out   
[https://x.com/aadityaura/status/1815836602607768041](https://x.com/aadityaura/status/1815836602607768041)

For the latest on AI models, datasets, and research in life sciences, check out **Open Life Science AI**.    
Don't miss any Model or dataset in the **AI x Healthcare domain**  [https://x.com/openlifesciai](https://x.com/openlifesciai)

https://preview.redd.it/5nzv5cd4zbed1.jpg?width=2168&format=pjpg&auto=webp&s=52a9abc2fb152393c9378e216e77a29df8e45ec4",MachineLearning,25,8,1721767700.0,1eainqi,aadityaura,https://www.reddit.com/r/MachineLearning/comments/1eainqi/d_llama_3_vs_llama_31_in_medical_domain_llama_3/,Discussion
[D] Best places to rent GPUs from,"Hello guys,

I want to have the flexibility to rent GPUs on demand and ofc not pay a lot. I have been looking at couple companies like brev.Dev, runpod and fluidstack. I wanted to know if you guys are using any of these or something different to run your workloads ",MachineLearning,29,24,1721180574.0,1e562n1,OGbeeper99,https://www.reddit.com/r/MachineLearning/comments/1e562n1/d_best_places_to_rent_gpus_from/,Discussion
[Discussion] Diminishing Return problem as a Machine Learning Engineer.,"For all my fellow Machine Learning Engineer or Applied Scientist, how do you deal with diminishing return problem at work?  Say you already move your model to DL, you already use transformer and you already used all the features you can think of, now what do you do to continue show impact as a Machine Learning Engineer in a product team.",MachineLearning,30,23,1718485563.0,1dgrmf4,The-AI-Alchemy,https://www.reddit.com/r/MachineLearning/comments/1dgrmf4/discussion_diminishing_return_problem_as_a/,Discussion
[D] How to reconcile double descent with chincilla scaling laws?,"When I heard about the Chinchilla scaling law a while back, they seemed to suggest that many of the mainstream LLMs were significantly undertrained, and that they should be trained on far more data, keeping their model size fixed.

However, I recently also came across the concept of 'double descent', which seems to argue the opposite - that you should just increase the number of parameters in your model as much as your compute budget will allow, even if the ratio of parameters/samples is very high, and as long as you have some kind of regularization the model will still perform very well out-of-sample despite massively overfitting.

How can I reconcile these two seemingly opposing arguments? One argues to lower the param/samples ratio, the other argues to raise it.",MachineLearning,28,8,1717743668.0,1da4j55,sam_the_tomato,https://www.reddit.com/r/MachineLearning/comments/1da4j55/d_how_to_reconcile_double_descent_with_chincilla/,Discussion
[D] What is your favorite way to expand your knowledge in the field post degree?,"I finished my M.S. in Statistics over a year ago, and my knowledge has gotten a bit rusty and I want to get back into pushing myself and growing my understanding of all these great concepts at our fingertips. I tend to love just picking up a textbook and working through it, or finding a cool data set and building out some models to solidify what I've been learning.

I know that beyond getting a PhD or working in a lab, it's hard to actually make meaningful strides in terms of research; however, I honestly just deeply love math/statistics/CS, so studying this stuff is probably my favorite way to relax at the end of the day. 

  
What's your favorite way to keep yourself growing?",MachineLearning,28,17,1716342308.0,1cxos4s,RawCS,https://www.reddit.com/r/MachineLearning/comments/1cxos4s/d_what_is_your_favorite_way_to_expand_your/,Discussion
[D] What role do you think machine learning will play in fields like computational biology and bioinformatics in the coming years?,"I believe that computation biology and bioinformatics are going to be adopting ML work more and more, and I’m quite excited to see what advancements are made. I think it is going to open up a whole new world in terms of matching diseases to current medications that could potentially be used off label. What other things should we be on the lookout for?

Who are some researchers working in this world?",MachineLearning,28,27,1716149967.0,1cvxcbc,RawCS,https://www.reddit.com/r/MachineLearning/comments/1cvxcbc/d_what_role_do_you_think_machine_learning_will/,Discussion
[D] Are LLM observability tools really used in startups and companies?,"There are many LLM observability and monitoring tools launching every week. Are they actually used by real startups and companies? 

These tools seem to do one or a combination of the following:
- **monitor LLM inputs and outputs** for prompt injection, adversarial attacks, profanity, off-topic content, rtc
- **monitor LLM metrics** over time such as cost, latency, readability, output length, and custom metrics (tone, mood, etc), drift
- **prompt management**: a/b testing, versioning, gold standard set

What have you observed — in real companies who have their own LLM-powered features or products, do they used these tools?",MachineLearning,27,23,1716148240.0,1cvwohz,WolvesOfAllStreets,https://www.reddit.com/r/MachineLearning/comments/1cvwohz/d_are_llm_observability_tools_really_used_in/,Discussion
[R] Teaching VLMs to Convert Handwritten Images into Digital Ink with Read and Write Tasks,"# InkSight: Offline-to-Online Handwriting Conversion by Learning to Read and Write

[**Project Page**](https://charlieleee.github.io/publication/inksight/) **|** [**Model Release**](https://github.com/google-research/inksight) **|** [**Google Research Blog**](https://research.google/blog/a-return-to-hand-written-notes-by-learning-to-read-write/) **|** [**Hugging Face**](https://huggingface.co/spaces/Derendering/Model-Output-Playground)

**TLDR:**

By teaching Vision-Language Models to read and write we are able to bridge the gap between traditional handwriting and digital ink, delivering high-quality digital tracings evaluated through blind studies with 87% judged as valid and 67% indistinguishable from human-generated ink.

Ablation studies highlight the importance of recognition (“reading”) tasks in ensuring semantic consistency, while inference strategies demonstrate flexibility in handling ambiguous handwriting. Additionally, using derendered ink as training data enhances handwriting recognition when combined with real-world datasets, reducing Character Error Rate to 4.6%. These findings showcase InkSight’s potential to advance handwriting digitization and recognition systems.",MachineLearning,26,0,1735158364.0,1hm8a6s,CharlieLee666,https://www.reddit.com/r/MachineLearning/comments/1hm8a6s/r_teaching_vlms_to_convert_handwritten_images/,Research
[D] AAMAS 2025 reviews are out! ,"I could not find a discussion thread, so I thought I would create one myself. ",MachineLearning,26,44,1732717709.0,1h15k8k,E-Cockroach,https://www.reddit.com/r/MachineLearning/comments/1h15k8k/d_aamas_2025_reviews_are_out/,Discussion
[D] Your ML PhD duration,How many years you take to finish ML PhD after bachelor’s? I understand different parts of the world usually have different duration. ,MachineLearning,26,32,1731783883.0,1gsue6g,AntelopeWilling2928,https://www.reddit.com/r/MachineLearning/comments/1gsue6g/d_your_ml_phd_duration/,Discussion
[D] Fourier weights neural networks,"Dear ML community,

I wanted to share an idea for discussion about the usage of Fourier coefficients to parametrize weights in neural networks. Typically in MLPs the weights are defined only in one direction, and are undefined in the other direction, which leaves it open: we can define the weights to be symmetric: w(r,s) = w(s,r) and we can use the Fourier coefficients of a two variable symmetric function to compute the weights via backpropagation and gradient descent. (I should mention that I am currently activeyl searching for an opportunity to bring my knowledge of Machine Learning to projects near Frankfurt am Main ,Germany.)

  
**Edit:** Maybe my wording was not so correct. Let us agree that in most cases the symmetry assumption is satisfied by MLPs with invertible activation function. The idea I would like to discuss is the usage of Fourier coefficients to (re-) construct the weights w(r,s) = w(s,r) . For this idea to make sense the FWNN do not learn the weights as usual MLPs / ANNs , but they learn the \_coefficients\_ of the Fourier series (at least some of them). By adjusting how many coefficients are learned, the FWNN could adjust its capacity to learn. Notice that by symmetry of the function w(r,s) we get terms like sum\_{j\] c\_j\*cos(j \* (r+s) ) where j ranges over some predefined range \[-R,R\] of integers. In theory this R should be infinity hence Z = \[-inf, +inf\] are the whole integers. Notice also that the parameter c\_j the network learns are 2\*R+1 in number, which at first glance is independent of the number of neurons N. Hence a traditional neural network with N neurons, has in theory to learn O(N\^2) weights, but with the Fourier transform we reduce this number of parameters to 2\*R+1. Of course it can happen that R = N\^2 but I can imagine that there are problems where 2\*R+1 << N\^2. I hope this clarifies the idea.

Code: [https://github.com/githubuser1983/fourier\_weighted\_neural\_network/blob/main/fourier\_weighted\_neural\_network.py](https://github.com/githubuser1983/fourier_weighted_neural_network/blob/main/fourier_weighted_neural_network.py)

Explanation of the method: [https://www.academia.edu/125262107/Fourier\_Weighted\_Neural\_Networks\_Enhancing\_Efficiency\_and\_Performance](https://www.academia.edu/125262107/Fourier_Weighted_Neural_Networks_Enhancing_Efficiency_and_Performance)",MachineLearning,26,7,1730665672.0,1giwdum,musescore1983,https://www.reddit.com/r/MachineLearning/comments/1giwdum/d_fourier_weights_neural_networks/,Discussion
[D] Faith and Fate: Transformers as fuzzy pattern matchers,,MachineLearning,26,1,1728797624.0,1g2irug,jsonathan,https://www.answer.ai/posts/2024-07-25-transformers-as-matchers.html,Discussion
I tried to code my own YOLO model to detect Football players [D],,MachineLearning,26,2,1725729358.0,1fbc185,AvvYaa,https://youtu.be/pGVTWZnixPc,Discussion
"[D] retrieval-augmented generation vs Long-context LLM, are we sure the latter will substitute the first?","I think this issue has been debated for a long time. But two interesting articles have recently come out on the issue that I would like to take as a starting point for the discussion on RAG vs. Long-context LLM.   
  
In summary, if we can put everything in the prompt, we don't need to do retrieval. However I really doubt that we can have a model capable of having a context length that can cover the huge amount of data that any organization has (and without horrendous computational costs).   
  
In any case, there have been unconvincing reports that LC-LLM works better in QA (so far at least I have not read an article that convinced me that LC-LLM works better than RAG). 

 Two articles came out discussing the impact of noise in LLM and RAG: 

* The first states that noise bumps the performance of an LLM and goes to great lengths to characterize this. [https://arxiv.org/abs/2408.13533](https://arxiv.org/abs/2408.13533)  
* The second one compares RAG and LC-LLMs and shows that by increasing the size of the context, we have a spike (we add relevant chunks) and then performance decreases because LLM has a harder time finding the correct information. [https://arxiv.org/abs/2409.01666](https://arxiv.org/abs/2409.01666)  

  
I think more or less the reason why we will eventually keep RAG, is that LLMs are sophisticated neural networks and therefore pattern recognition machines. In the end, optimizing signal-to-noise is one of the most common (and sometimes difficult) tasks in machine learning. When we start to increase this noise too much eventually the model is bound to start finding noise and get distracted from important information (plus there is also a subtle interplay between the LLM's parametric memory and context, and we still don't know why sometimes ignores the context)

Two, in my personal opinion, there is also a structural reason. self-attention seeks relevant relationships, and under conditions of increased context length, we tend toward a curse of dimensionality in which eventually spurious relationships are accentuated.

I would like to discuss your opinion for what reasons RAG will not be supplanted or if you think LC-LLM will eventually replace it? In the second case, how can it solve the problem of a huge amount of contextually irrelevant data?

",MachineLearning,27,18,1725618552.0,1fabu65,NoIdeaAbaout,https://www.reddit.com/r/MachineLearning/comments/1fabu65/d_retrievalaugmented_generation_vs_longcontext/,Discussion
"[P] New LLM Pre-training and Post-training Paradigms: Comparing Qwen 2, Llama 3.1, Gemma 2, and Apple's FMs",,MachineLearning,26,6,1723899610.0,1euh58q,seraschka,https://magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training,Project
[R] What is Flash Attention? Explained ,"A major advancement over the standard Attention mechanism (used in Attention is all you need) , Flash Attention improves it on space and time complexity. Check out to know more : https://youtu.be/znhk2mgplWY?si=Q3fz5GuMuyyWSdhd",MachineLearning,26,8,1720974763.0,1e36jye,mehul_gupta1997,https://www.reddit.com/r/MachineLearning/comments/1e36jye/r_what_is_flash_attention_explained/,Research
[D]How do you filter and digest every day news and papers efficiently in mid-2024? Any tool to recommend?,I spend too much time reading fewer than 10 papers per day. Are there any new tools for quickly digesting papers?,MachineLearning,25,12,1720838034.0,1e1zl10,Historical-Tree9132,https://www.reddit.com/r/MachineLearning/comments/1e1zl10/dhow_do_you_filter_and_digest_every_day_news_and/,Discussion
[R] Recruitment at top ML conferences,"I got lucky to have my first-authored publication accepted as spotlight at a top ML conference. Just recently got an email from Bytedance inviting me to chat either online or at the venue. I’m just wondering

1. Are they mass sending these emails to the authors of spotlight/oral papers? Sorry I don’t directly know anyone else, and I ask because it appears that my paper is not directly aligned with their focus which according to them is mostly LLMs

2. How do I make the most of these opportunities even if I can’t attend the conference in person? Will they be asking me to do LC interview or just talk about this particular project, or what?

I’m not super familiar with this and greatly appreciate any experience. Thanks!",MachineLearning,28,6,1719370074.0,1doo538,logichael,https://www.reddit.com/r/MachineLearning/comments/1doo538/r_recruitment_at_top_ml_conferences/,Research
[P] Automated LoRA Discovery,"I put together a suite of methods centered around methods that either 

1. Constrain the search space when training LoRAs 
2. Use generative models to explore the manifold of plausible LoRAs

A full writeup can be found [here](https://sweet-hall-e72.notion.site/Automated-LoRA-Discovery-and-Teaching-Neural-Networks-to-make-Neural-Networks-22aa3b5ad66e4bc985ff2c93896538d2) along with links to code, models, and datasets, but I'll give an overview of the main points here as well.

LoRAs and other adapter methods allow us to cut down on needed trainable parameters by a lot, but we may be able to cut down much much further if we have a prior over the kind of results we want. Additionally, LoRAs and similar are just small enough that we can train models on them.

https://preview.redd.it/cn4m3ql6tv3d1.png?width=1150&format=png&auto=webp&s=b203c7c48090d55a1d15c508119466c874e1c2cb

The main methods that had interesting results were directly training a diffusion model to generate LoRAs and training learnable coefficients to mix LoRAs either globally or per-layer, similar to work by [sakana.ai](http://sakana.ai) with evolutionary model-merging.

https://preview.redd.it/goh703gguv3d1.png?width=1718&format=png&auto=webp&s=b06172f7800eecabd120b324cc090fe54b12624e

For experimentation, I created a dataset of 136 LoRAs trained on different celebrities (something I could hope to be a reasonably low dimensional manifold) each of 1.3M parameters for Stable Diffusion. Only requiring an at-home 3090.

The dataset is first prepared by fusing all LoRAs into full weight matrices, then decomposing again using singular value decomposition to ensure consistent representations and get rid of an unnecessary symmetry while modeling.

Because 136 datapoints does not get you very far, and with the knowledge that merging LoRAs also generally results in something that quacks like an in-distribution LoRA, I used sets of augmentations revolving around randomly creating merges on the fly and adding small amounts of noise.

For the learnable merging method, we keep all of our pretrained LoRAs in memory and learned weighted coefficients of their outputs, thus all of the unique celebrities create the basis vectors of the space of possible results.

I first tried a layer-wise weighting method resulting in only 14k trainable parameters and then trained on images of my face. There is decent consistency

https://preview.redd.it/ghl03mb0yv3d1.png?width=1406&format=png&auto=webp&s=b355307a0d1a4c60ea8a269761649f803ce25eb5

Then I tried a global weighting, resulting only in as many trainable parameters as there are LoRAs, 136. It's much looser this time and a bit inconsistent but its pretty interesting considering how few parameters. I think the results could be better if we could ensure our basis LoRAs were maximally unique or somehow better covered the space we want to train over

https://preview.redd.it/m929rdf3yv3d1.png?width=1454&format=png&auto=webp&s=d5bfcc30b2dfc60384365cbf3d9f32cf8130828f

While the GAN and VAE methods did not work so well, the diffusion method really went beyond my expectations. My best guess is that noise added during training and in the generative process might act as an augmentation in itself. It's capable of generating a very diverse range of people (although they generally have this subjective celebrity look to them that is difficult to escape)

I suspect this method could be made conditional as well, using image embeddings to guide the generation potentially.

https://preview.redd.it/pk0p2ef2xv3d1.jpg?width=1990&format=pjpg&auto=webp&s=0c217c532afcf97dfd5cf2cb367fbe087b8e0acf",MachineLearning,26,1,1717214957.0,1d5ei7s,ethansmith2000,https://www.reddit.com/r/MachineLearning/comments/1d5ei7s/p_automated_lora_discovery/,Project
[P] [D] Is inference time the important performance metric for ML Models on edge/mobile?,"I am currently engaged on a project that aims to give some insight to machine learning engineers about how their models perform on vast variety of mobile devices.

It is starting to be a very popular practice to embed machine learning models within apps and use them without needing any api/network connection. You can see most examples especially for apps that use computer vision heavily. Passing each and every image to cloud for processing is simply unacceptable, data heavy and slow. With the latest improvements in the field, embedding ml models to apps gets easier and preferable.

**This comes with another price though.** 

There are 1000s of mobile devices out there that come with different chipsets like Qualcomm, Exynos, Snapdragon etc. They also come with different gpu capabilities and on top of that different OS versions.

All these combinations are very likely to create some uncertainty. Does my model performs the same way it does in the office's android test phone?

After working on a computer vision and machine learning startup for more than 3 years as a lead mobile engineer who embedded 10s of models inside apps, answer to that question is very clear to me. **No, my model will not perform same on a Xiaomi Android 11 phone as it performs on your office Samsung Android 13. And often you will not even know that.**

ML engineers will be highly isolated from the app environment. They can measure the performance of ml model already with their tools in the cloud when it comes to accuracy, recall etc. Which are very very important metrics. But, they already measure/evaluate that. **When it comes to inference time, it heavily depends on the system it works on**. It is not feasible to have each and every mobile device in the office available.

To solve this issue, we have decided to develop mobile SDK and a platform for collecting/visualising some metrics. And we have decided the most important metric, at the heart of the issue, would be the **inference time**.

I would like to ask you people if this makes sense and is reasonable. Is there other vital metrics you think a ml engineer would be interested in? 

The SDK we prepared collects all device related metadata( memory available, cpu usage, os, api level, battery etc.) and inference time parameter and shows charts like:

- OS System vs inference time

- Device model vs inference time

- Memory available vs inference time in a single session etc.











",MachineLearning,25,11,1714862654.0,1ckcn7b,orcnozyrt,https://www.reddit.com/r/MachineLearning/comments/1ckcn7b/p_d_is_inference_time_the_important_performance/,Discussion
[D] - Someone please explain me how multihead latent attention is used for autoregressive modeling,"Since key and value of entire sequence is compressed into a latent vector, latent vector has information from entire sequence. So the model can peek ahead and hence break the autoregressive setting. So how autoregressive modeling is done using it?",MachineLearning,25,7,1735717893.0,1hqyfoq,TwoSunnySideUp,https://www.reddit.com/r/MachineLearning/comments/1hqyfoq/d_someone_please_explain_me_how_multihead_latent/,Discussion
[Project] Claude Francois - Let an AI review your code in the style of François Chollet,"Demo here: [https://claude-francois.crossingminds.com](https://claude-francois.crossingminds.com/)

At the recent Anthropic Builder Day hackathon, we ([Crossing Minds](https://www.crossingminds.com/platform/ml-layer/ragsys-llm-fine-tuning)) built 'Claude François', an AI code reviewer trained in the style of [François Chollet](https://github.com/fchollet), the creator of Keras. It adapts Anthropic's Claude 3.5 Sonnet for code reviewing, but instead of regular fine-tuning, we used few-shot in-context learning with our custom RAG retrieval model, trained on PRs from the [Keras project](https://github.com/keras-team/keras). Compared to a typical AI code reviewer, it provides more succinct, high-quality code reviews focused on real issues rather than superficial nitpicking.

How it works:

* Dataset: Trained on a database of public Keras GitHub PRs and François's reviews.
* Fine-Tuned RAG Embeddings: Uses active learning and RLAIF to train embeddings optimized for generating ""fchollet-level"" reviews.
* Improved Retrieval: Retrieves relevant examples not just by embedding similarity but by optimizing for mutual information.
* Self-Reflection: Employs self-reflection techniques to enhance Sonnet’s reasoning capabilities.

This technology demo showcases how [Crossing Minds' RAGSys](https://www.crossingminds.com/platform/ml-layer/ragsys-llm-fine-tuning) ICL enables domain adaptation without fine-tuning. It can be used for countless other use cases beyond code reviews, like classification, summarization, translation, search, recommendations, and more. Arxiv paper coming soon!

Try it now: [https://claude-francois.crossingminds.com](https://claude-francois.crossingminds.com/)

We'd love to hear your feedback!",MachineLearning,25,13,1732551363.0,1gzmk5n,Crossing_Minds,https://www.reddit.com/r/MachineLearning/comments/1gzmk5n/project_claude_francois_let_an_ai_review_your/,Project
"[Discussion] R^2 is negative, but the correlation between prediction and actual values is statistically significant?","I have done a little bit of digging, but didnt really find the answer to this question, so if someones knows what might be wrong, please enlighten me. I have done some out of sample predictions (3000 observations) and I am getting really weird results when evaluating a model predicting demand levels. Model used is xgb regressor. So R\^2 point out that model performs worse than simply predicting the mean of the target variable, but at the same time the correlation between actual and predicted values is statistically significant. Moreover explained variance score says that model is worse than naive model, but Theil's U-statistic says the opposite? Code and results posted below. Thought that outstanding values might be the problem, but I clipped them at 0,05 and 0,95 quantile and it does not help.

https://preview.redd.it/10kpzdqs1c1e1.png?width=966&format=png&auto=webp&s=9b93f0ef588e2fa5cb16c06f69c0fea1902e0931

https://preview.redd.it/t2rapmo22c1e1.png?width=855&format=png&auto=webp&s=ce9d8d1d2ad54c8743873560bfff8a275a14378d

",MachineLearning,24,59,1731793301.0,1gsxror,maciek024,https://www.reddit.com/r/MachineLearning/comments/1gsxror/discussion_r2_is_negative_but_the_correlation/,Discussion
[D] How to visualize the effect of an LLM attention layer on a set of tokens with an image model,"Is it possible to visualize how an LLM “imagines” a token before and after processing it through the attention layer by feeding the token embeddings into an image model? I understand you can't copy paste it over, but is there a way to capture the latent transformation caused by the attention layer and apply this transformation to the embedding space of an image model?

For example if i were to enter ""poor man,"" into an LLM the embedding for ""man"" would shift toward ""beggar"" while entering ""royal man"" it could move closer to ""king."" I want to visualize that change. Then you could transfer the embedding for man to an image model and it would create the something like a beggar or a king in this example.

It could make a really cool visualization if you captured the transformation after each attention layer and made a video by interpolating each step.",MachineLearning,25,2,1731297685.0,1gojg09,jbrinkw,https://www.reddit.com/r/MachineLearning/comments/1gojg09/d_how_to_visualize_the_effect_of_an_llm_attention/,Discussion
"[D] Thinking LLMs - Instruction following with ""Thought Generation""","[https://arxiv.org/abs/2410.10630](https://arxiv.org/abs/2410.10630)

Greg Schoeninger [u/FallMindless3563](https://www.reddit.com/user/FallMindless3563/), [Oxen.ai](http://oxen.ai/) CEO and Master of Plain Speak, has attempted to reproduce the findings in this paper using only model inferencing, datasets, and a fine-tuning API.

Call to show results and dive in the paper starts at today at 10:00 AM Pacific, 1:00 PM Eastern.

[https://www.oxen.ai/community/?utm\_source=x&utm\_content=y](https://www.oxen.ai/community/?utm_source=x&utm_content=y)",MachineLearning,25,6,1730478539.0,1gh9ijv,ReluOrTanh,https://www.reddit.com/r/MachineLearning/comments/1gh9ijv/d_thinking_llms_instruction_following_with/,Discussion
[D] What’s the SOTA model for style transfer as of 2024?,"What’s the current state-of-the-art for image style transfer, and is diffusion a significant improvement over Gram matrix-based methods?

I’m familiar with Gram matrix-based methods from 2017, but they struggled with higher-level concepts. Are they still used nowadays?",MachineLearning,25,13,1727589037.0,1frxkwa,JellyBean_Collector,https://www.reddit.com/r/MachineLearning/comments/1frxkwa/d_whats_the_sota_model_for_style_transfer_as_of/,Discussion
[D] Flagged a potential dual submission case to program chairs but they don't care.,"Regarding [https://www.reddit.com/r/MachineLearning/comments/1f7axjm/d\_potential\_dual\_submissions\_2\_similar\_iclr\_24/](https://www.reddit.com/r/MachineLearning/comments/1f7axjm/d_potential_dual_submissions_2_similar_iclr_24/)

A while ago I came across these two papers, and I noticed they are highly similar. I sent an email to ICLR 2024 program chairs asking them about this, including:

Katerina Fragkiadaki (CMU)

Mohammad Emtiyaz Khan (RIKEN AIP, Tokyo)

Swarat Chaudhuri (UT Austin)

Yizhou Sun (UCLA).

https://preview.redd.it/4ia3rvgr3mrd1.png?width=1390&format=png&auto=webp&s=a6697aa0b319f3b5b2821073dc6b2e48eea99357

But none of them replied at all. It's clear that they don't care anything about integrity and honesty. No respect for rules.

Science is just a game of money.",MachineLearning,26,17,1727556263.0,1frnq72,One-Tax-2998,https://www.reddit.com/r/MachineLearning/comments/1frnq72/d_flagged_a_potential_dual_submission_case_to/,Discussion
"Superposition, Phase Diagrams, and Regularization [D]","Hi everyone! I am reading through the [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html) by Anthropic, which I highly recommend. The authors present the phase diagram of a small neural network, including both theoretical and empirical versions. However, they do not apply any form of regularization in their analysis, which piqued my curiosity about the effects of superposition. This inspired me to experiment with the concept.

I find these ideas quite interesting, so I wrote a blog post to share my thoughts. You can check it out [here](https://f14-bertolotti.github.io/posts/19-09-24-phase/index.html).

While my results are not as clean as those obtained by Anthropic, I believe they are still worth sharing. I would love to hear your feedback!

Here are the main points of my post:

We start with a very simple neural network: 

https://preview.redd.it/7okwz6njlypd1.png?width=237&format=png&auto=webp&s=e0f39f6e7e903d9573cb432d20ff0479131e05dc

We train to minimize the following reconstruction loss: 

https://preview.redd.it/ftk5l5nklypd1.png?width=410&format=png&auto=webp&s=960ba69bb3b1de21d2f8b73d3a6588fb5cf3ac66

Here, λ represents the regularization strength, x\_i are the input features, and r\_i indicates the relevance of each feature. Each feature is a number between 0 and 1. Additionally, we introduce a sparsity term s. Given s, we set each feature to 0 with a probability of s.

Suppose we have only two features, encoded in a single number (so, (W = \[w\_1, w\_2\])). The network has a limited number of choices:

* Set w\_1 = 0 and w\_2 = 0, minimizing the L2 regularization.
* Set w\_1 = 1 and w\_2 = 0, encoding only the first feature.
* Set w\_1 = 0 and w\_2 = 1, encoding only the second feature.
* Set w\_1 = 1 and w\_2 = -1 (or w\_2 = -1 and w\_1 = 1), superimposing both features.

Next, I conducted several experiments varying the sparsity, regularization strength, and the relevance of the second feature (for instance, the second feature may be irrelevant when r\_2 = 0 or as relevant as five times the first feature when r\_2 = 5). This GIF shows the results of the experiments: 

https://i.redd.it/do8erlkmlypd1.gif

I also created a theoretical version of the phase diagram by computing the expected loss for each of the four scenarios, I also put the two gifs side by side for comparison: 

https://i.redd.it/d81150snlypd1.gif

As you can see, the theoretical version somewhat matches the empirical one. While it’s not perfect, the effect of regularization is evident; it discourages the superposition of features. This makes sense when you consider that (W = \[-1, 1\]) has a norm that is definitely larger than (W = \[0, 1\]) or (W = \[1, 0\]).

What do you think? Do you have any suggestions for improving these figures? I’d love to hear your thoughts!",MachineLearning,26,0,1726835908.0,1flazmd,f14-bertolotti,https://www.reddit.com/r/MachineLearning/comments/1flazmd/superposition_phase_diagrams_and_regularization_d/,Discussion
"[D] What Tools Do You Use for AI/ML Development, Training, and Inference?","I'm curious about the enterprise tools you use in your AI/ML workflows. Whether it's for development, training, inference, or using pre-built models, I'd love to know what you rely on daily.

* **Development & Training**: Which platforms or services do you prefer for building and training models?
* **Inference & Deployment**: What tools do you use for serving models at scale?
* **Pre-built Models**: Do you use platforms like Hugging Face or OpenAI for ready-to-use models?
* **Data & Experiment Tracking**: Any tools you recommend for managing datasets and tracking experiments?

Looking forward to your insights! Thanks!",MachineLearning,25,38,1726548813.0,1fiqcxh,None,https://www.reddit.com/r/MachineLearning/comments/1fiqcxh/d_what_tools_do_you_use_for_aiml_development/,Discussion
[D] Population Minimizer of The Categorical Cross Entropy Loss (a blog post),"Hey Redditors!

The last time I shared something [here](https://www.reddit.com/r/MachineLearning/comments/1eqm0lr/comment/liea4kx/), your encouragement was incredible. Thanks to that support, I've finally decided to start a little blog about my notes and research. My blog is super minimalist, it is written in pure HTML with a bit of JavaScript---so no cookies or tracking involved. It’s inspired by Lilian Weng’s ""lil'log,"" though mine is still a bit rough around the edges.

I've just published my first post, which delves into Categorical Cross Entropy. Did you know that the population minimizer of conditional risk associated with Categorical Cross Entropy is actually the true probability label distribution? While this might seem intuitive, I haven’t come across a proof for it, especially in the context of neural networks. Luckily, I found this [paper](https://hastie.su.domains/Papers/samme.pdf) that provides practically the same proof for Adaboost, just with a different label encoding. I’ve adapted it to fit modern deep learning framework.

This also explains why people sometimes interpret the output of an NN (the one after the softmax layer) as actual probability of the labels.

I needed this proof for another research project, but for a long time, I either avoided searching for it or just couldn’t find anything. Eventually, I did track down what I was looking for. Unfortunately, Reddit doesn’t handle mathematical notation very well, so I couldn’t post it directly here. Instead, you can check out my blog post at:

[https://f14-bertolotti.github.io/posts/28-08-2024-cce-minimizer/index.html](https://f14-bertolotti.github.io/posts/28-08-2024-cce-minimizer/index.html)

I would love your feedback. Please, let me know if you spot any errors or if you have any additional insights!",MachineLearning,25,9,1725296869.0,1f7c0xw,f14-bertolotti,https://www.reddit.com/r/MachineLearning/comments/1f7c0xw/d_population_minimizer_of_the_categorical_cross/,Discussion
LLMs as Optimizers - Theory Paper Recommendation [R],"I recently learned about ""LLMs as optimizers,"" where there seems to be a line of work arguing that transformers can perform first-order optimization (gradient descent). I'm interested in the theory behind this, and I found the paper [Transformers Learn In-Context by Gradient Descent](https://arxiv.org/pdf/2212.07677) and plan to read it. I wonder if there are other classic/""must-read"" theory papers in this direction. Thanks for any input.",MachineLearning,25,5,1723522094.0,1eqylt9,mziycfh,https://www.reddit.com/r/MachineLearning/comments/1eqylt9/llms_as_optimizers_theory_paper_recommendation_r/,Research
[P] Direct Preference Optimization (DPO) for LLM Alignment From Scratch [Jupyter Notebook],,MachineLearning,26,1,1722781467.0,1ejwg9n,seraschka,https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb,Project
[P] DataChain: curate unstructured data using local models and LLM calls,"Hello! We are open sourcing DataChain today: [https://github.com/iterative/datachain](https://github.com/iterative/datachain)! What it does:

* reads data from S3/GCS/Azure/local & versions datasets
* applies transformations: local model inference, external LLM calls or custom code
* stores Python objects via Pydantic in internal DB (SQLite) or exports to parquet/CSV files
* runs code efficiently in parallel and out-of-memory, handling millions of files in a laptop
* executes vectorized operations: similarity search for embeddings, sum, avg, etc.

Example - evaluating chatbot dialogs using Mistral:

    from datachain import DataChain, Column
    from mistralai.client import MistralClient
    from mistralai.models.chat_completion import ChatCompletionResponse, ChatMessage
    
    def eval_dialogue(file: File) -> ChatCompletionResponse:
        return MistralClient().chat(
            model=""open-mixtral-8x22b"",
            messages=[ChatMessage(role=""system"", content=PROMPT),
                      ChatMessage(role=""user"", content=file.read())])
    
    chain = (
        DataChain.from_storage(""gs://datachain-demo/chatbot-KiT/"")
        .settings(parallel=4, cache=True)
        .map(response=eval_dialogue)
        .save(""mistral_dataset"")
    )

Under the hood, DataChain utilizes Pydantic for serializing Python objects; SQLite as a meta-storage and for executing vectorized operations, and DVC for working with data storages.

WDYT? Eager to hear your thoughts!",MachineLearning,25,10,1721742331.0,1ea881b,dmpetrov,https://www.reddit.com/r/MachineLearning/comments/1ea881b/p_datachain_curate_unstructured_data_using_local/,Project
[R] Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?,"A new benchmark for multimodal AI agents, focused on real-world Dara Engineering tasks.

Project page: [link](https://spider2-v.github.io/), paper: [link](https://spider2-v.github.io/static/data/Spider2-V.pdf), code: [link](https://github.com/xlang-ai/Spider2-V).

=====

TLDR: Autonomous LLM-agents can’t replace Data Engineers…yet. But at least we can track progress 🫡

Overview:

As AI technologies become more advanced, we need increasingly complex benchmarks to evaluate the quality of systems and measure progress. A distinct branch of benchmarks has emerged, focusing on working with professional tools/applications and websites (see [WorkArena](https://github.com/ServiceNow/WorkArena), [WebArena](https://webarena.dev/), [OSWorld](https://os-world.github.io/)).

In the Spider2-V project, a benchmark is being created to evaluate AI agents in data engineering. It consists of 494 tasks covering the entire work cycle:

* Data Warehousing (tools like Snowflake, BigQuery)
* Data Ingestion (e.g., Airbyte)
* Data Transformation (e.g., dbt)
* Data Visualization (e.g., Superset, Metabase)
* Data Orchestration (e.g., Airflow, Dagster)

(and beloved Excel files, because who can do without them?)

If you have experience with data engineering, you understand that this is a substantial set, though it doesn't cover the entire zoo of solutions you might encounter.

**Preparing each task took an average of 4 hours, so they are quite atomic and do not require very long horizon thinking**. Tasks are divided into three levels of difficulty:

* Easy (20%, no more than 5 steps to solve)
* Medium (63%, 6-15 steps)
* Hard (17%, 16-40 steps)

All tasks are based on DE/DS tutorials, derived from the web by human labelers. Feasible to say that they represent real use cases. An example of a simple task:

>Load data under the current Google Drive folder into a new table “data1” of the opened BigQuery dataset

Or a task of medium difficulty:

>Install dbt-cloud-cli from GitHub and extract the binary to the same folder as the dbt project ""analytics”

To solve tasks, LLM agents have access to an IDE and a browser (with accounts set up). The model generates Python code using [pyautogui](https://github.com/asweigart/pyautogui) to interact with the UI of the virtual machine, then the code is executed, and the process repeats step by step.

===

Guess how many tasks GPT-4 completed? 

Only 14%!  It seems low, but one can highlight more successful clusters—**40% of simple tasks and 25% of data visualization tasks were solved.**

In addition to proprietary models, open models (LLAMA 3 70B, Mixtral 8x7B) were tested, but since they are not multimodal and do not accept images as input, they were only shown a text description of the screen. This significantly lowered their metrics—they solved only a percentage of the tasks. However, we are eagerly awaiting LLAMA-3 405B, rumored to be multimodal and set to be released on July 23rd.

===

I am VERY eager to see the benchmark metric published with the release of GPT-5—then we'll see! >!Place your bets on what percentage of tasks the next-generation models will solve! !<",MachineLearning,25,1,1721244025.0,1e5qt1r,stalkermustang,https://www.reddit.com/r/MachineLearning/comments/1e5qt1r/r_spider2v_how_far_are_multimodal_agents_from/,Research
"[D] Best ""Retrieval Augmented Generation"" Orchestrator in your opinion?",So I'm currently developing a project that includes Gen AI with vertex ai for gemini 1.5 flash and I'm planning to add a RAG system for it and i plan on using MongoDB for the vector db to keep things simple. Now I trying to decide which orchestrator I should use for the RAG system to speed up development. What do y'all suggest?,MachineLearning,25,37,1721029920.0,1e3p5fx,PsychologicalAd7535,https://www.reddit.com/r/MachineLearning/comments/1e3p5fx/d_best_retrieval_augmented_generation/,Discussion
[D] What's the best way for me to go about building a robust yet human-like playable Poker AI Model,"I'm working on a (Texas hold 'em) Poker game and I'd like to have an AI that can play at a human-ish level. I've developed a win probability calculator which can find the odds of you having the best hand in the game given your cards, the community cards, and the number of players in the game.

I'm unsure of where to go from here. I study ML/AI in school but I've been having a hard time making the best decision on how to actually apply these tools in practice. Firstly, I'm unsure of what dataset to use, I found a [dataset](https://www.kaggle.com/datasets/smeilz/poker-holdem-games/data?select=File196.txt) of online poker game logs which might useful. 

Also, I don't know whether to develop a decision tree, use neural networks, or a combination of the two and/or other methods.

What's the best way to go about building my AI model using ML for this project?",MachineLearning,26,36,1716666632.0,1d0k0b0,HandfulOfAStupidKid,https://www.reddit.com/r/MachineLearning/comments/1d0k0b0/d_whats_the_best_way_for_me_to_go_about_building/,Discussion
"[P] A look at the latest major open LLM releases: Mixtral, Llama 3, Phi-3, and OpenELM",,MachineLearning,26,1,1715516152.0,1cq6jgz,seraschka,https://magazine.sebastianraschka.com/p/how-good-are-the-latest-open-llms,Project
[D] Strange Loss Curve while training,"https://preview.redd.it/z5wmyi0nb8zc1.png?width=599&format=png&auto=webp&s=97e108bd749f9cf0874759f7ba0b8aafb3260640

Today I was training a small (11.07 Million) parameter GPT model on text dataset and I came across this loss curve while training, is there any explaination as to why the loss first plataeus around 2.4 and then starts to exponentially fall there after? Also why is there a sudden spike in between at around 1200 steps?

- The dataset I am using is the entire novel ""one hundred years of solitude""  
- The total token count in the train dataset is 0.82 million, with a vocabulary size of 77 (I am using a character level tokenizer)  
- 6 transformer layer with 6 attention heads each no bias in any proj and no dropout layer a context length of 1024 trained with a batch size of 32",MachineLearning,26,19,1715186469.0,1cn94yq,ApartmentEither4838,https://www.reddit.com/r/MachineLearning/comments/1cn94yq/d_strange_loss_curve_while_training/,Discussion
[R] Categorical Deep Learning: An Algebraic Theory of Architectures,"**Paper**: [https://arxiv.org/abs/2402.15332](https://arxiv.org/abs/2402.15332)

**Project page**: [https://categoricaldeeplearning.com/](https://categoricaldeeplearning.com/)

**Abstract**:

>We present our position on the elusive quest for a general-purpose framework for specifying and studying deep learning architectures. Our opinion is that the key attempts made so far lack a coherent bridge between specifying *constraints* which models must satisfy and specifying their *implementations*. Focusing on building a such a bridge, we propose to apply category theory -- precisely, the universal *algebra of monads* valued in a 2-category of *parametric maps* -- as a single theory elegantly subsuming both of these flavours of neural network design. To defend our position, we show how this theory recovers constraints induced by geometric deep learning, as well as implementations of many architectures drawn from the diverse landscape of neural networks, such as RNNs. We also illustrate how the theory naturally encodes many standard constructs in computer science and automata theory.",MachineLearning,24,10,1714327159.0,1cfck8b,None,https://www.reddit.com/r/MachineLearning/comments/1cfck8b/r_categorical_deep_learning_an_algebraic_theory/,Research
[P] NLLB-200 Distill 350M for en-ko,"Hello r/MachineLearning,

I'm excited to share a project that was initially intended to use in my graduating product(Capstone) 

I made NLLB-200 Distill 350M model to translating English to Korean

It's pretty good to use. small and fast. so it can be run with CPU!

GPU servers are quite expensive, so I made it for university students who can't cost the server (like me.)

more details are in my page

If you know Korean, please give me a lot of feedback

thank you!!

[https://github.com/newfull5/NLLB-200-Distilled-350M-en-ko](https://github.com/newfull5/NLLB-200-Distilled-350M-en-ko)",MachineLearning,26,10,1714267564.0,1ceuj4t,SaeChan5,https://www.reddit.com/r/MachineLearning/comments/1ceuj4t/p_nllb200_distill_350m_for_enko/,Project
[D]How have recent advancements with incorporating physics and logic turned out?,"There was significant discussion about the promise this would bring around last year, then not much afterwards.",MachineLearning,22,11,1735176500.0,1hmds2k,sext-scientist,https://www.reddit.com/r/MachineLearning/comments/1hmds2k/dhow_have_recent_advancements_with_incorporating/,Discussion
[R] Continuous Numerical Tokenization for Scientific Language Models Improves Out-of-Distribution Performance,"This paper introduces a novel approach called **xVal** for handling numerical values in language models by using continuous numerical tokenization rather than discrete tokens. The key innovation is representing numbers as continuous values while preserving their mathematical relationships, allowing models to better understand numerical patterns and relationships in scientific text.

Key technical points:
- Numbers are encoded using a continuous representation instead of discrete tokens
- The approach maintains ordering and relative magnitude information
- Custom architecture modifications allow seamless integration of numerical and text processing
- Training uses a specialized loss function that accounts for numerical relationships
- Tested on scientific datasets containing significant numerical content

Results from their experiments:
- Improved performance on numerical reasoning tasks compared to baseline models
- Better generalization to out-of-distribution numerical values
- More efficient computation after initial training
- Enhanced ability to process scientific text with heavy numerical content
- Lower perplexity scores on scientific corpus validation sets

I think this could be particularly impactful for scientific applications of language models. The ability to properly handle numbers is crucial for tasks like paper analysis, experimental design, and data interpretation. While previous approaches struggled with numerical relationships, xVal provides a more natural way to process numbers in context.

I think the real value will be in enabling more reliable AI systems for scientific research assistance. Current models often make numerical errors that limit their usefulness in technical fields. This approach could help bridge that gap.

TLDR: New method for handling numbers in language models using continuous representations instead of discrete tokens. Shows improved performance on scientific tasks and better numerical reasoning abilities.

[Full summary is here](https://aimodels.fyi/papers/arxiv/xval-continuous-numerical-tokenization-scientific-language-models). Paper [here](https://arxiv.org/abs/2310.02989).",MachineLearning,24,3,1734461831.0,1hghwfb,Successful-Western27,https://www.reddit.com/r/MachineLearning/comments/1hghwfb/r_continuous_numerical_tokenization_for/,Research
[R] Enhancing LLM Reasoning Through Bidirectional Forward-Backward Thinking,"The key contribution here is a ""reverse thinking"" method that improves LLM reasoning without any model modifications. Instead of only reasoning forward from the question to an answer, the approach adds a backward verification step - working from potential answers back to the question to validate the reasoning chain.

Key technical points:
* Two-stage process: forward generation followed by backward verification
* Backward pass examines logical consistency between answer and premises
* No fine-tuning or architectural changes needed
* Tested across multiple reasoning benchmarks (GSM8K, CommonsenseQA, LogiQA)

Results:
* 8.3% improvement on GSM8K math reasoning
* 6.2% gain on CommonsenseQA 
* 5.4% increase on LogiQA
* Consistent improvements across different model sizes
* Performance gains come at cost of 2x inference time

I think this method points to untapped potential in how we prompt LLMs for reasoning tasks. While the doubled inference time is a real tradeoff, the consistent improvements across different benchmarks suggest this approach captures something fundamental about machine reasoning. The simplicity of implementation means it could be quickly adopted in many applications where reasoning accuracy matters more than speed.

TLDR: Adding a backward reasoning verification step improves LLM performance on math, logic and common sense tasks by 5-8%, with no model changes required. Doubles inference time but provides consistent gains across different models and tasks.

[Full summary is here](https://aimodels.fyi/papers/arxiv/reverse-thinking-makes-llms-stronger-reasoners). Paper [here](https://arxiv.org/abs/2411.19865).",MachineLearning,25,0,1733234238.0,1h5nyi0,Successful-Western27,https://www.reddit.com/r/MachineLearning/comments/1h5nyi0/r_enhancing_llm_reasoning_through_bidirectional/,Research
[D] A blog post explaining sparse transformers (the original paper),"Hi!

I'm sorry if it's not appropriate to publish such posts on this subreddit. I do stay out of this type of posts on this subreddit but I keep seeing articles or videos or whatever content explaining GPT-3 without delving into sparse transformers. And it keeps frustrating me because clearly in the paper they say ""we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer"".

But no one seems to care about explaining them. I understand why to be honest but it's frustrating to see all these articles, projects, videos etc. that try to explaining everything about the GPT not even mentioning the sparse transformers part. And besides many other elements specific to GPT-3 or general to reproducibility in ML, the sparse transformer part is a big dent into even prototyping GPT-3.

I have this habit of writing down stuff when trying to understand something so I wrote a blog post on sparse transformers. Never spoke about it because I did it to restructure my thoughts and as notes for me. So it's not something I'd avise anyone to read, I'm sure it's full of typos, my writing style is not neat etc. It's just something I did for me in a way *I* would understand and recover lost bits of information when skimming through it.

Anyways, in case you're reading papers by yourself and trying to constitute the knowledge just from them, maybe my notes can help you: [https://reinforcedknowledge.com/sparse-transformers/](https://reinforcedknowledge.com/sparse-transformers/)

Sorry again if this post is not appropriate and for yapping that much.

(If you happen to read it or if you notice any errors, do not hesitate to point them out, I'd be grateful to learn from them)",MachineLearning,24,4,1732640145.0,1h0gl2j,ReinforcedKnowledge,https://www.reddit.com/r/MachineLearning/comments/1h0gl2j/d_a_blog_post_explaining_sparse_transformers_the/,Discussion
[P] Real-Time Character Animation on Any Device,"I recently came across this paper [MIMO: Controllable Character Video Synthesis with Spatial Decomposed Modeling](https://menyifang.github.io/projects/MIMO/index.html) by Alibaba and it was really interesting. After skimming through the paper, I thought, 'Hey, this workflow could be replicated using some open-source tools!' I managed to create a plausible system that can run in real-time on-device at \~10fps and mind you this was on a potato laptop 8 GB of RAM and 4 GB of VRAM.

[Original Video](https://i.redd.it/pavxor7mg4xd1.gif)

[Reconstrued Video ](https://i.redd.it/ler3v5qvd4xd1.gif)

The current workflow looks something like this ->  
1. I created a unity app using [Tracking4All](https://www.tracking4all.com/), which can take an input from a webcam and generate an animated pose using Mediapipe.  
2. Next, I sent these generated images to a Python server, which receives the original frame, the animated character, and a mask of the person from the Mediapipe pose.  
3. Fianlly using [MI-GAN](https://github.com/Picsart-AI-Research/MI-GAN), I was able to remove the person in real-time.

  
This project currently have a few flaws  
1. The MI-GAN model, while fast, is the main bottleneck. I tried other algorithms available in [OpenCV](https://docs.opencv.org/4.x/df/d3d/tutorial_py_inpainting.html) but they were even worse and slow (\~1fps).  
2. The character resizing isn’t always accurate, though this can be easily adjusted in Unity.  
3. Occlusion issues remain a challenge.

Additionally, it’s worth noting that the Tracking4All package requires a license, which may limit accessibility.

Are there any algorithms available that can perform inpainting in real-time on various devices (mobile, Windows, Mac, and Linux)?

The goal of this project is to create an end-to-end workflow that anyone can run on any device. This has many applications in AR and VFX! whats your opinion on this and any things I should implement next on this?",MachineLearning,23,0,1729957778.0,1gco234,Jazzlike-Shake4595,https://www.reddit.com/r/MachineLearning/comments/1gco234/p_realtime_character_animation_on_any_device/,Project
[D] Efficient CNNs for inference,"I am working on an object detection project using high resolution images.

Are there any techniques that can make a trained CNN (UNet) efficient during inference? I know pruning is one such technique, but it risks loss of accuracy and parallelizability.",MachineLearning,26,24,1729498829.0,1g8kpl6,_My__Real_Name_,https://www.reddit.com/r/MachineLearning/comments/1g8kpl6/d_efficient_cnns_for_inference/,Discussion
[R] Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions,"**TL;DR:** another variant of recurrent/SSM language model, optionally augmented with attention, claiming SotA.

**Paper:** [https://arxiv.org/pdf/2410.06577](https://arxiv.org/pdf/2410.06577)

**Abstract:**

>Recent advancements in Transformer-based large language models (LLMs) have set new standards in natural language processing. However, the classical softmax attention incurs significant computational costs, leading to a O(T) complexity for per-token generation, where T represents the context length. This work explores reducing LLMs' complexity while maintaining performance by introducing Rodimus and its enhanced version, Rodimus+. Rodimus employs an innovative data-dependent tempered selection (DDTS) mechanism within a linear attention-based, purely recurrent framework, achieving significant accuracy while drastically reducing the memory usage typically associated with recurrent models. This method exemplifies semantic compression by maintaining essential input information with fixed-size hidden states. Building on this, Rodimus+ combines Rodimus with the innovative Sliding Window Shared-Key Attention (SW-SKA) in a hybrid approach, effectively leveraging the complementary semantic, token, and head compression techniques. Our experiments demonstrate that Rodimus+-1.6B, trained on 1 trillion tokens, achieves superior downstream performance against models trained on more tokens, including Qwen2-1.5B and RWKV6-1.6B, underscoring its potential to redefine the accuracy-efficiency balance in LLMs. Model code and pre-trained checkpoints will be available soon.

**Visual Abstract:**

https://preview.redd.it/7cs3hk7vzwtd1.png?width=1015&format=png&auto=webp&s=2078931d95af1b1ff6cb2ac1ece1edd2f73c478d

**Limitations:**

>...Additionally, Rodimus\* lacks the highly I/O-aware optimization found in models such as Mamba and Mamba2. There’s potential to enhance its performance by designing I/O-aware multi-head scalar decay and integrating it with Rodimus’s DDTS to broaden gating mechanisms without significantly impacting training efficiency. Furthermore, for Rodimus+, the memory usage of SW-SKA can be further reduced while achieving better practical application performance. We aim to address these issues in future work.

**Visual Highlights:**

https://preview.redd.it/8awoivwg1xtd1.png?width=1167&format=png&auto=webp&s=e81cdb467f42f86b6a4b7398a06dceff4b8cb9be

https://preview.redd.it/ef9d9nqh1xtd1.png?width=1305&format=png&auto=webp&s=e2f32a8753510ec1a9672cb8ebcf7104f50ea220

https://preview.redd.it/nkm1uo7l1xtd1.png?width=1281&format=png&auto=webp&s=f587f0ceea2cee7761d88445b76095ea096a7893

[Note that only 130M-class results constitute a fair architecture evaluations \(trained on the same dataset\). The larger models for comparison were taken \\""as is\\"". While I consider Qwen-2 a strong baseline, given the limited resources of the authors' team that they could dedicate to downstream performance enhancment, attributing the high scores solely to the proposed architecture would be unjustified](https://preview.redd.it/ur57zj8m1xtd1.png?width=815&format=png&auto=webp&s=7741139dd6539e4f7cd1c11625a27b95a98ddfc7)

https://preview.redd.it/7w8e889c3xtd1.png?width=1295&format=png&auto=webp&s=336370fd6e4e18eeee21d9e322208770f97c977f

[A neat cheat sheet on modern RNN\/SSM architectures](https://preview.redd.it/h0hedjud3xtd1.png?width=1133&format=png&auto=webp&s=a804218a4cee9848990c48174cc02a6f6a538a05)

[The proposed SKA cuts KV cache roughly by factor of two. Unfortunately, the authors don't mention compression factor for GQA in the experiment. If it's 4, as is common, the results look pretty trivial](https://preview.redd.it/2aprut918xtd1.png?width=531&format=png&auto=webp&s=d75b125f4c6a594783189fe974d0c28ebe3ab4ff)

",MachineLearning,25,1,1728562857.0,1g0hi2m,StartledWatermelon,https://www.reddit.com/r/MachineLearning/comments/1g0hi2m/r_rodimus_breaking_the_accuracyefficiency/,Research
[D] Planning on building 7x RTX4090 rig. Any tips?,"I'm planning on builing a 7x RTX4090 rig with a Ryzen Threadripper 7960X and 256GB ram and 2x 2000 watt power supplies. I'm not too sure about the motherboard, but a Pro WS WRX90E-SAGE SE or similar seems suitable with 7x PCIE 16x slots. I will need to underclock (power limit) my GPUs to avoid over straining my PSUs and I will also use riser cables to fit my GPUs on the motherboard.

Anyone got experience with a similar setup? Is the 24 cores of 7960X too little for 7 GPUS?

Are there possible bandwith issues when running model parallel pytorch (such as LLM fine tunning) with this setup?

Thanks in advance for any tips or suggestions!",MachineLearning,24,52,1727005823.0,1fmrfgw,Chance-Tell-9847,https://www.reddit.com/r/MachineLearning/comments/1fmrfgw/d_planning_on_building_7x_rtx4090_rig_any_tips/,Discussion
"[Discussion] - Mojo / Modular, has anyone used it in a real project?","First of all, I should mention that I am a huge fan of Chris Lattners.

But despite Mojo being a superset of Python, I personally don't see myself learning it.

My main argument against yet a new language is that I use LLMs to do all the typing in my code... I don't see Modular train a developer pool for Mojo before LLMs can write really complex C++ or Rust.

So I don't see the point of learning yet another language.

In short, I think Mojo is the right solution at the wrong time.

Of course, someone could argue that down the line, LLMs could also write Mojo, and at that point it might be a better language due to the MLIR compiler support. My argument is that to build reliable LLMs for a langauge, you need a fair bit of existing code and well, that's going to be lacking for Mojo the coming 5-10 years...",MachineLearning,24,31,1726625891.0,1fji12z,zarrasvand,https://www.reddit.com/r/MachineLearning/comments/1fji12z/discussion_mojo_modular_has_anyone_used_it_in_a/,Discussion
[D] Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise,"Hi everyone, 

The point of this post is not to blame the authors, I'm just very surprised by the review process.

I just stumbled upon this paper. While I find the ideas somewhat interesting, I found the overall results and justifications to be very weak.   
It was a clear reject from ICLR2022, mainly for a lack of any theoretical justifications. [https://openreview.net/forum?id=slHNW9yRie0](https://openreview.net/forum?id=slHNW9yRie0)  
The exact same paper is resubmitted at NeurIPS2023 and I kid you not, the thing is accepted for a poster. [https://openreview.net/forum?id=XH3ArccntI](https://openreview.net/forum?id=XH3ArccntI)

I don't really get how it could have made it through the review process of NeurIPS. The whole thing is very preliminary and is basically just consisting of experiments.  
It even llack citations of other very closely related work such as *Generative Modelling With Inverse Heat Dissipation* [https://arxiv.org/abs/2206.13397](https://arxiv.org/abs/2206.13397) which is basically their ""blurring diffusion"" but with theoretical background and better results (which was accepted to ICLR2023)...

I thought NeurIPS was on the same level as ICLR, but now it seems to me sometimes papers just get randomly accepted.

So I was wondering, if anyone had an opinion on this, or if you have encountered other similar cases ? ",MachineLearning,21,29,1726066321.0,1fec2jq,Commercial_Carrot460,https://www.reddit.com/r/MachineLearning/comments/1fec2jq/d_cold_diffusion_inverting_arbitrary_image/,Discussion
[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",MachineLearning,25,28,1725156910.0,1f63rhf,AutoModerator,https://www.reddit.com/r/MachineLearning/comments/1f63rhf/d_selfpromotion_thread/,Discussion
"[P] Dive into Transformers and LLM World – Llama 3.1 in Go, Step by Step","I’m so excited to show the updated version of my latest open-source project here: Llama Nuts and Bolts. The previous version was built for Llama 2 and was now updated to support Llama 3.1 8B-Instruct model.

Code and documentation: [https://github.com/adalkiran/llama-nuts-and-bolts](https://github.com/adalkiran/llama-nuts-and-bolts)

And now, the documentation is also available on Github Pages: [https://adalkiran.github.io/llama-nuts-and-bolts](https://adalkiran.github.io/llama-nuts-and-bolts)

If you are curious like me about how the LLMs (Large Language Models) and transformers work and have delved into conceptual explanations and schematic drawings in the sources but hunger for deeper understanding, then this project is perfect for you too!

You will not only find the details of the Llama architecture but will find explanations of a wide variety of related concepts in the documentation directory. From reading a Pickle, a PyTorch model, a Tiktoken tokenizer model files at byte-by-byte level, to the internals of BFloat16 data type, implementation from scratch of a Tensor structure and mathematical operations including linear algebraic computations.

This project was initially started to learn what an LLM does behind by running and debugging it and was made for experimental and educational purposes only, not for production use.

The goal is to make an experimental project that can perform inference on the Llama 3.1 8B-Instruct model completely outside of the Python ecosystem (using the Go language). Throughout this journey, the aim is to acquire knowledge and shed light on the abstracted internal layers of this technology.

This journey is an intentional journey of literally reinventing the wheel. While reading my journey in the documentation, you will see the details of how Large Language Models work, through the example of the Llama model.

I will be happy if you check out it and comments are welcome!",MachineLearning,23,3,1724076102.0,1ew2l3i,adalkiran,https://www.reddit.com/r/MachineLearning/comments/1ew2l3i/p_dive_into_transformers_and_llm_world_llama_31/,Project
"Cuda advanced learning materials, [D]","I am searching for cuda advanced learning materials that are above beginnr lvl, I already did the nvidia's course named as Introductions to cuda in c++ , but that doesn't felt enough to let me get advanced tips and tricks and patterns. 
Recommend any books or any materials learning. 
Will be much helpful for me , thankss ",MachineLearning,23,9,1719120875.0,1dmf24l,M-notgivingup,https://www.reddit.com/r/MachineLearning/comments/1dmf24l/cuda_advanced_learning_materials_d/,Discussion
[D] 1D CNN on Waveforms and Spectrograms vs. 2D CNN Performance,"It's counter-intuitive that most successful audio frameworks are using 2-dimensional convolutional neural networks (CNN), so I have tried to experiment while trying to train on [BirdCLEF-2024 on Kaggle](https://www.kaggle.com/competitions/birdclef-2024) using simple frameworks, and I have questions regarding learning:

1. When learning waveform input, why 1D CNN does not converge and even diverge immediately on validation split?
2. When training on spectrogram magnitude (stft -> abs -> log1p), why 1D CNN performs worse than 2D CNN?
3. While it seems that spectrogram loses phase offset information when taking magnitude, it performs better than raw waveform. So, do humans/animals have `torch.stft` in their ear for better perception? For example, children can understand ""never seen"" high-pitched Mickey Mouse speech.",MachineLearning,24,17,1718507926.0,1dgyjum,ivanstepanovftw,https://www.reddit.com/r/MachineLearning/comments/1dgyjum/d_1d_cnn_on_waveforms_and_spectrograms_vs_2d_cnn/,Discussion
Perception of TMLR and other new/niche venues in Academia [D] ,"I'm going to be applying to Grad School this winter and so far I've only managed to get one paper in Theoretical Deep Learning, and it got into Transactions of Machine Learning Research (TMLR).I don't want to go into why I published there but essentially, my advisor pushed us to publish there. Although the review was very relaxed and insightful, I'm worried that the perception of (Theory) Academia towards TMLR is not that great. I have talked to a few professors and they all seem to ask me why I published in such a strange venue and I'm really worried about the GradApp committee having some doubts about work. The work by itself is cute, not something phenomenal but solved one of the questions asked in an older paper by some prominent authors and I'm quite proud of my work but I feel like the committee might not see it that way. I would love to know what Full-time academics think about venues like TMLR. ",MachineLearning,24,17,1718086160.0,1dd76xy,filletedforeskin,https://www.reddit.com/r/MachineLearning/comments/1dd76xy/perception_of_tmlr_and_other_newniche_venues_in/,Discussion
[D] Vector Neural Networks (VNNs) – Enhancing Geometric Deep Learning with 2D Vector Neurons and Geometric Tensors,"# Hello r/MachineLearning community!

I’m here to discuss a neural network architecture I’ve been working on: the **Vector Neural Network (VNN)**. This architecture enhances neural network capabilities by incorporating 2D vector operations directly into the learning process.

# Motivation

Traditional neural networks often struggle with tasks requiring rotational invariance and understanding spatial relationships. By integrating vector operations, VNNs aim to address these limitations, offering a more natural way to process and learn from geometric data.

# Key Concepts and Innovations:

1. **Geometric Tensors**:
   * The VNN uses matrices of vectors, with magnitudes on the left half and angles (in radians) on the right half. This allows the network to work directly with vector representations.
2. **Vector Decomposition**:
   * The core operation is based on the formula I+O=(A+B)×W:
      * **I**: Input vector
      * **O**: Learned offset vector
      * **A**: Learned basis vector
      * **B**: Calculated vector that cancels out the effect of A
      * **W**: Learned scalar weight
   * **Explanation**: The formula decomposes the input vector I and adds a learned offset vector O. This sum is expressed as a weighted combination (W) of a learned basis vector (A) and a calculated vector (B), where B is derived to neutralize A's influence, mirroring concepts like force cancellation in physics. It is used to decompose each vector into ten different vectors to improve the learning capacity of the network.
3. **Vector-Based Matrix Multiplication**:
   * Instead of traditional scalar multiplication, this operation finds the difference vector between two vectors and weights it by a learned weight. This approach is used to perform vector-based matrix multiplications and element-wise matrix additions.
4. **Element-Wise Weighted Add Operation**:
   * This operation sums each pair of 2D vectors element-wise, weighting the magnitude by a scalar. It allows for fine-tuning the contributions of different vectors, providing flexibility in how vector magnitudes influence the network's output.
5. **Paired Probabilities and Vector Attention**:
   * The network uses paired probabilities (from a dense layer) to scale and rotate vectors in their polar form. This guides the vectors towards angles that benefit the task at hand through a vector attention mechanism.
6. **Output and Classification**:
   * The network creates a long vector by summing all of the vector components from the results of vector attention operations. This vector is then used for binary or multi-class classification. An arc-length Euclidean distance loss ensures that the resultant vector points accurately in the target direction.

# Applications and Results:

1. **Obstacle Avoidance**:
   * **Example**: In robotics, the VNN can process sensor data as vector inputs to navigate around obstacles effectively.
   * **Result**: Improved pathfinding efficiency and robustness in dynamic environments.
2. **Augmented Reality (AR) Object Recognition**:
   * **Example**: The VNN can handle 3D spatial data, recognizing and manipulating objects in AR environments by encoding positions and orientations as vectors.
   * **Result**: Enhanced object tracking and interaction within AR applications.
3. **Optical Character Recognition (OCR)**:
   * **Example**: The VNN shows rapid convergence and sustained learning capabilities, performing well on OCR tasks with minimal training data.
   * **Result**: High accuracy in recognizing characters from diverse fonts and handwritten texts with minimal examples.

# Introducing the Expanding VGRU:

I'm also building a vector neural network called an **expanding VGRU**. Here’s a brief overview:

* **Input Structure**: It starts with one 11x11 tile as input, including associated weights and gradients.
* **Directional Expansion**: For each time step, it concatenates an additional tile in a certain direction (up, left, down, or right). The direction is chosen based on which one results in the lowest loss and makes the best prediction for the subsequent time step.
* **Dynamic Growth**: The network can expand from 11x11 to 11x22, then 22x22, 22x33, etc., with each time step.
* **Forward and Backward Pass**: For each time step, the forward pass is always executed, but the backward pass is performed only if it results in the lowest loss. It uses truncated BPTT during the backward pass.
* **Spatial Optimization**: The VGRU optimizes the spatial configuration of tiles as it progresses through each time step based on the input received.
* **Predictive Capabilities**: The network processes the previous hidden state and the current input. It tends to fit certain types of input to particular expansion paths. Then, once it fits one path and is exposed to another type of input, it naturally fits that type of input to a different path, making it capable of predicting diverse patterns without overfitting.

# Observations:

* **Pattern Prediction**: Averaging predictions for each direction at the last time step closely aligns with the target vector.
* **Spatial Bucketing**: The network buckets specific sequences into different locations on the grid, with low predicted values toward one side and higher predicted values toward another side.
* **Parameter Efficiency**: The network uses fewer parameters when moving up or down compared to moving left or right due to the increase in columns and the resultant parameter usage. This means that the network uses implicit architecture search via reinforcement learning.

# Advantages:

* **Rapid Convergence**: VNNs show signs of fast learning and sustained performance improvements, making them efficient even with limited data.
* **Geometric Bias**: The inherent geometric transformations make VNNs suitable for tasks requiring rotational invariance or spatial data understanding.
* **Expressive Power**: By directly working with vectors, VNNs can encode complex geometric relationships that traditional scalar networks might struggle with.

# Future Directions:

1. **Hierarchical Vector Decomposition**:
   * **Idea**: Exploring recursive decomposition to handle different levels of abstraction.
   * **Potential**: Enhances the network's ability to learn complex hierarchical relationships in data.
2. **3D VNNs**:
   * **Idea**: Expanding the architecture to three dimensions for applications in augmented reality and physics simulations.
   * **Potential**: Extends the benefits of VNNs to tasks requiring 3D spatial understanding.
3. **Vectorized Neural Network Models**:
   * **Idea**: Adapting concepts to other models like VLSTM, VCNN, VGAN, etc., to leverage vector operations across various neural network types.
   * **Potential**: Opens new avenues for integrating geometric learning into diverse neural network architectures.

# Let me know if you have any feedback, thoughts, or questions on any topic covered here or want to discuss potential applications and improvements.

For an in-depth look into VNNs, I recommend the book I published, 'Vector Neural Networks: With Geometric Tensors'. Also, an important resource is the seminal paper on vector neurons, 'Vector Neurons: A General Framework for SO(3)-Equivariant Networks'. A running example of the VGRU can be found on Github by searching for ParallelReverseAutoDiff.

",MachineLearning,24,55,1717465909.0,1d7lpfu,sjseto0519,https://www.reddit.com/r/MachineLearning/comments/1d7lpfu/d_vector_neural_networks_vnns_enhancing_geometric/,Discussion
[D] Why is R^2 so crazy?,"&#x200B;

https://preview.redd.it/jpiyt4b9yhwc1.png?width=1165&format=png&auto=webp&s=95d80f8f9c9241d722717ad25215be4077d541ca

Based on the MSE looks good right? But why is my R\^2 starting off so negative and approaching 0? Could it be a bug in how i am calculating it?   


This happened after i min maxed the labels before training.

This is an LSTM that is predicting runs scored for baseball games.",MachineLearning,24,43,1713994815.0,1ccagwc,Cloverdover1,https://www.reddit.com/r/MachineLearning/comments/1ccagwc/d_why_is_r2_so_crazy/,Discussion
[Discussion] SOTA for implicit feedback in recommender systems,"What are industry standards and the newest advancements in terms of handling lots of implicit observations for the purpose of recommending content/financial instruments etc?

From what I could research there are a couple important papers on this topic (excluding more well known algorithms like SVD++):

Spotify:  
[Logistic Matrix Factorization for Implicit Feedback Data](https://stanford.edu/~rezab/nips2014workshop/submits/logmat.pdf)

AT&T  
[Collaborative Filtering for Implicit Feedback Datasets](http://yifanhu.net/PUB/cf.pdf)

I would be interested to know if there are other approaches that perform well on i.e. the Netflix benchmark (when only taking 1 if there is a rating, else 0 and not the rating itself).

",MachineLearning,23,4,1735125346.0,1hlyy4i,Illustrious-Bag2386,https://www.reddit.com/r/MachineLearning/comments/1hlyy4i/discussion_sota_for_implicit_feedback_in/,Discussion
"[Project] Matrix Recurrent States, a Attention Alternative","[https://github.com/mikayahlevi/mru-lm](https://github.com/mikayahlevi/mru-lm)  
Hi, I'm posting here to share a project I just published on GitHub. I'll start with a description, some of which will be copy/pasted from the GitHub repo.  
The idea of a matrix recurrent unit is dictated by the update rule H\_t = H\_{t-1} X\_{t-1} and H\_1 = X\_1 where X and H are s×n×n sequences of square matrices. The primary difference between this and a traditional RNN is that no initial vector is passed through the linears, instead the first state is a matrix, leading to the output also being a matrix. My motivation for coming up with this idea are based on the following reasons:

* Matrix multiplication is associative but not commutative. The associativity means I can compute the cumulative matrix product using an (inclusive) parallel scan. The lack of commutativity means that the order of tokens is automatically incorporated into the MRU.
* When you try to do this scan on an traditional RNN, the number of operations scales cubically with the amount of elements in the output state, meaning that limited information is retained compared to the amount of computation. On the other hand, if the states are matrices, the number of operations as a function of elements in the output state is (n\^2)\^(3/2), where n\^2 is the number of elements in the square n×n matrix state. Here's a paper including some information about this: https://arxiv.org/abs/1709.04057.
* When processing the tokens sequentially or in parallel with the (not-yet implemented) Brent-Kung parallel scan the network scales linearly with time, in contrast to attention which scales quadratically with time.

I tried generating matrix X by different methods in the different branches. All of the ways to generate X and fold the output hidden state back into a vector, are arbitrary combinations of linears and reshapes and just based on what I found worked well.

[Loss vs Steps for a Transformer and an MRU-LM on shakespeare-char](https://preview.redd.it/6zyiw7g9vu6e1.png?width=640&format=png&auto=webp&s=34fd01cc5bacde21148fd324203aa40e7c977bf7)

This approach seems to work pretty well based on the toy dataset shakespeare-char. I would appreciate if anyone can help me train the model on larger datasets and further evaluate it. ",MachineLearning,22,9,1734200340.0,1he8vhw,IonizedPro,https://www.reddit.com/r/MachineLearning/comments/1he8vhw/project_matrix_recurrent_states_a_attention/,Project
[R] Evaluating the world model implicit in a generative model,,MachineLearning,23,26,1733915946.0,1hbra2d,jsonathan,https://arxiv.org/pdf/2406.03689,Research
[D] Daily Paper Discussions - FlashAttention 3,"As a part of daily paper discussions on the Yannic Kilcher discord server, I will be volunteering to lead the analysis of FlashAttention-3 🧮 🔍

📜 **FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision**  
🌐 [https://arxiv.org/abs/2407.08608](https://arxiv.org/abs/2407.08608)  
🕰 Thursday, Dec 5th, 2024 01:30 AM UTC // Thursday, Dec 5th, 2024 7.00 AM IST // Wednesday, Dec 4th, 2024 5:30 PM PT

**FlashAttention-3** introduces three smart ideas to boost performance on the Hopper GPUs -

1️⃣ Producer-Consumer Asynchrony: This technique divides tasks into separate parts. As an example, if we have 2 warpgroups (labeled 1 and 2 – each warpgroup is a group of 4 warps), we can use synchronization barriers (bar.sync) so that warpgroup 1 first does its GEMMs (e.g., GEMM1 of one iteration and GEMM0 of the next iteration), and then warpgroup 2 does its GEMMs while warpgroup 1 does its softmax, and so on. By doing this, it makes better use of GPU resources and hides delays that would otherwise slow down performance.

2️⃣ Hiding Softmax Operations: FlashAttention-3 improves efficiency by overlapping the slower softmax calculations with the faster matrix multiplications (GEMM). Instead of waiting for Softmax to finish before starting the next calculations, it processes them in parallel, speeding up the overall process.

3️⃣ Hardware-Accelerated Low-Precision Computations: This approach uses advanced GPU features to perform calculations with lower precision (FP8), which are faster and use less memory. FlashAttention-3 tweaks its algorithms to handle these low-precision calculations effectively, nearly doubling the processing speed while maintaining accuracy.

https://preview.redd.it/impb6wfc1w4e1.png?width=1063&format=png&auto=webp&s=82e24c828b373175ee119070027495a8a2a7bb6a

",MachineLearning,23,6,1733342581.0,1h6pmvd,CATALUNA84,https://www.reddit.com/r/MachineLearning/comments/1h6pmvd/d_daily_paper_discussions_flashattention_3/,Discussion
[R] Fast Matrix-Based Counterfactual Regret Minimization Using GPU Parallelization,"A novel GPU implementation of Counterfactual Regret Minimization (CFR) that accelerates the computation of optimal strategies in extensive-form games. The core innovation is parallelizing the regret updates and strategy computations across GPU cores while carefully managing memory access patterns.

Key technical points:
- Custom memory layout that maps game states and actions to GPU threads
- Batch processing of information sets to maximize GPU utilization
- Parallel computation of counterfactual values and regret updates
- Multi-GPU scaling through game tree partitioning
- Evaluated on Leduc Hold'em and Limit Texas Hold'em poker variants

Results:
- Up to 30x speedup compared to CPU implementation
- Linear scaling with number of GPUs up to 8 devices
- Memory usage scales with game size and number of information sets
- Solution quality matches CPU baseline within statistical error
- Successfully solved games with up to 10^14 states

I think this work could make CFR much more practical for real-world applications beyond poker. The ability to solve larger games faster opens up possibilities in areas like automated negotiation, security games, and resource allocation. The multi-GPU scaling is particularly interesting as it suggests potential for solving even more complex games.

The memory optimization techniques developed here might also transfer well to other game-theoretic algorithms that need to process large state spaces efficiently.

TLDR: GPU-accelerated CFR implementation achieves 30x speedup through careful parallelization and memory management, with linear multi-GPU scaling. Makes solving large extensive-form games significantly more tractable.

[Full summary is here](https://aimodels.fyi/papers/arxiv/gpu-accelerated-counterfactual-regret-minimization). Paper [here](https://arxiv.org/abs/2408.14778).",MachineLearning,24,1,1732802914.0,1h1wq6b,Successful-Western27,https://www.reddit.com/r/MachineLearning/comments/1h1wq6b/r_fast_matrixbased_counterfactual_regret/,Research
[R] Performance Analysis of GPU Interconnect Technologies Across Three Modern Supercomputer Architectures,"I found this study examining GPU-to-GPU communication in supercomputer systems to be quite informative. The key contribution is a systematic analysis of different interconnect technologies and their impact on multi-GPU performance.

The main technical findings include:

- NVLink provides highest bandwidth (up to 900 GB/s bidirectional) but comes with higher latency overhead
- InfiniBand shows lower latency (1-2μs) but reduced bandwidth compared to NVLink
- PCIe demonstrates consistent but lower performance metrics across all tests
- Topology and physical GPU arrangement significantly impact communication patterns

Some key methodology points:
- Tested multiple hardware configurations with 4-16 GPUs
- Measured bandwidth, latency, and completion time for standard communication patterns
- Analyzed impact of different data sizes and communication patterns
- Compared theoretical vs achieved bandwidth across interconnects

The practical implications I think are...

- Optimal interconnect choice depends heavily on workload characteristics
- Large model training benefits more from high bandwidth (NVLink)
- Distributed inference may prefer lower latency solutions (InfiniBand)
- Physical GPU topology should match common communication patterns

**TLDR**: Comprehensive analysis of GPU interconnect performance showing tradeoffs between bandwidth, latency, and topology. Results suggest workload-specific optimization of interconnect choice is crucial for multi-GPU systems.

[Full summary is here](https://aimodels.fyi/papers/arxiv/exploring-gpu-to-gpu-communication-insights-into). Paper [here](https://arxiv.org/abs/2408.14090).",MachineLearning,23,1,1731945020.0,1gu7swl,Successful-Western27,https://www.reddit.com/r/MachineLearning/comments/1gu7swl/r_performance_analysis_of_gpu_interconnect/,Research
"[R] Jay McClelland explains Parallel Distributed Processing, how the brain works, Hebbian learning, and backpropagation","Jay McClelland is a pioneer in the field of artificial intelligence and is a cognitive psychologist and professor at Stanford University in the psychology, linguistics, and computer science departments. Together with David Rumelhart, Jay published the two volume work Parallel Distributed Processing, which has led to the flourishing of the connectionist approach to understanding cognition.

In this conversation, Jay gives us a crash course in how neurons and biological brains work. This sets the stage for how psychologists such as Jay, David Rumelhart, and Geoffrey Hinton historically approached the development of models of cognition and ultimately artificial intelligence. We also discuss alternative approaches to neural computation such as symbolic and neuroscientific ones and the development of backpropagation.

https://preview.redd.it/s7xv0pmk2xzd1.png?width=1920&format=png&auto=webp&s=2e5be31c51a8eb78bf7033d1def25fa29f0863af

https://preview.redd.it/h4sqjoim2xzd1.png?width=1080&format=png&auto=webp&s=e7c952d579322379c67a77adadf1d392afe8d3c6

Youtube:  
[https://www.youtube.com/watch?v=yQbJNEhgYUw&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO&index=1&pp=iAQB](https://www.youtube.com/watch?v=yQbJNEhgYUw&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO&index=1&pp=iAQB)

Spotify: [https://open.spotify.com/show/1X5asAByNhNr996ZsGGICG](https://open.spotify.com/show/1X5asAByNhNr996ZsGGICG)

RSS: [https://feed.podbean.com/cartesiancafe/feed.xml](https://feed.podbean.com/cartesiancafe/feed.xml)",MachineLearning,25,1,1731175871.0,1gng4fy,IamTimNguyen,https://www.reddit.com/r/MachineLearning/comments/1gng4fy/r_jay_mcclelland_explains_parallel_distributed/,Research
[D] Evolving Matrix Computation Techniques for Modern AI: What's New?,"As AI models continue to scale in both complexity and size, I'm interested in how the field of matrix computations is evolving to meet these new challenges. What are some of the latest advancements or strategies in matrix computation that are improving efficiency and adaptability for modern AI systems? Are there any recent breakthroughs or shifts in our approach to these computations that are making a significant impact in AI research and applications?",MachineLearning,22,11,1730893895.0,1gkwpht,Glittering_Age7553,https://www.reddit.com/r/MachineLearning/comments/1gkwpht/d_evolving_matrix_computation_techniques_for/,Discussion
[D] Train on full dataset after cross-validation? Semantic segmentation,"I am currently working on a semantic segmentation project of oat leaf disease symptoms. The dataset is quite small, 16 images. Due to time constraints, I won't be able to extend this.

I am currently training 3 models, 3 backbones, and 3 losses--using 5-fold cross validation and grid search.

Once this is done, I plan to then run cross validation on a few different levels of augmentations per image.

My question is this:

Once I have established the best model, backbone, loss, and augmentation combination, can I train on the full dataset since it is so small? If I can do this, how do I know when to stop training to prevent overfitting but still adequately learn the data?

I have attached an image of some results so far.

https://preview.redd.it/sx394c58l5xd1.png?width=2000&format=png&auto=webp&s=3cefbf5c84bf3fbf48936c47810c4e3039dcb410

Thanks for any help you can provide!",MachineLearning,24,30,1729971489.0,1gct22r,Entire_Commission169,https://www.reddit.com/r/MachineLearning/comments/1gct22r/d_train_on_full_dataset_after_crossvalidation/,Discussion
Top conferences for AI in medical imag-ing [D],"Sorry for imag-ing in title, title can't have 'AGI'.

I'm working on my first first-author research and my advisor feels it's going a good direction. I really want it to go through some good conferences by next year.

I know about MICCAI and MIDL but can't find a reliable source to check for all other conferences in 2025 related to medical imaging or AI in medicine in general. I hope people here must have some experience with few others. Any suggestions?

Also, what does workshop paper mean? I know it's not called a actual publication but is it worth submitting to a highly regarded workshop or rather a mid-ranked conference?

Thanks in advance!",MachineLearning,24,28,1729233465.0,1g6bu6z,ade17_in,https://www.reddit.com/r/MachineLearning/comments/1g6bu6z/top_conferences_for_ai_in_medical_imaging_d/,Discussion
[D] What qualifies as a sensitive attribute in equity and fairness research?,"Hi, long time lurker and first time poster. Basically the title.

Over the last few years of MICCAI (International Conference on Medical Image Computing and Computer-Assisted Intervention; the premier conference for medical image analysis research) I have noticed a worrying trend in health equity and fairness research: authors claiming unexpected attributes of datasets as ""sensitive attributes"" in their analysis and modeling:

* \[MICCAI 2022\] [FairPrune: Achieving Fairness Through Pruning for Dermatological Disease Diagnosis](https://conferences.miccai.org/2022/papers/207-Paper2316.html) (click on the **SharedIt link** under **Link to paper** for the paper's PDF): **Early Accept** (early accept is a paper that received all accept decisions and did not have a rebuttal phase).
   * The authors use dermatoscopic (aka dermoscopic) images from [the ISIC 2019 dataset](https://www.kaggle.com/datasets/andrewmvd/isic-2019) and use the gender label as the ""sensitive attribute"" with the female group as the ""privileged group"", simply because ""*The vanilla trained model has a higher accuracy on the female images*.""
* \[MICCAI 2023\] [Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis](https://conferences.miccai.org/2023/papers/653-Paper0994.html)
   * This paper shares one author with the previous paper, and they make the same assumption: ""*we take gender as our sensitive attribute*"" and ""*The female is the privileged group with higher accuracy by vanilla training.*""
* \[MICCAI 2024\] [BiasPruner: Debiased Continual Learning for Medical Image Classification](https://papers.miccai.org/miccai-2024/099-Paper2799.html): **Best Paper Award Nominee**.
   * Patient age is used as the sensitive attribute (age≥60, age<60) in dermatoscopic images from the HAM10000 dataset. Different set of authors.

# Why are gender and age strange choices?

Dermatoscopic images are acquired using a [dermatoscope](https://en.wikipedia.org/wiki/Dermatoscopy) and have extremely low FOV. For sample images, see this imgur link: [https://imgur.com/a/ggNOPj2](https://imgur.com/a/ggNOPj2). As you can see, it **should be** impossible for these low FOV images to contain any age or gender specific information that may be the source of bias. It would be understandable if age and gender were considered bias sources for CXRs (chest x-rays), or skin tone for skin images, but age and gender are not even reflected in these dermatoscopic images. How can they be sources of bias? And why an age threshold of 60 years?

Are these simply instances of [HARKing](https://en.m.wikipedia.org/wiki/HARKing), since there is no literature (to the best of my knowledge) that says age and gender are reflected in dermatoscopic images? I also fear that if a similar analysis was carried out using any other unrelated metadata as the sensitive attribute (e.g., skin tone in CXRs), it is possible that performance may improve, but that doesn't mean that CXRs have skin-tone related bias.

Please help me understand how these sensitive attributes are chosen. Thank you in advance.",MachineLearning,22,13,1729043531.0,1g4oef0,thrownicecatch,https://www.reddit.com/r/MachineLearning/comments/1g4oef0/d_what_qualifies_as_a_sensitive_attribute_in/,Discussion
[R] LongCite: Enabling LLMs to Generate Fine-Grained Citations in Long-Context QA,"I just read an interesting paper that aims to address (or improve) information retrieval with fine-grained citations, ""LongCite: Enabling LLMs to Generate Fine-Grained Citations in Long-Context QA"" (https://arxiv.org/abs/2409.02897).

In this paper, the researchers use off-the-shelf LLMs to generate a dataset consisting of long-context QA instances with precise sentence-level citations and then use that dataset to finetune an open-weight LLM to generate answers with citations. The resulting LongCite 8B and 9B models are surprisingly good compared to GPT4o, Llama 3.1, etc.

How does this work? Here is the 4-step procedure for generating the dataset for instruction-finetuning:

(a) Starting with long texts or documents, their method uses an existing LLM to generate a Q&A dataset (a query and its associated answer) using Self-Instruct (Wang et al. 2023; discussed in one of my previous posts).

(b) Next, they use the answer to retrieve several 128-token chunks from the input text for coarse-grained citations.

(c) The LLM then looks for relevant sentences within these chunks to provide more fine-grained sentence-level citations

(d) The researchers filter out Q&A pairs where less than 20% of the statements in the answer don't have citations

The resulting dataset is then used to train an LLM in a conventional (SFT) fashion.

https://preview.redd.it/gucywp0o8jud1.jpg?width=6000&format=pjpg&auto=webp&s=4ed2ff078327082133d390c566250a2ef41c1a05

  
",MachineLearning,23,1,1728829153.0,1g2qohf,seraschka,https://www.reddit.com/r/MachineLearning/comments/1g2qohf/r_longcite_enabling_llms_to_generate_finegrained/,Research
"[P] Paper Central, first portal to bring together all key sources in one place, including arXiv, Hugging Face paper pages, GitHub, and conference proceedings.","Hugging Face launched Paper Central today, the most up-to-date information on the latest research papers.

app: [https://huggingface.co/spaces/huggingface/paper-central](https://huggingface.co/spaces/huggingface/paper-central)

post: [https://x.com/IAMJBDEL/status/1841627341195510256](https://x.com/IAMJBDEL/status/1841627341195510256)",MachineLearning,24,7,1727924048.0,1fuy2qk,Illustrious_Row_9971,https://www.reddit.com/r/MachineLearning/comments/1fuy2qk/p_paper_central_first_portal_to_bring_together/,Project
"[D] How much knowledge of differential equations is required to understand works on flows, diffusion SciML and related fields?","I was of the impression that basic calculus, probability, statistics and linear algebra builds a solid foundation to understand works in deep learning. But seeing the recent papers on flow matching, normalising flow, diffusion and in the domains of scientific machine learning, I can't understand a thing beyond a certain point. 

I understand that they make heavy use of differential equations. I have almost below novice level knowledge in that domain. Where to learn more about differential equations? I want to gain the ability to understand the works in these domains and how and why the authors of a paper came up with that implementation.",MachineLearning,24,12,1727247937.0,1foygx0,HopeIsGold,https://www.reddit.com/r/MachineLearning/comments/1foygx0/d_how_much_knowledge_of_differential_equations_is/,Discussion
Discovering a Pitfall in Cross-Entropy Loss for Large Vocabularies. [R],"In this short publication, I uncover a significant issue with using cross-entropy loss in models with large vocabularies, which can lead to performance degradation in fine-tuned LLMs. I provide both theoretical insights and empirical results to back up these findings. If you’re working with large vocabularies, this is a must-read: [Unveiling a Pitfall in Cross-Entropy Loss for Large Vocabularies | by Oswaldo Ludwig | Aug, 2024 | Medium](https://medium.com/@oswaldoludwig/unveiling-a-pitfall-in-cross-entropy-loss-for-large-vocabularies-ef388d846bc1)",MachineLearning,23,11,1727122127.0,1fnu24s,Gold-Plum-1436,https://www.reddit.com/r/MachineLearning/comments/1fnu24s/discovering_a_pitfall_in_crossentropy_loss_for/,Research
[D]What are the top 3 countries in development of AI model besides America and China?,"Undoubtedly, the United States and China are leading in the development of Large Language Models. How are other countries performing?",MachineLearning,26,95,1726956594.0,1fme9af,Realistic-Ad-6231,https://www.reddit.com/r/MachineLearning/comments/1fme9af/dwhat_are_the_top_3_countries_in_development_of/,Discussion
[P]Building a Toy Neural Network Framework from Scratch in Pure Python – Inspired by Karpathy’s Micrograd,"[https://github.com/ickma/picograd](https://github.com/ickma/picograd)

Last weekend, I started a project to build a toy neural network framework entirely from scratch using only pure Python—no TensorFlow, PyTorch, or other libraries. The idea for this project came from Andrej Karpathy’s [micrograd](https://github.com/karpathy/micrograd), and I wanted to challenge myself to really understand how neural networks work under the hood.



I implemented both forward and backward propagation, and after some testing, I managed to achieve 93% accuracy on the Iris classification dataset.



This project serves as a good learning tool to explore the internals of neural networks, such as how weights and biases are updated during training and how different layers communicate during forward and backward passes. If you’re looking to dive deeper into the mechanics of neural networks without relying on existing frameworks, this might be helpful to you as well.



I Feel free to ask questions or share any feedback!

https://preview.redd.it/jwaltnn6aqpd1.png?width=846&format=png&auto=webp&s=3eb14eacf57fd323ac2eeb75b614ddb5f27bf8a2

",MachineLearning,26,7,1726735257.0,1fkg3yd,Potential-Dingo-6424,https://www.reddit.com/r/MachineLearning/comments/1fkg3yd/pbuilding_a_toy_neural_network_framework_from/,Project
"[D] What are the best open source, fine-tunable, large context, encoder-decoder models today?","I'm looking for model recommendation to fine-tune for a translation task. 

The input sequence pairs are pretty long, up to 1MB each, although the data set can be truncated to only contain \~200kB sequences. The sequences are program code (basically transpiling) but my intuition is that I would still benefit from a base model trained on natural language since it captures some basic general knowledge that improves performance.

I also would like to train the same model architecture from scratch and compare the performance with the fine-tuned version to make this point.

Criteria for the model:

* open license for research (not necessarily for commercial purposes but it's a plus)
* transformer-based with encoder/decoder legs
* long context length in the hundreds of thousands of tokens
* ideally inference can run on a newer Mx chip MacBook (not a must-have)
* ideally a newer, more state-of-the-art model (not a must-have)
* ideally available in Huggingface (not a must-have)

Regrettably anything based on BERT (e.g. DistilBERT) would not have a large enough context window. I've been looking at XLNet and Longformer that fit this criteria. Both seem to fit the bill more or less but I'd like to explore all the options.

Thank you so much!",MachineLearning,23,15,1725390317.0,1f88bmp,huopak,https://www.reddit.com/r/MachineLearning/comments/1f88bmp/d_what_are_the_best_open_source_finetunable_large/,Discussion
[N][R] Releasing Re-LAION 5B: transparent iteration on LAION-5B with additional safety fixes,,MachineLearning,23,4,1725113000.0,1f5o0kj,Jamais_Vu206,https://laion.ai/blog/relaion-5b/,News
[D] - KANs (Kolmogorov-Arnold Networks) have a future?,"Hi everyone, 

I recently saw that Jamba-1.5 has been released. 

[https://huggingface.co/collections/ai21labs/jamba-15-66c44befa474a917fcf55251?utm\_source=tldrai](https://huggingface.co/collections/ai21labs/jamba-15-66c44befa474a917fcf55251?utm_source=tldrai)

In general, I saw that several articles have come out about Mamba and derivatives. The interest in space state model has not faded. SSMs have managed to carve out a niche for themselves.  For example, in bioinformatics there is a particular interest in their ability to model long sequences. I've seen various works dealing with using them for DNA sequences, and several colleagues have shown interest in testing them

Over time, though, we have seen a number of models that seem to be competing with the transformer (RWKV, Hyena Hierarchy, xLSTM, and so on)

[https://arxiv.org/abs/2305.13048](https://arxiv.org/abs/2305.13048)

[https://arxiv.org/abs/2302.10866](https://arxiv.org/abs/2302.10866)

[https://arxiv.org/abs/2405.04517](https://arxiv.org/abs/2405.04517)

For all of these other architectures, interest seems to be waning, yet there was a lot of hype in the beginning. 

Now, I found the Kolmogorov-Arnold Networks particularly intriguing.

[https://arxiv.org/abs/2404.19756](https://arxiv.org/abs/2404.19756)

I wonder if the hype is over here too and they will end up in oblivion, or are there perhaps cases where they will find applications. Maybe niches where they can establish themselves, or if they too will remain more as an interesting historical experiment than something that is adapted by the community.",MachineLearning,24,20,1724832217.0,1f34nou,NoIdeaAbaout,https://www.reddit.com/r/MachineLearning/comments/1f34nou/d_kans_kolmogorovarnold_networks_have_a_future/,Discussion
[R] TurboEdit: Instant text-based image editing,,MachineLearning,22,1,1724463205.0,1eztooi,AhmedMostafa16,https://arxiv.org/abs/2408.08332,Research
[R] Transformers are Universal In-context Learners,"This work explores the expressivity of transformers, focusing on their ability to handle an arbitrarily large number of context tokens. Notably, a single transformer can process an infinite number of tokens with a fixed embedding dimension and a constant number of heads. The use of MLP layers between attention layers is also carefully regulated",MachineLearning,25,1,1723439189.0,1eq5bkz,AhmedMostafa16,https://arxiv.org/abs/2408.01367,Research
[P] Vison Language Models from Scratch,,MachineLearning,22,1,1723366358.0,1epga39,themathstudent,https://sachinruk.github.io/blog/2024-08-11-vision-language-models.html,Project
[D] Diffusion vs Flow,Flux uses rectified flow for image generation. I notice meta also push out flow matching methods for audio generation. Does anyone have any intuition why these frontier lab are switching to flow base method instead of diffusion?,MachineLearning,24,16,1722844780.0,1eki8kn,Internal_War3919,https://www.reddit.com/r/MachineLearning/comments/1eki8kn/d_diffusion_vs_flow/,Discussion
[D] Segment Anything 2 Paper Breakdown,Sharing a video where I breakdown the key architectural innovations in Meta’s new SAM-2 video segmentation model! Enjoy!,MachineLearning,22,3,1722701877.0,1ej77fu,AvvYaa,https://youtu.be/wMGb97EZkVU,Discussion
[D] Any good fine-tunable TTS? (Not Coqui or TorToiSe),"Hello guys! I have a very emotional dataset of around 20-30 minutes of crystal clear recordings of an English speaking voice. I want to make a fine-tune model to fully replicate that voice and be able to inference it WITHOUT CUDA, just MPS. Any ideas besides Coqui and TorToiSe (these don’t work on high-pitched data) with a good step-by-step doc?",MachineLearning,24,35,1722557406.0,1ehw1nf,yukiarimo,https://www.reddit.com/r/MachineLearning/comments/1ehw1nf/d_any_good_finetunable_tts_not_coqui_or_tortoise/,Discussion
[D] What is the average size of datasets you work in your company projects? #D ,"Wanted to know how much size datasets you work with in your projects, is it in GBs or TBs?

and how do you manage to work with large datasets and what best practices you follow in doing so.",MachineLearning,24,43,1721548541.0,1e8h7ra,PraveenKumarIndia,https://www.reddit.com/r/MachineLearning/comments/1e8h7ra/d_what_is_the_average_size_of_datasets_you_work/,Discussion
"[D] What happened to ""creative"" decoding strategy?","For GPT-2 and most models at that time, the naive greedy decoding is extremely prone to generating repetitive and nonsensical outputs very fast, and many techniques, such as top-p sampling, nucleus sampling, repetition penalty, n-gram penalty, etc. are needed. (e.g. [https://arxiv.org/pdf/1904.09751](https://arxiv.org/pdf/1904.09751) )

For recent LLMs, I haven't been using any of these tricks, and instead, any temperature between 0 and 1 seems to work just fine. The only repetitive generation that I've observed seem to be in math reasoning, when the model wants to do some exhaustive search that didn't succeed. 

So are all these custom decoding strategies a thing of the past, and we don't need to worry about degenerate content generation anymore?  ",MachineLearning,22,12,1721068511.0,1e42das,zyl1024,https://www.reddit.com/r/MachineLearning/comments/1e42das/d_what_happened_to_creative_decoding_strategy/,Discussion
[R] Weight Rescaling: Applying Initialization Strategies During Training,,MachineLearning,21,6,1719159206.0,1dmpp29,rasten41,https://ecp.ep.liu.se/index.php/sais/article/view/1002/910,Research
[R] Scalable MatMul-free Language Modeling,,MachineLearning,23,7,1717900383.0,1dbjcbh,topcodemangler,https://arxiv.org/pdf/2406.02528,Research
[R] Extracting Concepts from GPT-4,"Similar to the recent [report by Anthropic](https://www.anthropic.com/research/mapping-mind-language-model), OpenAI released a report, some code, and a visualizer for the features extracted by an autoencoder from their model.

OpenAI blog post: [https://openai.com/index/extracting-concepts-from-gpt-4/](https://openai.com/index/extracting-concepts-from-gpt-4/)

Paper: [https://cdn.openai.com/papers/sparse-autoencoders.pdf](https://cdn.openai.com/papers/sparse-autoencoders.pdf)

Code: [https://github.com/openai/sparse\_autoencoder](https://github.com/openai/sparse_autoencoder)",MachineLearning,23,2,1717753027.0,1da6ml3,valdanylchuk,https://www.reddit.com/r/MachineLearning/comments/1da6ml3/r_extracting_concepts_from_gpt4/,Research
[D] Is multi-label classification the best approach in this case? Me and my manager seem to be in a headlock. ,"My company deals with device telemetry data from smart valve devices. It is basically a lot of digital data from different sensors and some analogue components for things like current and voltage. The data is huge, both in terms of features and samples. My manager basically wants to use machine learning to identify faults in the device, which consists of multiple sub-assemblies so there can be multiple faults in one sub assembly, or multiple faults in different sub-assemblies in one dataset.   
I have previously used XGBoost and Random Forest Classifier for identifying single problems for example switch failure only, because that was the focus at that time. I have also worked with LSTM on some failures. Now with this new goal to basically detect all faults with ML, my manager wants me to label and train models individually for all faults. So if there can be 50 types of faults, there will be 50 models. 

I feel like this is super inefficient, hard to maintain, and apart from that, I think it is ignoring the fact that one dataset can have two or three faults, and when training for one fault, labelling all others the same as you would label normal data might be misleading in some way. 

I want to go for a multilabel classifier. The constraint here is model choice, as I think it is hard to code multi-label wrappers for some models so we would have to rely on probably random forest to pick up all the faults. 

Can you guys give advice/tips/arguments for and against each side/general opinion on how to pick an approach and a model for such data?   
Do you think it would be possible to just have one model for like 1 TB of data and classifying all its faults through it? Or it would be better to follow my manager's approach which I feel like defeats the purpose of 'learning' part of machine learning. ",MachineLearning,24,18,1717685183.0,1d9jxt0,General_Working_3531,https://www.reddit.com/r/MachineLearning/comments/1d9jxt0/d_is_multilabel_classification_the_best_approach/,Discussion
[D] Library for named entity recognition,"Hi Guys, I'm needing to decide which library to use for named entity recognition. I've used spaCy, which works well, but I need a library that allows me to categorize entities and also sub-entities. Has anyone done something similar? I mean, where the same word can be more than one entity. spaCy offers the SpanCat pipeline, which theoretically allows this, but l've had trouble creating the training corpus. I think it's because they expect you to purchase an annotation text framework like Prodigy.",MachineLearning,21,9,1716043977.0,1cuz1i2,Original_Ad8019,https://www.reddit.com/r/MachineLearning/comments/1cuz1i2/d_library_for_named_entity_recognition/,Discussion
Non Technical ML Podcasts? [D],"Hey everyone. For context, I’m a recent CS graduate and current entry level Data Engineer, and I’ve always loved learning about ML models and techniques and how to implement, deploy, and scale them. I’m looking for a good podcast to keep my knowledge of ML trends up to date, but the challenge is that I don’t really like listening to podcasts that are technical as I am still a newbie and generally understand complexities better if I read them. I’ve tried some podcasts but most of the time the stuff goes over my head and I get lost. Looking for something I can listen to without having to think too hard on my way to work. Would love any suggestions!",MachineLearning,21,15,1715118744.0,1cmntyr,C-beenz,https://www.reddit.com/r/MachineLearning/comments/1cmntyr/non_technical_ml_podcasts_d/,Discussion
[D] How to handle varying Feature Dimensions in Graph Neural Networks training?,"I have a question about handling datasets with varying feature dimensions in Graph Neural Network training. For example, in one training instance (let's call it Dataset A), the node features have a dimension of 4, and the edge features have a dimension of 16. In another instance (Dataset B), the node features have a dimension of 5, and the edge features have a dimension of 25. Other datasets may have different feature dimensions as well.

What are the standard methods used to handle varying feature dimensions for each instance when training a GNN model with such datasets? I would appreciate any guidance or direction on how to approach this. Thanks!",MachineLearning,23,8,1733086655.0,1h4dbvi,bipulthapa,https://www.reddit.com/r/MachineLearning/comments/1h4dbvi/d_how_to_handle_varying_feature_dimensions_in/,Discussion
[D] Cerebras Inference Results for 405B,"Cerebras has just shared some very interesting results on LLM inference. I was first skeptical and thought maybe they used some large batch sizes or some trick to hit almost 1k tokens/s for llama 405B. I tested llama-70B on their website. It's really fast...

I've been reading up on their published paper, but there haven't shared any details on how they run a 405B  parameter model on this huge chip. They have 40GB SRAM, which is huge, but running a 405B model at such low latency and high throughput still sounds interesting. Their papers discuss weight streaming. I think they must have used some advanced data flow analyses to keep the compute busy from the off-chip memory where this huge can be stored.

Does anyone know where I can get more information on this?

Ref: [https://cerebras.ai/blog/llama-405b-inference](https://cerebras.ai/blog/llama-405b-inference)

Paper: [https://arxiv.org/abs/2409.00287](https://arxiv.org/abs/2409.00287)

White Paper: [https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10123162](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10123162)

Disclaimer: I have nothing to do with Cerebras systems, just genuinely interested and curious about this. This feels like a pretty big deal for AI in general.",MachineLearning,23,7,1732085941.0,1gvjrmo,JanGehlYacht,https://www.reddit.com/r/MachineLearning/comments/1gvjrmo/d_cerebras_inference_results_for_405b/,Discussion
[R] treemind: Simplifying Gradient Boosting Model Analysis,"`treemind` is a powerful Python library designed to analyze gradient boosting models like `xgboost`, `lightgbm`, and `catboost`. It helps you uncover how features and their interactions influence predictions across specific intervals, offering fast, intuitive insights.

### Key Features:
- **Feature & Interaction Analysis:** Understand feature contributions and complex interactions up to `n` features.
- **Advanced Visualizations:** User-friendly plots to explain model decisions.
- **High Performance:** Optimized with Cython for lightning-fast execution, even on large datasets.
- **Easy Integration:** Seamlessly works with popular frameworks for regression and binary classification.

### Algorithm & Performance:
- **Algorithm:** Focuses on analyzing feature contributions and interactions in tree-based models for meaningful interval-based insights. [Read more about the algorithm](https://treemind.readthedocs.io/en/latest/algorithm.html)
- **Performance:** The library's performance has been tested on synthetic datasets, where it is benchmarked against SHAP for accuracy and efficiency. [View performance experiments](https://treemind.readthedocs.io/en/latest/experiments/experiment_main.html)

### Quick Start:
```bash
pip install treemind
```

Check out the full documentation for examples, visualizations, and API details.

[GitHub Repo](https://github.com/sametcopur/treemind) | [Docs](https://treemind.readthedocs.io/)

**Note:**  
While the algorithm produces desirable results in practice, it currently lacks formal mathematical proof. We would greatly appreciate your feedback and ideas to help improve and validate the approach further!",MachineLearning,22,6,1731866743.0,1gtjkci,zedeleyici3401,https://www.reddit.com/r/MachineLearning/comments/1gtjkci/r_treemind_simplifying_gradient_boosting_model/,Research
[R] The geometry of data: the missing metric tensor and the Stein score,"Just [sharing an article](https://blog.christianperone.com/2024/11/the-geometry-of-data-part-ii/) for those interested in differential geometry, ML and score-based models. I made a long introduction and then later I show how you can derive an efficient to compute metric tensor for the data manifold using the Stein score alone.",MachineLearning,21,0,1731589504.0,1gr4bfl,perone,https://www.reddit.com/r/MachineLearning/comments/1gr4bfl/r_the_geometry_of_data_the_missing_metric_tensor/,Research
[D] [R] I am currently exploring a weird (?) ML sub area for my thesis and I think I am stun-locked at the scope of the problem.,"I'm working on my final year thesis for my uni, and I decided to tackle Reservoir Computing in a weird way. My inital goal was to enable critical phenomenon within a digital reservoir and use it as an emergent computational system.

For the model I am working on, here are the concepts that I have dove deep into for the past few months:

**Main Concept/s**

* *Reservoir Computing*: The main computational unit. A lattice based reservoir will be used in tandem with either single or multiple readout networks so that it acts as a multi-modal network.
* *Neuromorphic Computing* (?): The model was going to utilize Neuromorphic nodes only at first, but I decided for it to be an option within the model.

**Interpretability and Control**

* *Dynamical Systems*: I decided to tackle the problem as a dynamical systems problem. This is because the model evolves over time and I want to understand the trajectory of the evolution of the system.
* *Control Theory*: A bunch of control and order parameters will be set up to adjust the trajectories of the model's evolution.
* *Lyapunov Exponents* (?): I am debating whether I should explicitly find the Lyapunov functions within the phase space of the model because frankly, it's too hard for now. I really don't have too much of a solid grasp of the techniques involved yet.

**Self-Organization and Emergent Phenomena**

* *Phase Transitions*: I dove deep into phase transitions because interestingly, neural networks *apparently* exhibit this phenomena. Personally, I think there is a connection between the vanishing/exploding gradient problem and phase transitions within the network, although I haven't found literature on this yet.
* *Critical Phenomenon*: Information transfer is maximized within critical systems. This is an interesting property to utilize and maximize within neural networks I think.
* *Superradiance and Superradiant Quantum Effects*: This is a bit of a weird tangent concept. I came about it when I was doing quantum computing projects. I wanted oscillatory behavior within my system in order to synchronize the global state of the system. While I failed at my initial plan, I found superradiance, which is this weird quantum synchronization behavior that happens even in noisy large scale systems. I am still looking in ways to integrate this as a loss function for now.

**Implementation**

* *Cellular Automata*: The main implementation of the reservoir is basically a lattice matrix of weights. So it can be treated as a cellular automata.
* *Neural Cellular Automata (Convolutional)*: The system comprises of an weighted adjacency matrix and an output matrix. The inputs are passed through the adjacency matrix, summed up, and passed through an activation function.
* *Ising Model Topologies and Architectures*: The topology of the model is basically homeomorphic to a 2d ising model. This is to ensure that a 2nd order phase transition is possible.

**Interpretability and Control pt. 2**

* *Graph and Hypergraph Theory*: I can treat the cellular automaton reservoir as a graph/hypergraph of the nodes and their connections so I can do PCA on it. Pretty straightforward.
* *Hypergraph Projection Eigenvalue Analysis*: Related to phase transition analysis. The phase transition of a hypergraph can be studies by projecting the hyperedges onto an adjacency matrix. We then take the eigenvalues of the adjacency matrix. The eigenvalues must be stable for the system to be 'good'. In my case, we want all the eigenvalues to be negative and be close to zero (indicating quasi-critical behavior).

To be honest, I'm kind of way in over my head right now. I do have some basic toy examples for different parts of the model, but I am stuck on how to implement them together. And I am currently kind of at a loss in how to implement criticality and superradiance measures as a loss function. I am not a physicist by any means, so I am not really too knowledgable with the concepts needed for this model.

I'm willing to discuss about bits of knowledge that I lack, or any ideas on how to implement and train this model. I can also provide my references if anyone wants to. I don't know if this subreddit is the best place to post this, but I don't see any specialized ML subreddits lmao.",MachineLearning,23,18,1730960952.0,1glk9g0,Fr_kzd,https://www.reddit.com/r/MachineLearning/comments/1glk9g0/d_r_i_am_currently_exploring_a_weird_ml_sub_area/,Discussion
"[P] Benchmarking 1 Million Files from ImageNet into DVC, Git-LFS, and Oxen.ai for Open Source Dataset Collaboration","Hey all!

If you haven't seen the Oxen project yet, we have been building a fast [open source unstructured data version control tool](https://github.com/Oxen-AI/oxen-release) and platform to host the data ([https://oxen.ai](https://oxen.ai/)). It’s an alternative to dumping data on Hugging Face with git-lfs or their datasets library and goes together with their models like chocolate and peanut butter - Oxen can be used for iterating on and editing the data and Hugging Face for public models.

We were inspired by the idea of making large machine learning datasets living & breathing assets that people can collaborate on, rather than the static dumps. Lately we have been working hard on optimizing the underlying Merkle Trees and data structures with in [Oxen.ai](http://oxen.ai/) and just released v0.19.4 which provides a bunch of performance upgrades and stability to the internal APIs.

# 1 Million Files Benchmark

To put it all to the test, we decided to benchmark the tool on the 1 million+ images in the classic ImageNet dataset.

The TLDR is [Oxen.ai](http://oxen.ai/) is faster than raw uploads to S3, 13x faster than git-lfs, and 5x faster than DVC. The full breakdown can be found here 👇

[https://docs.oxen.ai/features/performance](https://docs.oxen.ai/features/performance)

If you are in the ML/AI community, or just data aficionados, would love to get your feedback on both the tool and the codebase. We would love some community contribution when it comes to different storage backends and integrations into other data tools.",MachineLearning,22,9,1730677422.0,1gj0si8,FallMindless3563,https://www.reddit.com/r/MachineLearning/comments/1gj0si8/p_benchmarking_1_million_files_from_imagenet_into/,Project
[R] Riemannian Generative Models,"Hi everyone,

I’m currently interested in exploring generative models defined over Riemannian manifolds. Though the idea is theoretically appealing, I have trouble understanding the practical motivation behind this approach, and whether any useful/large scale model has been developed lately based on it.

To be more precise, I am looking at the following set of papers.

Generalizing diffusion models to the Riemannian setting :

[Riemannian Diffusion Models](https://arxiv.org/abs/2208.07949), [Riemannian Score-Based Generative Modelling](https://arxiv.org/abs/2202.02763)

Scaling these models:

[Scaling Riemannian Diffusion Models](https://arxiv.org/abs/2310.20030)

I don’t understand how impactful the experimental results really are, and what the interest for these models are whether in the industry or in the research community. 

If anyone has any thoughts about the interrogations I have, I’d be happy to start a discussion here. I’d be extremely grateful for your insights! Thanks for any help",MachineLearning,24,7,1730298225.0,1gfnqkf,LostSleepyDreamer,https://www.reddit.com/r/MachineLearning/comments/1gfnqkf/r_riemannian_generative_models/,Research
[D] Feedback on ML Project: Using Transformers to improve Ant Colony Optimization Algorithm,"I'm currently working on a personal project trying to build an improved version of the ant colony optimization algorithm. 

I'm using a positional-encoded transformer neural network to predict optimal pheromone matrices before running the algorithm.

The Improved Ant Colony Optimization Algorithm is initialized with a pheromone matrix outputted by a positional-encoded transformer neural network that was trained on pheromone matrix data from the normal ant colony optimization algorithm.

To analyze the improvement of the algorithm, I'm having the improved ACO run with normal ACO run different map sizes for multiple iterations, calculating each algorithm's best runs, and calculating for a p-value to verify if the improved algorithm has statistical significance. 

So far, the enhanced ACO shows promising results with p-values of 0.06 and 0.05 for node sizes of 30 and 35, respectively. 

However, I'm aiming to achieve significance (p < 0.05) across a wider range of node sizes.

I would appreciate any feedback!

Project Link: [https://github.com/ronantakizawa/improvedaco/blob/main/ronan\_acotransformer\_experiment.ipynb](https://github.com/ronantakizawa/improvedaco/blob/main/ronan_acotransformer_experiment.ipynb)",MachineLearning,20,4,1728744123.0,1g21loo,None,https://www.reddit.com/r/MachineLearning/comments/1g21loo/d_feedback_on_ml_project_using_transformers_to/,Discussion
[R] SWE-bench Multimodal: Do AI Agents Generalize to Visual Software Domains?,"Hi!

We just put [SWE-bench Multimodal](https://www.swebench.com/multimodal) out!

It has 617 brand new task instances- in each of them, an agent is evaluated on its ability to fix a real bug from one of 17 JavaScript GitHub repositories. All of the bugs that we use in this dataset have an image in the issue text. And all of the repositories are user-facing libraries like mapping, plotting, and web UI libraries.

[Here are a few example bugs from SWE-bench Multimodal, with their associated images](https://preview.redd.it/4i2ybld59jtd1.png?width=2423&format=png&auto=webp&s=75c6bb47998861c69a36121ae7d9bbdfa36a0e58)

This dataset was \*super\* challenging for existing open source agents, including Agentless, AutoCodeRover, and Moatless. We made a new version of SWE-agent that's able to correctly solve 12% of the issues. Our new Multimodal SWE-agent has the ability to take screenshots of the web page that it's editing, in order to iteratively fix visual bugs.

https://preview.redd.it/0n5kqqzg9jtd1.png?width=1256&format=png&auto=webp&s=60a54b411436987d1036909ab5d6b0a4e7513a29

Paper: [https://arxiv.org/pdf/2410.03859](https://arxiv.org/pdf/2410.03859)

We're super excited about this paper and think that there's lots of room for further research in this direction.

I'll be around here if you have any questions, and you can also find me today at COLM.",MachineLearning,23,4,1728393733.0,1fyzo4x,ofirpress,https://www.reddit.com/r/MachineLearning/comments/1fyzo4x/r_swebench_multimodal_do_ai_agents_generalize_to/,Research
"[P] The Essential Guide to Large Language Model’s Structured Output, and Function Calling","For the past year, I’ve been building production systems using LLMs. When I started back in August 2023, materials were so scarce that many wheels had to be reinvented first. As of today, things have changed, yet the community is still in dire need of educational materials, especially from a production perspective.



Lots of people talk about LLMs, but very few actually apply them to their users/business.



Here is my new contribution to the community, “[The Essential Guide to Large Language Model’s Structured Output, and Function Calling](https://pavelbazin.com/post/the-essential-guide-to-large-language-models-structured-output-and-function-calling?utm_source=reddit&utm_medium=social&utm_campaign=structured_output&utm_content=sub_machinelearning)” article.



It is a hands-on guide (long one) on structured output and function calling, and how to apply them from 0 to 1. Not much of requirements, just some basic Python, the rest is explained.



I had quite a bit of success applying it at the company to the initiative “Let's solve all customer support issues via LLMs for 200K+ users.” We haven’t hit 100% of the goal yet, but we are getting there fast, and structured output in particular is what made it possible for us.



Spread the word, and let’s share more on our experience of applied LLMs beyond demos.",MachineLearning,23,2,1727307760.0,1fpiqlj,p_bzn,https://www.reddit.com/r/MachineLearning/comments/1fpiqlj/p_the_essential_guide_to_large_language_models/,Project
"[R] CPL: Critical Planning Step Learning Boosts
LLM Generalization in Reasoning Tasks","**TL;DR** Improve planning abilities of your LLM via MCTS and per-step Advantage Preference Optimization

**Paper:** [https://arxiv.org/pdf/2409.08642](https://arxiv.org/pdf/2409.08642)

**Abstract:**

>Post-training large language models (LLMs) to develop reasoning capabilities has proven effective across diverse domains, such as mathematical reasoning and code generation. However, existing methods primarily focus on improving task-specific reasoning but have not adequately addressed the model's generalization capabilities across a broader range of reasoning tasks. To tackle this challenge, we introduce Critical Planning Step Learning (CPL), which leverages Monte Carlo Tree Search (MCTS) to explore diverse planning steps in multi-step reasoning tasks. Based on long-term outcomes, CPL learns step-level planning preferences to improve the model's planning capabilities and, consequently, its general reasoning capabilities. Furthermore, while effective in many scenarios for aligning LLMs, existing preference learning approaches like Direct Preference Optimization (DPO) struggle with complex multi-step reasoning tasks due to their inability to capture fine-grained supervision at each step. We propose Step-level Advantage Preference Optimization (Step-APO), which integrates an advantage estimate for step-level preference pairs obtained via MCTS into the DPO. This enables the model to more effectively learn critical intermediate planning steps, thereby further improving its generalization in reasoning tasks. Experimental results demonstrate that our method, trained exclusively on GSM8K and MATH, not only significantly improves performance on GSM8K (+10.5%) and MATH (+6.5%), but also enhances out-of-domain reasoning benchmarks, such as ARC-C (+4.0%), BBH (+1.8%), MMLU-STEM (+2.2%), and MMLU (+0.9%).

**Visual Abstract:**

https://preview.redd.it/afih1qsaw6pd1.png?width=975&format=png&auto=webp&s=bd0c3c4385897c581dff193a02267052481a0e68

**Performance:**

https://preview.redd.it/hehec8txw6pd1.png?width=1117&format=png&auto=webp&s=a3d069590221c7acd509e1af83ea07a691f4e507

https://preview.redd.it/xgm1m55zw6pd1.png?width=1125&format=png&auto=webp&s=566f067d6bff31605b04f0ec0edb79c35945f77f

",MachineLearning,21,5,1726500933.0,1fi7ovt,StartledWatermelon,https://www.reddit.com/r/MachineLearning/comments/1fi7ovt/r_cpl_critical_planning_step_learning_boosts_llm/,Research
[R] Jamba-1.5: Hybrid Transformer-Mamba Models at Scale,"**TL;DR:** Large (up to 94B/398B active/total params) hybrid open-weights models with up to 256k context

**Paper:** [https://arxiv.org/pdf/2408.12570](https://arxiv.org/pdf/2408.12570)

**Blog:** [https://www.ai21.com/blog/announcing-jamba-model-family](https://www.ai21.com/blog/announcing-jamba-model-family)

**Abstract:**

>We present Jamba-1.5, new instruction-tuned large language models based on our Jamba architecture. Jamba is a hybrid Transformer-Mamba mixture of experts architecture, providing high throughput and low memory usage across context lengths, while retaining the same or better quality as Transformer models. We release two model sizes: Jamba-1.5-Large, with 94B active parameters, and Jamba-1.5-Mini, with 12B active parameters. Both models are fine-tuned for a variety of conversational and instruction-following capabilties, and have an effective context length of 256K tokens, the largest amongst open-weight models. To support cost-effective inference, we introduce ExpertsInt8, a novel quantization technique that allows fitting Jamba-1.5-Large on a machine with 8 80GB GPUs when processing 256K-token contexts without loss of quality. When evaluated on a battery of academic and chatbot benchmarks, Jamba-1.5 models achieve excellent results while providing high throughput and outperforming other open-weight models on long-context benchmarks. The model weights for both sizes are publicly available under the Jamba Open Model License and we release ExpertsInt8 as open source.

**Visual Highlights:**

[In hybrid architecture, Mamba-1 blocks outperform Mamba-2 blocks. Probably Mamba-2 processing specifics are redundant when model has attention](https://preview.redd.it/4l7mmaeadtkd1.png?width=873&format=png&auto=webp&s=138bd2a103d320d6e77864f07b1a5f71c2a08057)

https://preview.redd.it/onrdw742ftkd1.png?width=1129&format=png&auto=webp&s=dff7d4dd10ccc0b8d1481bab7de084cb2ac1b586

https://preview.redd.it/nm79gn64ftkd1.png?width=1145&format=png&auto=webp&s=650327eba37add3af6fd629371b98010789f10a2

https://preview.redd.it/yd3l7k46ftkd1.png?width=1115&format=png&auto=webp&s=bce413b0f95ec2cb786cb388589cbe22c06d5ca9

https://preview.redd.it/qe4choo7ftkd1.png?width=1129&format=png&auto=webp&s=eca7f428af74099d15dd1ebd7db1cdbe7d6d721e

https://preview.redd.it/vy26ido9ftkd1.png?width=1109&format=png&auto=webp&s=b6b2ee51a650c2910a01c465a810e33ad7880ebb

[Infinite-Bench](https://preview.redd.it/qaqy5m2cftkd1.png?width=1151&format=png&auto=webp&s=245466182aea9faa5d096930c7575107d8b1d4c2)

https://preview.redd.it/61dfzmugftkd1.png?width=1147&format=png&auto=webp&s=c53aa8a8d43b49aeb81871db9f25227874cc34ad

**Download**: [https://huggingface.co/collections/ai21labs/jamba-15-66c44befa474a917fcf55251](https://huggingface.co/collections/ai21labs/jamba-15-66c44befa474a917fcf55251)",MachineLearning,22,2,1724594133.0,1f0wvnz,StartledWatermelon,https://www.reddit.com/r/MachineLearning/comments/1f0wvnz/r_jamba15_hybrid_transformermamba_models_at_scale/,Research
[R] MAMBA 2 Head Dimension,"I've been reading the MAMBA 2 paper. I think I'm pretty well versed on MAMBA (1?), and understand MAMBA 2 at a high level, but Im having trouble understanding the difference between D in the original paper, and P in the MAMBA 2 paper. In MAMBA 1, the incoming tensor is shape B,L,D. Where D is some projection (I think). In MAMBA 2, they say the head dimension of MAMBA 1 was 1, but no longer in MAMBA 2.

They increase P from 1 to 64 or some other number in MAMBA 2. In the code snippet in the paper, it would appear P is an additional projection off of D, making our incoming tensor 4D, B,L,D,P. But some other sections of the paper make me think that P is really some division of D, sort of lining up with how you would divide the input sequence in a transformer into multiple heads. Which is correct? How should I interpret P?",MachineLearning,22,2,1723869571.0,1eu9ft8,redwat3r,https://www.reddit.com/r/MachineLearning/comments/1eu9ft8/r_mamba_2_head_dimension/,Research
[D] Pro's about writing a benchmark paper,"As someone who has never written a paper proposing a benchmark, I can only imagine the insights/gains from writing one. Maybe discovering the under-the-hood performance of models is one. For those who have written something similar to this, did you find your experience valuable? Would you recommend?",MachineLearning,22,8,1723428528.0,1eq22hg,Haunting_Air3071,https://www.reddit.com/r/MachineLearning/comments/1eq22hg/d_pros_about_writing_a_benchmark_paper/,Discussion
[R] Weak baselines and reporting biases lead to overoptimism in machine learning for fluid-related partial differential equations,,MachineLearning,21,3,1721577907.0,1e8pp4r,nuclear_knucklehead,https://arxiv.org/abs/2407.07218,Research
[R] Confidence scores LLMs,"Are there good studies on how to measure confidence scores of LLM extracted entities or classification etc. 
not convinced by idea that asking LLM is good way to estimate it. Hallucinations wouldn’t be a thing if LLM knows its confidence level. 
Any papers or ideas are appreciated ",MachineLearning,22,42,1721481281.0,1e7vdk3,EducationalSpread478,https://www.reddit.com/r/MachineLearning/comments/1e7vdk3/r_confidence_scores_llms/,Research
[D] [P] Exponential Growth of Context Length in Language Models,"https://preview.redd.it/0v289d9r2rad1.png?width=5376&format=png&auto=webp&s=014fc378d270ac8f8a7090eab1880fb381fe67f4

LLMs context length sizes seem to have been growing exponentially in the last few years — from 512 tokens with T5/BERT/GPT-1, to up to 2 million with the most recent Gemini 1.5 Pro.

It's unclear if the context window will continue growing at this pace, or if it will plateau at some point. How much context window becomes unnecessary?  

(If we estimate 100 tokens to be about 75 words, then all 7 Harry Potter books can fit in 1.5M tokens.)

----------  
Notes on data collection:

Had to track down each individual model's release blog (if there was one) and cross reference with their API docs (if it existed). Or a paper (if there was one). This field changes so fast, and also it's not uncommon for a company to release a model with X context window then 1 month later update the API docs and be like ""BUT WAIT! The context length is now Y"")

Sharing the raw data below, since I spent so much time painstakingly collecting this data. Also, open to spot checking in case I missed something.

[https://docs.google.com/spreadsheets/d/1xaU5Aj16mejjNvReQof0quwBJEXPOtN8nLsdBZZmepU/edit?gid=0#gid=0](https://docs.google.com/spreadsheets/d/1xaU5Aj16mejjNvReQof0quwBJEXPOtN8nLsdBZZmepU/edit?gid=0#gid=0)",MachineLearning,23,12,1720208069.0,1dw6e1r,porkbellyqueen111,https://www.reddit.com/r/MachineLearning/comments/1dw6e1r/d_p_exponential_growth_of_context_length_in/,Discussion
[R] Interpretability research in LLMs,"Most work in interpretable ML for LLMs has focused on mechanistic interpretability, rather than previous approaches in the literature like counterfactuals, case-based reasoning, prototypes, saliency maps, concept-based explanation, etc...

Why do you think that is? My feeling is it's because mech interp is just less computationally intensive to research, so it's the only option people really have with LLMs (where e.g., datasets are too big to do case-based reasoning). The other explanation is that people are just trying to move the field in different directions and mech interp is just that. Like people just want causal formal guarantees of LLM inference.

But I wanted to gauge people's feelings, do you think I'm right or are there other reasons for this trend?",MachineLearning,23,10,1719479379.0,1dpmuy9,SkeeringReal,https://www.reddit.com/r/MachineLearning/comments/1dpmuy9/r_interpretability_research_in_llms/,Research
"[P] [D] Updated On: Hi I'm a senior machine learning engineer, looking for for buddies to build cool stuff with!","**Old Post**: [https://www.reddit.com/r/MachineLearning/comments/1dj8pg6/p\_d\_hi\_im\_a\_senior\_machine\_learning\_engineer/](https://www.reddit.com/r/MachineLearning/comments/1dj8pg6/p_d_hi_im_a_senior_machine_learning_engineer/)

**Wow, I wasn't expecting this post to gain so much attention!**

I've created a Google form for those who are serious about working on projects, Kaggle competitions, or Leetcode challenges. Please take a moment to fill it out.  
Form: [https://forms.gle/k3jzCfNJy3rgz4ec6](https://forms.gle/k3jzCfNJy3rgz4ec6)

Here's how it will work:

* I will create groups based on your goals and expertise.
* Each group will have a team leader to assist with progress, alongside my support.
* It will take some time to organize everyone into teams, so please be patient. I'll reach out to you soon.

Thank you!",MachineLearning,21,11,1718812789.0,1djmpwx,Rude-Eye3588,https://www.reddit.com/r/MachineLearning/comments/1djmpwx/p_d_updated_on_hi_im_a_senior_machine_learning/,Discussion
[R] Why Has Predicting Downstream Capabilities of Frontier AI Models with Scale Remained Elusive?,,MachineLearning,21,3,1718042733.0,1dcs47t,RSchaeffer,https://arxiv.org/abs/2406.04391,Research
[R] Hydra: Enhancing Machine Learning with a Multi-head Predictions Architecture,,MachineLearning,22,3,1717834816.0,1day3hc,bluzkluz,https://www.researchgate.net/publication/381009719_Hydra_Enhancing_Machine_Learning_with_a_Multi-head_Predictions_Architecture,Research
[R] xLSTM official code + Kilcher video,"Paper, for reference, soon to be updated (per the video - see below):

[https://arxiv.org/pdf/2405.04517](https://arxiv.org/pdf/2405.04517)

NX-AI finally released a python package for their xLSTM implementation:

[https://github.com/nx-ai/xlstm](https://github.com/nx-ai/xlstm)

Yannic Kilcher also released a new video explaining the paper:

[https://www.youtube.com/watch?v=0OaEv1a5jUM](https://www.youtube.com/watch?v=0OaEv1a5jUM)

Has anyone reproduced the paper's results? I find that sLSTM is a massive improvement on vanilla LSTM, but I could not make mLSTM work by itself.  
",MachineLearning,22,3,1717514262.0,1d7zppm,Builder_Daemon,https://www.reddit.com/r/MachineLearning/comments/1d7zppm/r_xlstm_official_code_kilcher_video/,Research
[R] Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum,"**TL;DR**: do NOT stuff more than one document in the context window while training an LM.

**Paper:** [https://arxiv.org/abs/2405.13226](https://arxiv.org/abs/2405.13226)

**Abstract:** Large language models (LLMs) are commonly trained on datasets consisting of fixed-length token sequences. These datasets are created by randomly concatenating documents of various lengths and then chunking them into sequences of a predetermined target length. However, this method of concatenation can lead to cross-document attention within a sequence, which is neither a desirable learning signal nor computationally efficient. Additionally, training on long sequences becomes computationally prohibitive due to the quadratic cost of attention. In this study, we introduce dataset decomposition, a novel variable sequence length training technique, to tackle these challenges. We decompose a dataset into a union of buckets, each containing sequences of the same size extracted from a unique document. During training, we use variable sequence length and batch size, sampling simultaneously from all buckets with a curriculum. In contrast to the concat-and-chunk baseline, which incurs a fixed attention cost at every step of training, our proposed method incurs a penalty proportional to the actual document lengths at each step, resulting in significant savings in training time. We train an 8k context-length 1B model at the same cost as a 2k context-length model trained with the baseline approach. Experiments on a web-scale corpus demonstrate that our approach significantly enhances performance on standard language evaluations and long-context benchmarks, reaching target accuracy 3x faster compared to the baseline. Our method not only enables efficient pretraining on long sequences but also scales effectively with dataset size. Lastly, we shed light on a critical yet less studied aspect of training large language models: the distribution and curriculum of sequence lengths, which results in a non-negligible difference in performance.

**Visual Summary:**

https://preview.redd.it/nnvi519tvj2d1.png?width=1123&format=png&auto=webp&s=334b8990f4ac2d4298e1a622d71301cd7d6beae3",MachineLearning,22,10,1716633281.0,1d095cn,StartledWatermelon,https://www.reddit.com/r/MachineLearning/comments/1d095cn/r_dataset_decomposition_faster_llm_training_with/,Research
[R] Robust agents learn causal world models,"**Paper**: [https://arxiv.org/abs/2402.10877](https://arxiv.org/abs/2402.10877)

**Abstract**:

>It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound under a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.",MachineLearning,21,3,1716044769.0,1cuzbta,None,https://www.reddit.com/r/MachineLearning/comments/1cuzbta/r_robust_agents_learn_causal_world_models/,Research
[P] [D] Examples of client projects that you have delivered ,"Short version: give me some examples of client deliverables in the field of ML. Will help to judge where I stand to start freelance consulting.

Hi, I am an SWE learning ML on the side. My day to day job doesn’t have much exposure to ML but a lot of GPU stuff. I started learning ML and am at a stage where I can implement some models from research papers. 

Looking for some examples in the real world what are some deliverables that you have successfully done for a client. 

This would greatly help to understand where I stand in terms of taking up full time consultancy. 

Does it even make sense in the age of this humongous models to start an independent consultancy?",MachineLearning,21,8,1714791616.0,1cjqdmp,SmallTimeCSGuy,https://www.reddit.com/r/MachineLearning/comments/1cjqdmp/p_d_examples_of_client_projects_that_you_have/,Discussion
[D] Fine-tune Phi-3 model for domain specific data - seeking advice and insights,"Hi,

I am currently working on fine-tuning the Phi-3 model for financial data. While the loss is decreasing during training, suggesting that the model is learning quite well, the results on a custom benchmark are surprisingly poor. In fact, the accuracy has decreased compared to the base model.

Results I've observed:

* Phi-3-mini-4k-instruct (base model): Average domain accuracy of 40%
* Qlora - Phi-3-mini-4k-instruct (fine-tuned model): Average domain accuracy of 35%

I have tried various approaches, including QLora, Lora, and FFT, but all the results are poor compared to the base model. Moreover, I have also experimented with reducing the sequence length to 2k in an attempt to constrain the model and prevent it from going off-track, but unfortunately, this has not yielded any improvement.

I'm wondering if there might be issues with the hyperparameters, such as the learning rate, or if there are any recommendations on how I can effectively fine-tune this model for better performance on domain-specific data.

If anyone has successfully fine-tuned the Phi-3 model on domain-specific data, I would greatly appreciate any insights or advice you could share. Thank you in advance for your help and support!  


qlora configuration:

&#x200B;

    sequence_len: 4000 
    sample_packing: true 
    pad_to_sequence_len: true 
    trust_remote_code: True 
    adapter: qlora 
    lora_r: 256 
    lora_alpha: 512 
    lora_dropout: 0.05 
    lora_target_linear: true 
    lora_target_modules:   
        - q_proj   
        - v_proj   
        - k_proj   
        - o_proj   
        - gate_proj   
        - down_proj   
        - up_proj  
    
    gradient_accumulation_steps: 1 
    micro_batch_size: 2 
    num_epochs: 4 
    optimizer: adamw_torch 
    lr_scheduler: cosine 
    learning_rate: 0.00002 
    warmup_steps: 100 
    evals_per_epoch: 4 
    eval_table_size: 
    saves_per_epoch: 1 
    debug: 
    deepspeed: 
    weight_decay: 0.0

https://preview.redd.it/7afyhxcjv5yc1.png?width=976&format=png&auto=webp&s=1ce3efe6df6e4533bad5ec2f23e4f4968736bd56

&#x200B;",MachineLearning,23,26,1714720219.0,1cj2hzb,aadityaura,https://www.reddit.com/r/MachineLearning/comments/1cj2hzb/d_finetune_phi3_model_for_domain_specific_data/,Discussion
[N] Save 80% Memory for DPO and ORPO in Liger-Kernel,"Introducing the first open-source optimized post-training losses in Liger Kernel with \~80% memory reduction, featuring DPO, CPO, ORPO, SimPO, JSD, and more, achieving up to 70% end-to-end speedup through larger batch size. Use it as any PyTorch module - Available today in Liger v0.5.0!

[https://x.com/hsu\_byron/status/1866577403918917655](https://x.com/hsu_byron/status/1866577403918917655)",MachineLearning,21,0,1733984315.0,1hcewdl,Icy-World-8359,https://www.reddit.com/r/MachineLearning/comments/1hcewdl/n_save_80_memory_for_dpo_and_orpo_in_ligerkernel/,News
[D] Meta's new LLama model,"So meta just dropped a new, more efficient Llama model, Llama 3.3 70B, that basically promises to cut compute costs for large AI models. Has anyone here had a chance to test it out? Curious to see how it performs compared to previous versions, in terms of speed, resource usage, and accuracy",MachineLearning,20,7,1733785260.0,1han33i,Frosty_Programmer672,https://www.reddit.com/r/MachineLearning/comments/1han33i/d_metas_new_llama_model/,Discussion
[D] Small language models defining vocabulary using old vectors instead of new vectors,"I've been thinking a lot about why language models were so big and how they could be smaller. I thought about how every human brain can't possibly contain the entirity of human knowledge. I believe humans roughly have something along the lines of a probability matrix of words X other words, but not every word X every word.

It occurred to me that we frequently define unusual words (low frequency, not often used words) using other existing words we know. Can we potentially have a language model which uses vectors for the highest frequency words only, and ""unusal words"" which dont have their own vectors, but instead reference existing vectors? This could drastically decrease the word X word matrix as common words consists of a much smaller subset of the language. Maybe such a model could dynamically move reference words into and out of primary vectors when retrained on text that is specific to niche topics.

Knowing that I've never had an original thought, are there any other projects like this already?",MachineLearning,22,24,1731853561.0,1gtenw8,meteoraln,https://www.reddit.com/r/MachineLearning/comments/1gtenw8/d_small_language_models_defining_vocabulary_using/,Discussion
[D] What is the likely architecture/dataset for tiktok's realtime GAN models used in filters?,"I'm curious about how tiktoks filters perform so well at erasing hair (https://effecthouse.tiktok.com/learn/guides/technical-guides/objects/generative-effects/hair-eraser) and eyebrows (https://effecthouse.tiktok.com/learn/guides/technical-guides/objects/generative-effects/eyebrow-eraser).

Ive tried to do something similar (removing items from peoples faces in realtime) using a lightweight Pix2Pix style model on a paired dataset I created using OpenCV methods, but the quality of the generated image decreased too much as I reduced the size of the generator.

Anyone have any ideas on how they achieve such consistent results on such a lightweight model? Thanks",MachineLearning,20,9,1731360861.0,1gp3vz0,DjPoliceman,https://www.reddit.com/r/MachineLearning/comments/1gp3vz0/d_what_is_the_likely_architecturedataset_for/,Discussion
[D] COLING 2025 Results / rebuttals,"I'll go first.

Soundness: 3,3,4

Overall: 2,2,3

🥺",MachineLearning,21,83,1730286079.0,1gfjtnu,monkeyofscience,https://www.reddit.com/r/MachineLearning/comments/1gfjtnu/d_coling_2025_results_rebuttals/,Discussion
[D] M4 chips for training ML? (MPS),"Apple is (purposefully) creating a lot of buzz regarding their “Apple Intelligence”, stating that their M4 chips are built for AI.

My question is this,
Will this only be helpful for running the built in Apple Intelligence - or is this supposed to vastly improve on MPS when actually training large transformer models etc.? I haven’t heard them mention any improvements on MPS.
",MachineLearning,22,58,1730232628.0,1gf46km,Hmm_okay_jeps,https://www.reddit.com/r/MachineLearning/comments/1gf46km/d_m4_chips_for_training_ml_mps/,Discussion
"[D] New Interview with Leland McInnes: UMAP, HDBSCAN & the Geometry of Data | Learning from Machine Learning #10","This episode of Learning from Machine Learning explores the intersection of pure mathematics and modern data science with Leland McInnes, the mind behind an ecosystem of tools for unsupervised learning including UMAP, HDBSCAN, PyNN Descent and DataMapPlot. As a researcher at the Tutte Institute for Mathematics and Computing, McInnes has fundamentally shaped how we approach and understand complex data.

Resist the urge to chase the hype, seek a true understanding and really make a difference.
",MachineLearning,20,1,1730056142.0,1gdimqa,NLPnerd,https://youtu.be/6sSOr2Yaq80?si=MXKgRtdPy0B7D5CM,Discussion
[D] An interesting thread from December 2021 discussing the efficacy of transformers,,MachineLearning,22,4,1729324262.0,1g73ym7,next-choken,https://www.reddit.com/r/MachineLearning/comments/rboh2r/d_are_transformers_overhyped/,Discussion
[D] How important is the university reputation/ranking for PhD?,"Hi, Everyone!

I am currently in the search of a PhD position (in Europe) and I am deciding between multiple PhD positions. I have a solid profile (highly ranked university, nice research experience, good internships) and luckily for me I am getting interviews with almost every lab I apply to.

Since I could not find a concise answer to the following questions, I wanted to ask the community!

**1. How important is the university's ranking/reputation?**

I have found great labs all over the board. I have found some amazing labs in the universities ranked as low as 800qs. While I know how rankings are calculated, I fear not going to a reputable/known university. As someone who did bachelor's/master's at the #1 national universities, I am afraid that I would be putting myself at a disadvantage by getting a PhD somewhere like this.

**2. PI reputation vs the university reputation?**

This question mainly boils down to the difference between doing a PhD at a known university with a supervisor with few collaborators and a small research network, against a supervisor who is from an unknown university but is collaborating with top people in the field. Small fish in a big pond or a large fish in a large pond.

**3. University <> PI <> Research fit? How would you rank them? Which 2/3 would you pick?**

Since it's pretty unlikely you can find everything that you want. On what would you compromise?",MachineLearning,22,55,1728937728.0,1g3pw4t,Stoick01,https://www.reddit.com/r/MachineLearning/comments/1g3pw4t/d_how_important_is_the_university/,Discussion
[Project] A lossless compression library taliored for AI Models - Reduce transfer time of Llama3.2 by 33%,"If you're looking to cut down on download times from Hugging Face and also help reduce their server load—(Clem Delangue mentions HF handles a whopping 6PB of data daily!)

—> you might find ZipNN useful.

ZipNN is an open-source Python library, available under the MIT license, tailored for compressing AI models without losing accuracy (similar to Zip but tailored for Neural Networks).

It uses lossless compression to reduce model sizes by 33%, saving third of your download time.

ZipNN has a plugin to HF so you only need to add one line of code.

Check it out here:

[https://github.com/zipnn/zipnn](https://github.com/zipnn/zipnn)

There are already a few compressed models with ZipNN on Hugging Face, and it's straightforward to upload more if you're interested.

The newest one is Llama-3.2-11B-Vision-Instruct-ZipNN-Compressed

Take a look at this Kaggle notebook:

For a practical example of Llama-3.2 you can at this Kaggle notebook:

[https://www.kaggle.com/code/royleibovitz/huggingface-llama-3-2-example](https://www.kaggle.com/code/royleibovitz/huggingface-llama-3-2-example)

More examples are available in the ZipNN repo:  
[https://github.com/zipnn/zipnn/tree/main/examples](https://github.com/zipnn/zipnn/tree/main/examples)",MachineLearning,19,3,1727721144.0,1ft2v10,Candid_Raccoon2102,https://www.reddit.com/r/MachineLearning/comments/1ft2v10/project_a_lossless_compression_library_taliored/,Project
[R] Hyperbolic Brain Representations: Improving Representation Learning with Hyperbolic Geometry,"A new paper that looks at how hyperbolic geometry is used in the brain and how this can be used to help us improve our AI models.

https://arxiv.org/abs/2409.12990v3",MachineLearning,20,7,1727176543.0,1fo9yb2,platinumposter,https://www.reddit.com/r/MachineLearning/comments/1fo9yb2/r_hyperbolic_brain_representations_improving/,Research
[R] LowFormer: Hardware efficient Transformer Backbone Design,Throughput & Latency optimized Backbone Architecture with hardware efficient Macro and Micro Design. It also features a simple and efficient adaptation of Multi-Head Self-Attention.,MachineLearning,20,2,1725953168.0,1fdc52w,Mr_Fragwuerdig,https://arxiv.org/pdf/2409.03460,Research
[P] Python tool for steganography through LLMs,"https://github.com/user1342/Tomato
",MachineLearning,20,8,1725800077.0,1fbx3ny,OppositeMonday,https://www.reddit.com/r/MachineLearning/comments/1fbx3ny/p_python_tool_for_steganography_through_llms/,Project
[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",MachineLearning,21,29,1722737709.0,1ejkdhj,AutoModerator,https://www.reddit.com/r/MachineLearning/comments/1ejkdhj/d_selfpromotion_thread/,Discussion
[D] An interesting property of self-attention layers in umasked transformers like ViT,"I wrote a custom transformer implementation, and at initial minimalist tests of the attention code I noticed something that first seemed a bug, but on second thought may be an inherent property of attention.

Since the attention map is softmaxed for each embedding vector, its application will pull each vector towards the weighted average of other vectors in the contextual/spatial sequence. Oc the first attention layer may only pull each embedding towards a few specific other vectors (with the highest attention scores). And the pull is not directly towards other vectors but to their transformations with the value matrix (and also affected by the residual connection), but since the value matrix shares weights for all contextual/spatial slots, the effect remains: after an attention layer the embedding vectors are slightly more similar than before it.

And this is a positive feedback loop, since the now-slightly-more-similar embeddings will produce a more uniform attention map in the next attention layer, for more uniform averaging, and so on, for an increasingly strong homogenization effect.

Oc there are other factors that may counter this effect (like the intermittent other layers and nonlinearities, how the output layer is connected to the embedding vectors and how its gradients flow back and so on - the amount of training also matters for attention map uniformity). And for vision transformers this may even be a desirable effect (as the vectors/patches progressively assimilate class information from the entire image). But it still seems a unique property of attention and its use as a general spatial/contextual connection operator, that differs significantly from fully connected or convolutional layers.

I think this self-amplifying effect should be noticeable even in complete models (using various other blocks besides attention) as long as the model uses multiple attention blocks w/o causal mask (so less apparent for language models for example). Anybody have experienced this phenomenon?",MachineLearning,20,6,1720819309.0,1e1sy5u,lostn4d,https://www.reddit.com/r/MachineLearning/comments/1e1sy5u/d_an_interesting_property_of_selfattention_layers/,Discussion
"[R] Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation","**Title:** Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation

**Paper:** [**https://arxiv.org/abs/2406.16678**](https://arxiv.org/abs/2406.16678)

**Code:** [https://github.com/segment-any-text/wtpsplit](https://github.com/segment-any-text/wtpsplit)

https://preview.redd.it/6frvmpc36a9d1.png?width=1849&format=png&auto=webp&s=08c9769384d63bfd3ad786b0259f1dd4a97d4bce

**Abstract:**

>Segmenting text into sentences plays an early and crucial role in many NLP systems. This is commonly achieved by using rule-based or statistical methods relying on lexical features such as punctuation. Although some recent works no longer exclusively rely on punctuation, we find that no prior method achieves all of (i) robustness to missing punctuation, (ii) effective adaptability to new domains, and (iii) high efficiency. We introduce a new model - Segment any Text (SaT) - to solve this problem. To enhance robustness, we propose a new pretraining scheme that ensures less reliance on punctuation. To address adaptability, we introduce an extra stage of parameter-efficient fine-tuning, establishing state-of-the-art performance in distinct domains such as verses from lyrics and legal documents. Along the way, we introduce architectural modifications that result in a threefold gain in speed over the previous state of the art and solve spurious reliance on context far in the future. Finally, we introduce a variant of our model with fine-tuning on a diverse, multilingual mixture of sentence-segmented data, acting as a drop-in replacement and enhancement for existing segmentation tools. Overall, our contributions provide a universal approach for segmenting any text. Our method outperforms all baselines - including strong LLMs - across 8 corpora spanning diverse domains and languages, especially in practically relevant situations where text is poorly formatted. Our models and code, including documentation, are available at [this https URL](https://huggingface.co/segment-any-text) under the MIT license.

",MachineLearning,22,0,1719566555.0,1dqfhn6,markus_583,https://www.reddit.com/r/MachineLearning/comments/1dqfhn6/r_segment_any_text_a_universal_approach_for/,Research
[R] Should I respond to reviewers after I got an Accept recommendation for an ICML workshop?,"I've got three reviews and an area-chair meta-review recommending an acceptance to an ICLR workshop. The paper will also be published in PMLR. 

  
I'm wondering whether I should discuss with the reviewers in OpenReview. I've done it for other conferences since there was a ""rebuttal period"", but there's no such thing for this submission. Therefore it feels like the discussion part is not necessary, particularly after it's been accepted already by the area chair.   
  
However I think it's of course good to address their questions. Should I spend time on this? ",MachineLearning,20,8,1718883742.0,1dk9jmw,howtorewriteaname,https://www.reddit.com/r/MachineLearning/comments/1dk9jmw/r_should_i_respond_to_reviewers_after_i_got_an/,Research
[D] Need help finding an old Geoffrey Hinton video,"Marking as \[D\] even though, it's not really that.  
  
Something like 15 years ago, I saw a video on YouTube that was a Geoff Hinton lecture at the University of Toronto. In the video he was explaining classic Neural Network for MNIST digit recognition. At some point in the video, he talks about how one can run the inference in reverse, effectively making the network ""Imagine"" a digit. I can't seem to find this video anywhere. If I recall correctly, it was perhaps uploaded at some point to the Andrew Ng's learning platform, that for some reason this sub doesn't let me say by name.

Please help me finding this video. Thanks in advance!",MachineLearning,21,12,1718487432.0,1dgs9xk,edirgl,https://www.reddit.com/r/MachineLearning/comments/1dgs9xk/d_need_help_finding_an_old_geoffrey_hinton_video/,Discussion
[P] OpenMetricLearning 3.0 which uniformly supports images and texts!,"Hello everyone!

I want to share the release of [OpenMetricLearning](https://github.com/OML-Team/open-metric-learning) 3.0!

>OML — is a  library for representation learning & retrieval, with a zoo of models, losses, miners, samplers, metrics, and other useful stuff like DDP, integrations with PyTorchLightning and PyTorch Metric Learning, different experiment trackers and so on. 

  
**What's new?**

\* **We've added text support, and now we are adding audio!** (Users have already used OML not only for images, but now we provide out-of-the-box support, tests, and examples.)

\* The code works uniformly for images, texts, and will work for sounds! I invite you to check out the side-by-side [comparison](https://github.com/OML-Team/open-metric-learning?tab=readme-ov-file#examples) on images and texts.

\* The retrieval part has been separated, which can be used both for model validation and for inference with the following re-ranking or other post-processing.

\* Features of the library have been described in [one place](https://github.com/OML-Team/open-metric-learning?tab=readme-ov-file#oml-features) for easier navigation, and we've generally improved the documentation and examples.

\* Some calculations, especially memory-related, have been optimized.



**We welcome potential contributors:**

\* The code has become more modular, so the entry threshold has been lowered — you can take a separate piece of code and work on it.

\* We've also updated the [board](https://github.com/OML-Team/open-metric-learning/projects/1) with our issues/tasks.



Your ⭐️ on [GitHub](https://github.com/OML-Team/open-metric-learning) greatly helps us in further development!  


[OML](https://preview.redd.it/reyiwrjl1b6d1.jpg?width=1280&format=pjpg&auto=webp&s=f284ebc29fe0bc8d60a04bf35e3ec0acdec07266)

  
",MachineLearning,21,2,1718269574.0,1deujz2,Zestyclose-Check-751,https://www.reddit.com/r/MachineLearning/comments/1deujz2/p_openmetriclearning_30_which_uniformly_supports/,Project
[D] Thoughts on DSPy,"I have been tinkering with DSPy and thought I will share my 2 cents here for anyone who is planning to explore it:

The core idea behind DSPy are two things:

1.	⁠Separate programming from prompting
2.	⁠incorporate some of the best practice prompting techniques under the hood and expose it as a “signature”

Imagine working on a RAG. Today, the typical approach is to write some retrieval and pass the results to a language model for natural language generation. But, after the first pass, you realize it’s not perfect and you need to iterate and improve it. Typically, there are 2 levers to pull:

1.	⁠Document Chunking, insertion and Retrieval strategy
2.	⁠Language model settings and prompt engineering

Now, you try a few things, maybe document the performance in a google sheet, iterate and arrive at an ideal set of variables that gives max accuracy.

Now, let’s say after a month, model upgrades, and all of a sudden the accuracy of your RAG regresses. Again you are back to square one, cos you don’t know what to optimize now - retrieval or model? You see what the problem is with this approach? This is a very open ended, monolithic, brittle and unstructured way to optimize and build language model based applications.

This is precisely the problem DSPy is trying to solve. Whatever you can achieve with DSPy can be achieved with native prompt engineering and program composition techniques but it is purely dependent on the programmers skill. But DSPy provides native constructs which anyone can learn and use for trying different techniques in a systematic manner.

DSPy the concept:

Separate prompting from programming and signatures

DSPy does not do any magic with the language model. It just uses a bunch of prompt templates behind the scenes and exposes them as signatures. Ex: when you write a signature like ‘context, question -> answer’, DSPy adds a typical RAG prompt before it makes the call to the LLM. But DSPy also gives you nice features like module settings, assertion based backtracking and automatic prompt optimization.

Basically, you can do something like this with DSPy,

“Given a context and question, answer the following question. Make sure the answer is only “yes” or “no””. If the language model responds with anything else, traditionally we prompt engineer our way to fix it. In DSPy, you can assert the answer for “yes” or “no” and if the assertion fails, DSPy will backtrack automatically, update the prompt to say something like, “this is not a correct answer- {previous_answer} and always only respond with a “yes” or “no”” and makes another language model call which improves the LLMs response because of this newly optimized prompt. In addition, you can also incorporate things like multi hops in your retrieval where you can do something like “retrieve -> generate queries and then retrieve again using the generated queries” for n times and build up a larger context to answer the original question.

Obviously, this can also be done using usual prompt engineering and programming techniques, but the framework exposes native easy to use settings and constructs to do these things more naturally. DSPy as a concept really shines when you are composing a pipeline of language model calls where prompt engineering the entire pipeline or even module wise can lead to a brittle Pipeline.

DSPy the Framework:

Now coming to the framework which is built in python, I think the framework as it stands today is

1.	⁠Not production ready
2.	⁠Lacks clear documentation
3.	⁠Poorly designed with not so clean interfaces and abstractions

To me it felt like a rushed implementation with little thought for design thinking, testing and programming principles. The framework code is very hard to understand with a lot of meta programming and data structure parsing and construction going behind the scenes that are scary to run in production.

This is a huge deterrent for anyone trying to learn and use this framework. But, I am sure the creators are thinking about all this and are working to reengineer the framework. There’s also a typescript implementation of this framework that is fairly less popular but has a much better and cleaner design and codebase:

https://github.com/dosco/llm-client/

My final thought about this framework is, it’s a promising concept, but it does not change anything about what we already know about LLMs. Also, hiding prompts as templates does not mean prompt engineering is going away, someone still needs to “engineer” the prompts the framework uses and imo the framework should expose these templates and give control back to the developers that way, the vision of separate programming and prompting co exists with giving control not only to program but also to prompt.

Finally, I was able to understand all this by running DSPy programs and visualizing the LLM calls and what prompts it’s adding using my open source tool - https://github.com/Scale3-Labs/langtrace . Do check it out and let me know if you have any feedback.",MachineLearning,22,5,1715572047.0,1cqq1in,cryptokaykay,https://www.reddit.com/r/MachineLearning/comments/1cqq1in/d_thoughts_on_dspy/,Discussion
[R] An Analysis of Linear Time Series Forecasting Models,"Our work on analysing linear time series forecasting models was accepted to ICML.

ArxiV: https://arxiv.org/abs/2403.14587

### Abstract:
Despite their simplicity, linear models perform well at time series forecasting, even when pitted against deeper and more expensive models. A number of variations to the linear model have been proposed, often including some form of feature normalisation that improves model generalisation. In this paper we analyse the sets of functions expressible using these linear model architectures. In so doing we show that several popular variants of linear models for time series forecasting are equivalent and functionally indistinguishable from standard, unconstrained linear regression. We characterise the model classes for each linear variant. We demonstrate that each model can be reinterpreted as unconstrained linear regression over a suitably augmented feature set, and therefore admit closed-form solutions when using a mean-squared loss function. We provide experimental evidence that the models under inspection learn nearly identical solutions, and finally demonstrate that the simpler closed form solutions are superior forecasters across 72% of test settings.

### Summary
Several popular works have argued that linear regression is sufficient for forecasting (DLinear and FITs are examples for the discerning reader). It turns out that if you do the maths these models are essentially equivalent. We do the math and also the experiments. Perhaps most interestingly: the ordinary least squares (OLS) solution is almost always better than other linear models trained using gradient descent. Importantly: we did **not** do a hyper parameter search to set, for example, the regularisation coefficient. We reserve that for future work.

OLS is extremely efficient - a model can be fit in the order of milliseconds if set up right. 

Finally, although we don't go to lengths to show this: many of our results are superior to large and complex models, begging the question of when and where such models are effective.",MachineLearning,21,5,1714821880.0,1cjy2d9,Gramious,https://www.reddit.com/r/MachineLearning/comments/1cjy2d9/r_an_analysis_of_linear_time_series_forecasting/,Research
What cool thing are you using it for?[D],"Hey everyone, I just wanted to hear some cool things that people are successfully using ML/DL for, professionally and personal?   


Maybe some cool detection system for agriculture, or for counting wild life in some scenario. Possibly you are working on making a self driving little car, that uses reinforcement learning and Lidar, or maybe some generative AI for art?   


I'd love to hear some details, successes, failures, anything really, about the projects you are working on. Thanks! ",MachineLearning,21,31,1714502430.0,1ch094g,Brilliant-Donkey-320,https://www.reddit.com/r/MachineLearning/comments/1ch094g/what_cool_thing_are_you_using_it_ford/,Discussion
[D] Mathematical aspects of tokenization,"I recently made a video covering our recent work on the mathematical aspects of tokenization, specifically:
- formalization of tokenization as compression
- bounds of Byte-Pair Encoding optimality 
- link between tokenization entropy and performance

I'd be very grateful for any feedback as I'm still learning how to make educational videos. Thank you!

https://youtu.be/yeEZpf4BlDA",MachineLearning,21,0,1714208809.0,1cea5qn,zouharvi,https://www.reddit.com/r/MachineLearning/comments/1cea5qn/d_mathematical_aspects_of_tokenization/,Discussion
[D] What's the current SOTA for Biomedical Encoder Models?,"What you would consider the current SOTA for biomedical models for creating vector embeddings? I'm mostly interested in sentence similarity (and some document retrieval).

I personally think of PubMedBERT and BioBERT to be a really good baselines but are there fine-tunes that you trust more?

What would you even consider a good benchmark for this domain? I find MTEB too general and BioASQ, BIOSSES, MedSTS too specific to measure anything meaningful. What do you guys think?",MachineLearning,18,7,1735610580.0,1hq3zwq,ayushwashere,https://www.reddit.com/r/MachineLearning/comments/1hq3zwq/d_whats_the_current_sota_for_biomedical_encoder/,Discussion
[D] How can we use ML for good?,"Hello, i'm finishing my bachelor degree and i really want to help people using what i learned about machine learning and data engineering, i have some prior experience but it would be nice if we could partner with some startups to share our knowledge about ML and engineering and actually create value.

D**o** you know some startups or NGO that use their data skills to help people or fostering innovation?

How can we use our skills to help others?

I've been thinking that setting up basic data infra to small businesses and then use ML algorithms to help them increase productivity would be nice, but I don't know where to begin with.",MachineLearning,21,15,1735492563.0,1hp0m3o,Southern_Respond846,https://www.reddit.com/r/MachineLearning/comments/1hp0m3o/d_how_can_we_use_ml_for_good/,Discussion
[D] Training with synthetic data and model collapse. Is there progress?,"About a year ago, research papers talked about model collapse when dealing with synthetic data. Recently I’ve been hearing about some progress in this regard. I am not expert and would welcome your views on what’s going on. Thank you and have a fantastic day.",MachineLearning,19,24,1734084223.0,1hd92mt,BubblyOption7980,https://www.reddit.com/r/MachineLearning/comments/1hd92mt/d_training_with_synthetic_data_and_model_collapse/,Discussion
[R] Population-based Model Merging via Quality Diversity,"In case any of you are interested, here is a [blog post](https://sakana.ai/cycleqd/) about our recent paper [Agent Skill Acquisition for Large Language Models via CycleQD](https://arxiv.org/abs/2410.14735).",MachineLearning,19,0,1733194947.0,1h5dmnb,hardmaru,https://www.reddit.com/r/MachineLearning/comments/1h5dmnb/r_populationbased_model_merging_via_quality/,Research
"[R] Cellular Automaton-Driven Mirrored Tensor Surface for Structured Perturbation in Neural Networks: A Novel Approach to Dynamic Regularization, Enhanced Plasticity, and Multi-Scale Learning through Continuous State-Based Weight Modulation","# Cellular Automaton-Driven Weight Perturbation: A Novel Approach to Neural Network Optimization

## Abstract

Current neural network training methodologies often result in models that, while converged, may not represent optimal weight configuration for given datasets. Traditional approaches, including dropout and noise injection, attempt to address this by introducing randomness during training. However, these methods lack structure and may not efficiently explore the weight space. This shrek paper proposes a novel training paradigm utilizing cellular automaton-driven weight perturbation to enhance neural network optimization.

Our approach introduces a mirrored tensor surface governed by a [continuous state cellular automaton](https://streamable.com/va555y), which interacts with the network's weight space during training. The full model architecture is duplicated into a mirror whose weights now represent cells of a continuous state automaton. This method aims to provide structured, multi-scale perturbations that are more aligned with the inherent patterns in data and the network's learned representations than uniform noise.

Key intuitions driving this approach include:

1. The potential for structured perturbations to guide more meaningful exploration of the weight space.
2. The ability of cellular automata to generate complex, emergent behaviors from simple rules.
3. The possibility of multi-scale effects that could enhance feature learning across various levels of abstraction.

We hypothesize that this method addresses several limitations of current models:

1. Overcoming local optima: The structured perturbations may help models escape suboptimal convergence points.
2. Enhancing generalization: By promoting more diverse weight configurations, the approach could lead to better generalization.
3. Improving adaptability: The dynamic nature of the perturbations could result in more adaptable models.

This approach can be viewed as an advanced form of dropout, offering more controlled and potentially more beneficial regularization. Unlike dropout, which randomly deactivates neurons, our method introduces structured changes to weights, potentially preserving important learned features while encouraging exploration.

# Intuitions

More intuitions behind this method can be found here https://www.reddit.com/r/LocalLLaMA/comments/1fyx27y/im_pretty_happy_with_how_my_method_worked_out/lqzoqfg/ but in essence:

We theorize that current models make poor use of the available total weight count, and that backpropagation on models initialized from uniform noise lead to **enormous representation redundancy**, which results in...

1. Hallucinations due to redundancies with slight variances.
2. Slower convergence as a result of meta-structures required in the late layers which negotiate redundancies to produce coherent outputs that humans like.
3. A need for larger models as a result of the negotiation structure required to mediate redundancy.

Using more sophisticated training methods, we postulate that models in the millions of parameters could be made to perform on the level of models hundreds of time their size. We encourage the /r/LocalLLaMA community to experiment and play with this concept.

# Future Research Directions

If this preliminary concept is proven, we can then hope to augment it with policy networks which learn to pilot the automaton, receiving the loss history and quality evaluations as input to instill a feedback loop. Using a larger evaluation LLM, the model in training is dynamically probed for functional progress and 'levels of consciousness' such that we have a meta-optimization loop where we train this policy network to pilot perturbations in more and more effective ways. The LR could increase, enabling faster and faster convergence back to a functioning model.

The CSA itself could be swapped to a Neural State Automaton with dynamically learned rules. The insight embedded in diffusion model could also be fine-tuned and repurposed into a temporal pattern generator, and instead use text prompts to craft a whole realm of possible dynamics.

# Implementation Steps

Here's a concrete explanation of how you might implement CADMTS:

1. **Initialize the Neural Network:**
   - Create a standard neural network architecture (e.g., a transformer for language tasks).
   - Initialize the weights normally (e.g., using Xavier or He initialization).

2. **Create the Mirrored Cellular Automaton:**
   - Duplicate the structure of your neural network.
   - Instead of normal weight values, initialize this mirror with cellular automaton states.
   - These states could be continuous values between 0 and 1, representing the ""activity"" of each cell.

3. **Define Cellular Automaton Rules:**
   - Create rules for how each ""cell"" (mirroring a weight in the original network) updates based on its neighbors.
   - For example, a simple rule could be: `new_state = (avg_of_neighbors + current_state) / 2`
   - More complex rules could involve thresholds, non-linear functions, or even small neural networks.
   - The automaton in the video above is available for experimentation here https://claude.site/artifacts/f28fcfb9-8718-4305-bacc-03a2e1912b18 

4. **Training Loop:**
   For each training batch:
   1. *Forward Pass:*
      - Perform a normal forward pass through the neural network.
   
   2. *Backward Pass:*
      - Compute gradients as usual.
   
   3. *Cellular Automaton Update:*
      - Update the state of each cell in the mirrored CA based on your defined rules.
   
   4. *Weight Perturbation:*
      - Use the CA states to perturb the weights of the original network.
      - For example: `perturbed_weight = original_weight + (ca_state - 0.5) * perturbation_strength`
      - Or: `perturbed_weight = original_weight + (ca_state * random_value) * perturbation_strength` where `random_value` is generated with `randn_like` for the tensor being modified. 
   
   5. *Weight Update:*
      - Apply the computed gradients to the perturbed weights.

5. **Hyperparameter Tuning:**
   - Adjust the strength of the CA influence (`perturbation_strength`).
   - Experiment with different CA update rules.
   - Try various schedules for when to apply the CA perturbation (e.g., every N steps)
   - Try various schedules of the `perturbation_strength` (e.g. a cyclical sine wave, the dB of a rotating corpus of jazz music, ...)

6. **Evaluation:**
   - Compare the performance of your CA-perturbed model against a baseline without perturbation.
   - Analyze how the CA states evolve over time and correlate with model performance.

7. **Advanced Implementations:**
   1. *Multi-scale CA:*
      - Implement different CA rules at different layers of the network.
      - For example, faster-changing CAs in lower layers, slower in higher layers.
   
   2. *Adaptive CA Rules:*
      - Implement meta-learning to adapt the CA rules based on model performance.
   
   3. *Visualization:*
      - Create tools to visualize the CA states and how they correlate with weight importance.

8. **Integration with Existing Techniques:**
   - Combine this method with other regularization techniques like dropout or weight decay.
   - Experiment with using the CA states to influence learning rates for each weight.

9. **Continuous Refinement:**
   - Based on empirical results, continuously refine your CA rules and perturbation strategies.
   - Consider implementing a policy network that learns to control the CA based on model performance metrics.

This implementation approach allows for a great deal of experimentation. You could start with simple, uniform CA rules and gradually increase complexity. The key is to create a system where the CA provides structured, meaningful perturbations to the weights, potentially allowing the network to explore weight configurations that might be missed by standard gradient descent.

Remember, the goal is to create perturbations that are more structured and potentially more meaningful than random noise, hopefully leading to better exploration of the weight space and ultimately better model performance.",MachineLearning,22,28,1728589847.0,1g0rhsx,ryunuck,https://www.reddit.com/r/MachineLearning/comments/1g0rhsx/r_cellular_automatondriven_mirrored_tensor/,Research
[D] Option to make NeurIPS rejected paper reviews public?,"The decision notification e-mail from NeurIPS mentioned that we would be offered the option to opt in to publicly releasing reviews for a rejected paper and that instructions would follow in a few days.

It's been over a week and we have not yet received any e-mail nor is there any author task to opt in. Since last year this e-mail came only 3 days after the notification I'm wondering if there was some issue and if no1 has received the e-mail yet?

",MachineLearning,20,9,1728045161.0,1fvy0n4,Aj0o,https://www.reddit.com/r/MachineLearning/comments/1fvy0n4/d_option_to_make_neurips_rejected_paper_reviews/,Discussion
"[D] Llama-3.2 vs llama-3.1 in Medical Domain: Llama-3.1 70B Outperforms Llama-3.2 90B
","[Large LLama Models in Medical Domain \(90B, 70B, 11B\)](https://preview.redd.it/v9di045iq3rd1.png?width=2084&format=png&auto=webp&s=e7bd3de9dcc4f7025f0cd5e9ad2a6c2e88ced26e)

Exploring how LLama 3.2 Large models and Llama 3.1 models perform in the medical field. (Without Fine-tuning)

🥇 Meta-Llama-3.1-70B-Instruct: Overall champion with 84% average score

* Excels in MMLU College Biology (95.14%)
* Strong performance in MMLU Professional Medicine (91.91%)

🥈 Meta-Llama-3.2-90B-Vision (Instruct and Base): Tied for second place with 83.95% average

* Consistent performance across Instruct and Base versions
* Best in MMLU College Biology (93.06%) and MMLU Professional Medicine (91.18%)

🥉 Meta-Llama-3-70B-Instruct: Third place with 82.24% average

* Strongest in MMLU Medical Genetics (93%)
* Solid performance in MMLU College Biology (90.28%)

[Small LLama & Phi Models in Medical Domain \(3B, 1B\)](https://preview.redd.it/r7r2n03jz3rd1.png?width=2078&format=png&auto=webp&s=32de7cdf48f21f5997972ec3b279ba1b698822df)

I also analyzed Smaller models and compared them with phi-3 to explore how small models perform in the medical field. (Without Fine-tuning)

🥇 Phi-3-4k: Top performer with 68.93% average score

* Excels in MMLU College Biology (84.72%)
* Strong performance in MMLU Clinical Knowledge (75.85%)

🥈 Meta-Llama-3.2-3B-Instruct: Second place with 64.15% average

* Best in MMLU College Biology (70.83%)
* Solid performance in PubMedQA (70.6%)

🥉 Meta-Llama-3.2-3B: Third place with 60.36% average

* Strongest in MMLU College Biology (63.89%)
* Good performance in PubMedQA (72.8%)

Additional Observations:

[Eval Results](https://preview.redd.it/zm361cl8h4rd1.png?width=2744&format=png&auto=webp&s=ff617d221d238aa4f7e8b1b59d5de1c10b34389b)

1. **Identical Performance in Vision Models**:
   * Meta-Llama-3.2-90B-Vision Instruct and Base versions show identical performance (83.95% average) across all metrics and all 9 datasets, down to the decimal point.
   * Similarly, Meta-Llama-3.2-11B-Vision Instruct and Base versions also demonstrate identical scores (72.8% average) in all categories. (evaluated twice)
2. Unusual Consistency:
   * This perfect alignment between the Instruct and Base versions is a little atypical, as Instruct and base variants usually show slight performance differences..
   * I am guessing is it due to vision instruct tuning? Could vision models' capabilities be less dependent on specific instruction tuning for medical tasks?
   * Results are in JSON, available [here on Github](https://github.com/monk1337/Medical_LLM_Evals/blob/main/llama_3_results.json)

Will be evaluating more models soon for the Medical Domain Here - [Source Post](https://x.com/aadityaura/status/1839233111927750830)",MachineLearning,20,1,1727342985.0,1fps53b,aadityaura,https://www.reddit.com/r/MachineLearning/comments/1fps53b/d_llama32_vs_llama31_in_medical_domain_llama31/,Discussion
"[D] Last Week in Medical AI: Top Research Papers/Models  🏅(September 1  - September 7, 2024) ","[Top papers of the week \(September 1  - September 7, 2024\) ](https://preview.redd.it/rbq4jd4nkfnd1.jpg?width=1386&format=pjpg&auto=webp&s=276221ec2a1bb01917930389099bef9e4623dcf9)

  
**Medical LLM & Other Models :**

* CancerLLM: Large Language Model in Cancer Domain 
   * CancerLLM, a 7-billion-parameter model designed for cancer-specific tasks. Pre-trained on 2.67 million clinical notes and 515,524 pathology reports across 17 cancer types. 

* MedUnA: Vision-Language Models for Medical Image 
   * The paper introduces Medical Unsupervised Adaptation (MedUnA). It aligns text embeddings with class labels using BioBERT, then integrates with MedCLIP's visual encoder for visual-text alignment via contrastive entropy loss.

* Foundation Model for Robotic Endoscopic Surgery  
   * This paper presents Depth Anything in Robotic Endoscopic Surgery (DARES), which introduces Vector-LoRA, a new adaptation technique for self-supervised monocular depth estimation in robotic-assisted surgery (RAS).

* Med-MoE: MoE for Medical Vision-Language Models  
   * This paper introduces Med-MoE (Mixture-of-Experts), a lightweight framework designed for both discriminative and generative multimodal medical tasks. Med-MoE operates in three stages:   

* CanvOI: Foundation Model for Oncology
   * This paper introduces CanvOI, a ViT-g/10-based foundation model for digital pathology, optimized for oncologic histopathological images.   

  
**Medical Benchmarks and Evaluations:**

* TrialBench: Clinical Trial Datasets & Benchmark 
* LLMs for Medical Q&A Evaluation 
* MedFuzz: Exploring Robustness Medical LLMs 
* MedS-Bench: Evaluating LLMs in Clinical Tasks 
* DiversityMedQA: Assessing LLM Bias in Diagnosis

  
**LLM Digital Twins:**

* Digital Twins for Rare Gynecological Tumors 
* DT-GPT: Digital Twins for Patient Health Forecasting  

....  
  
Check the full thread in detail: [https://x.com/OpenlifesciAI/status/1832476252260712788](https://x.com/OpenlifesciAI/status/1832476252260712788)

Thank you for reading! If you know of any interesting papers that were missed, feel free to share them in the comments. If you have insights or breakthroughs in Medical AI you'd like to share in next week's edition, connect with us on Twt/x: [OpenlifesciAI](https://x.com/OpenlifesciAI)",MachineLearning,20,3,1725734965.0,1fbe5qg,aadityaura,https://www.reddit.com/r/MachineLearning/comments/1fbe5qg/d_last_week_in_medical_ai_top_research/,Discussion
[R] Generalized Power Attacks against Hardware Cryptography using Long-Range Deep Learning,"https://preview.redd.it/8x0oesms1fnd1.jpg?width=2000&format=pjpg&auto=webp&s=3d3d331ddb4ebf23eface0d49b312b28cd5e2fcd

Happy Saturday

I am thrilled to announce that after 3 years of R&D we finally have published GPAM our generalized model power-side-channel attacks model:

* slides & paper: [https://elie.net/publication/generalized-power-attacks-against-crypto-hardware-using-long-range-deep-learning](https://elie.net/publication/generalized-power-attacks-against-crypto-hardware-using-long-range-deep-learning)
* code & datasets: [https://github.com/google/scaaml/tree/main/papers/datasets/ECC/GPAM](https://github.com/google/scaaml/tree/main/papers/datasets/ECC/GPAM)

Compared to previous approach GPAM represent a generational leap because it is **able to attack multiples algorithms (AES, ECC) and counter-measures without the need of human intervention** and **without the need to pre-process the input traces**. It does requires some automated hyper-tuning thus: \~700 GPU/h per attack.",MachineLearning,22,1,1725727495.0,1fbbbb3,ebursztein,https://www.reddit.com/r/MachineLearning/comments/1fbbbb3/r_generalized_power_attacks_against_hardware/,Research
[P]⚡️Fastest Pre-training Code: LLM in 9 days,"We created an LLM that outperform OpenELM and Phi on MT-Bench, in just 9 days. It's built on the Lightning framework with optimisations from TinyLlama, achieving ultra high throughput (\~99.6% GPU utilization). Releasing it for everyone, please give a star if you like what we do.

Code: [https://github.com/pints-ai/1.5-Pints](https://github.com/pints-ai/1.5-Pints)",MachineLearning,20,1,1725714166.0,1fb6fet,calvintwr,https://www.reddit.com/r/MachineLearning/comments/1fb6fet/pfastest_pretraining_code_llm_in_9_days/,Project
[D] Bayesian Models vs Conformal Prediction (CP),"Hi all,

I am creating this post to get your opinion on two main uncertainty quantification paradigms. I have seen a great rivalry between researchers representing them. I have done research on approximate reference (and Bayesian Deep Learning) but beyond a basic tutorial on CP,  I am not very familiar with CP. My personal opinion is that both of them are useful tools and could perhaps be employed complementary:

CP can provide guarantees but are poshoc methods, while BDLs can use prior regularization to actually \*improve\* model's generalization during training. Moreover, CP is based on the IID assumption (sorry if this is not universally true, at least that was the assumption in the tutorial), while in BDL inputs are IID only when conditioned on an observation of the parameter: in general p(yi,yj|xi,xj)!=p(yi|xi)p(yj|xj) but   p(yi,yj|xi,xj,theta)=p(yi|xi, theta)xp(yj|xj, theta). So BDLs or Gaussian Processes might be more realistic in that regard.

  
Finally, couldn't one derived CP for Bayesian Models? How much the set of predictions provided by CP and those by the Bayesian Model agree in this case? Is there a research paper bridging these approaches and testing this?

  
Apologies in advance if my questions are too basic. I just want to keep an unbiased perspective between the two paradigms.

  


  


",MachineLearning,20,23,1725619802.0,1fac5u5,South-Conference-395,https://www.reddit.com/r/MachineLearning/comments/1fac5u5/d_bayesian_models_vs_conformal_prediction_cp/,Discussion
[P] I built an open-source tool that lets you build GPU-accelerated NNs and Transformers directly on the Web,"Everyone loves PyTorch's syntax.

[JS-PyTorch](https://github.com/eduardoleao052/js-pytorch) lets you write PyTorch-like code and run it directly on Web Browsers using JavaScript. The tool also supports GPU acceleration, running up to 30x faster than native Python or JS.

I'd love to hear some feedback, and what you'd like to see in this project to make it better!",MachineLearning,20,6,1722883635.0,1ekvomn,suspicious_beam,https://www.reddit.com/r/MachineLearning/comments/1ekvomn/p_i_built_an_opensource_tool_that_lets_you_build/,Project
[D] How to Perform LLM Inference in a Production Environment. Share your best practices.,"Greetings to all,

Could you please suggest and provide recommendations on how to configure the LLM Inference engine for LLM models such as Meta3.1 8B and 70B in accordance with well-known standards and best practices? We have a single bare-metal DGXA100 server with 8 40 GB A100 GPUs.

* Should we use Kubernetes or just Docker?
* Should we use Nvidia Triton, HuggingFace TGI, or vLLM?
* In terms of load balancing, what would be the best approach? Should we use HA proxy or a network balancer?
* How should we go about implementing a throttling mechanism to ensure the system's stability and prevent overloading?

Thank you very much for your help in advance!

Best regards,

Shakhizat",MachineLearning,19,3,1722861935.0,1ekmuwe,shakhizat,https://www.reddit.com/r/MachineLearning/comments/1ekmuwe/d_how_to_perform_llm_inference_in_a_production/,Discussion
[R] Low rank field-weighted factorization machines,"Our paper '***Low Rank Field-Weighted Factorization Machines for Low Latency Item Recommendation***', by *Alex Shtoff, Michael Viderman, Naama Haramaty-Krasne, Oren Somekh, Ariel Raviv, and Tularam Ban*, has been accepted to RecSys 2024.

I believe it's of interest to the ML-driven recommender system community. I think it's especially interesting to researchers working on large scale systems operating under extreme time constraints, such as online advertising.

  
**TL;DR**: We reduce the cost of inference of FwFM models with n features and nᵢ item features from O(n²) to O(c nᵢ), where c is a small constant. This is to facilitate much cheaper large scale real-time inference for item recommendation.

Code and paper: [GitHub link](https://github.com/michaelviderman/pytorch-fm/tree/dev).

**Details**

FMs are widely used in online advertising because they strike a good balance between representation power, and blazing fast training and inference speed. It is is paramount for large scale recommendation under tight time constraints. 

The main trick devised by Rendle et. al is computing \*pairwise\* interactions of n features in O(n) time. Moreover, user / context features, which are the same when ranking multiple items for a given user, can be handled separately (see the image below). The computational cost of a single recommendation becomes O(nᵢ) per item, where nᵢ is the number of item features. Consequently, adding more user or context features is practically free.

[FM formula in linear time](https://preview.redd.it/qt9peb3csfed1.png?width=971&format=png&auto=webp&s=34a0191edabfce02c1b1f898145d9d7eba2d3af0)

The more advanced variants, such as Field-Aware and Field-Weighted FMs do not enjoy this property and require O(n²) time. This poses a challenge to such systems, and requires carefully thinking weather an additional user or context feature is worth the cost at inference. Typically, aggressive pruning of the field interactions is employed to dramatically reduce the computational cost, at the expense of model accuracy. 

In this work we devise a reformulation of the Field-Weighted FM family using diagonal plus low-rank (DPLR) factorization of the field interaction matrix, that facilitates inference in O(c nᵢ) time per item, where c is a small constant that we control. As is the case with pruning, the price is a slight reduction in model accuracy. We show that with a comparable number of parameters, the DPLR variant outperforms pruning on real world datasets, while facilitating significantly faster inference speeds, and gaining back the ability to add user context items practically for free. Here is a short chart summarizing the results:

[Diagonal+LowRank \(DPLR\) inference time significantly outperforms pruned time, and decreases quickly as the portion of context features \(out of 40 total features\) is increased. Plotted for various ad auction sizes and model ranks.](https://preview.redd.it/kijfagqqsfed1.png?width=1638&format=png&auto=webp&s=8fd094ca32d8084f156e3a75bc0310bcfd0a0ed3)

",MachineLearning,20,5,1721814006.0,1eaxapg,alexsht1,https://www.reddit.com/r/MachineLearning/comments/1eaxapg/r_low_rank_fieldweighted_factorization_machines/,Research
Likelihood computation in diffusion models [P],"I'm currently working on diffusion models (mainly the SDE approach related in [this paper (Song et al.)](http://arxiv.org/abs/2011.13456) and [this paper (Song et al.)](http://arxiv.org/abs/2101.09258)) and I'm curious about this statement about likelihood computation : they state that to have access to exact likelihood for the generated data $log(q\_\\theta(y))$,  one need to rely on the probability flow ODE, which has the same marginals as the SDE used for training, because SDE likelihoods aren't tractable.

The problem is I don't understand why : each step of the reverse SDE amounts to applying almost the same transformation to the data as the ODE, except that we also add some brownian noise :

https://preview.redd.it/es5l2ybuajad1.png?width=758&format=png&auto=webp&s=76e189e4c529c49210fed98d0b7cf6787e445032

To compute the likelihood we would like to know the jacobian of transformations between $y_t$ and $y\_{t+dt}$ at each step, which for me seems feasible in the SDE because the brownian motion is just some gaussian noise doesn't involve $y_t$ (but I think that's where my understanding of SDEs falls short).

Would anyone here be able to explain the problem in this reasoning ?",MachineLearning,20,4,1720113039.0,1dvbzrh,Antoine_m8,https://www.reddit.com/r/MachineLearning/comments/1dvbzrh/likelihood_computation_in_diffusion_models_p/,Project
[D] Job prep as a fresh PhD grad?,"Hi everyone,

I'm a fresh PhD grad in ML, on the job market as of now. I'm looking for both genAI and traditional ML roles. Does anyone here have experience applying to jobs in ML, and what resources did you find to be helpful? Thanks a ton in advance.",MachineLearning,21,9,1719429774.0,1dp73ct,Temporary_Study2354,https://www.reddit.com/r/MachineLearning/comments/1dp73ct/d_job_prep_as_a_fresh_phd_grad/,Discussion
[R] Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B,,MachineLearning,20,1,1718605452.0,1dhrfjk,hardmaru,https://arxiv.org/abs/2406.07394,Research
[D] Why Does CycleGAN work?,"I know this is an old model that is not super fashionable anymore, but does anyone know of any work showing how CycleGAN works? (ie this paper: https://arxiv.org/abs/1703.10593).  A long time ago, I tried applying the paper to a numerical problem, but couldn't get it to work. Learning a conditional distribution from two marginals seems like magic.  Does it work because the structure of images is so distinctive?  If anyone has any answers or research, it would interest me to learn more.  ",MachineLearning,20,16,1718223221.0,1degbf8,www3cam,https://www.reddit.com/r/MachineLearning/comments/1degbf8/d_why_does_cyclegan_work/,Discussion
[D] What are the Best Deep Learning Notebook Experiments you Seen? ,"I'm digging around for some of the best written publicly available deep learning experiments in Notebooks.

One of the cool one I found is this one on [Adversarial Attack](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/adversarial_fgsm.ipynb#scrollTo=wpYrQ4OQSYWk) using Fast Gradient Signed Method (FGSM) by Goodfellow.

What's your favorite?",MachineLearning,21,0,1718051559.0,1dcvsno,research_pie,https://www.reddit.com/r/MachineLearning/comments/1dcvsno/d_what_are_the_best_deep_learning_notebook/,Discussion
[D] Tips for Getting Used to Reading AI Research Papers for a Student,"Hello everyone,

I'm currently in my second year of a five-year computer science program, and I'm planning to delve into the field of AI next year. It's been a passion of mine for some time now, and I'm eager to go beyond the average expectations to deepen my knowledge. My current goal is to graduate in three years as an elite MLE with a profound mastery of the field.

I consulted one of my professors at university about how I could improve and immerse myself in AI before formally studying it, he strongly recommended that I start reading AI research papers to get used to it for the next few years. However, I find that diving into these papers can be quite daunting for a relatively novice student like myself (I only have basic knowledge in linear algebra, probabilities, and statistics, and some understanding of how ML algorithms work). So, I'm wondering if there are any tips or strategies you could share to help me get accustomed to this type of reading and make the most out of it.

Any suggestions, resources, or personal experiences you could share would be greatly appreciated.

Thanks in advance for your help",MachineLearning,21,13,1717685885.0,1d9k7rq,ratybox_,https://www.reddit.com/r/MachineLearning/comments/1d9k7rq/d_tips_for_getting_used_to_reading_ai_research/,Discussion
[D] Should data in different modalities be represented in the same space?,"As I've studied language AI primarily, I'm getting used to multimodal AI. However it seems training methodologies are so diverse, not to mention evaluating those are much more difficult imo. At least, I've thought data in different modalities should be represented different spaces. Is there any 'better method(maybe)' researchers agree?",MachineLearning,20,8,1716283507.0,1cx3pg9,Capital_Reply_7838,https://www.reddit.com/r/MachineLearning/comments/1cx3pg9/d_should_data_in_different_modalities_be/,Discussion
"[D] Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow 2nd Edition ","I bought this book when it came out and worked through a couple of chapters. I really enjoyed but ended up never finishing it but now that I actually have an opportunity to dedicate time to it, im wondering if it's up to date enough (it's from 2019) or of there would be a more recent book that covers similar topics. 

Any tips appreciated 👍",MachineLearning,22,17,1715407467.0,1cpalcs,ApplesAndAmazons,https://www.reddit.com/r/MachineLearning/comments/1cpalcs/d_handson_machine_learning_with_scikitlearn_keras/,Discussion
[R] AlphaMath Almost Zero: process Supervision without process,"**Paper**: [https://arxiv.org/abs/2405.03553](https://arxiv.org/abs/2405.03553)

**Code**: [https://github.com/MARIO-Math-Reasoning/Super\_MARIO](https://github.com/MARIO-Math-Reasoning/Super_MARIO)

**Model**: [https://huggingface.co/MARIO-Math-Reasoning/AlaphaMath-7B](https://huggingface.co/MARIO-Math-Reasoning/AlaphaMath-7B)

**Abstract**:

>Recent advancements in large language models (LLMs) have substantially enhanced their mathematical reasoning abilities. However, these models still struggle with complex problems that require multiple reasoning steps, frequently leading to logical or numerical errors. While numerical mistakes can largely be addressed by integrating a code interpreter, identifying logical errors within intermediate steps is more challenging. Moreover, manually annotating these steps for training is not only expensive but also demands specialized expertise. In this study, we introduce an innovative approach that eliminates the need for manual annotation by leveraging the Monte Carlo Tree Search (MCTS) framework to generate both the process supervision and evaluation signals automatically. Essentially, when a LLM is well pre-trained, only the mathematical questions and their final answers are required to generate our training data, without requiring the solutions. We proceed to train a step-level value model designed to improve the LLM's inference process in mathematical domains. Our experiments indicate that using automatically generated solutions by LLMs enhanced with MCTS significantly improves the model's proficiency in dealing with intricate mathematical reasoning tasks.",MachineLearning,20,0,1715251726.0,1cnu9mx,None,https://www.reddit.com/r/MachineLearning/comments/1cnu9mx/r_alphamath_almost_zero_process_supervision/,Research
[D] Any-dimensional equivariant neural networks,"I found this paper very interesting. We kind of make same assumptions, that the authors are making, while using covnet for computer vision. I was wondering can we extend for computer vision use cases 


Abstract
```
Traditional supervised learning aims to learn an unknown mapping by fitting a function to a set of input-output pairs with a fixed dimension. The fitted function is then defined on inputs of the same dimension. However, in many settings, the unknown mapping takes inputs in any dimension; examples include graph parameters defined on graphs of any size and physics quantities defined on an arbitrary number of particles. We leverage a newly-discovered phenomenon in algebraic topology, called representation stability, to define equivariant neural networks that can be trained with data in a fixed dimension and then extended to accept inputs in any dimension. Our approach is user-friendly, requiring only the network architecture and the groups for equivariance, and can be combined with any training procedure. We provide a simple open-source implementation of our methods and offer preliminary numerical experiments.
```",MachineLearning,19,4,1714857559.0,1ckat4d,No-Natural36,https://arxiv.org/abs/2306.06327v2,Discussion
[D] Importance of HPO per field / model type / applications,"I’ve noticed that the time spent on hyperparameter optimization vary significantly, not just between industry and academia but also across different fields like NLP, computer vision, or reinforcement learning. I’m curious—what’s your experience?

* Is tuning something you prioritize heavily, or do you often settle for “good enough” configurations to move faster?
* What field / model type / applications do you think experience most(or least) bottleneck in workflow due to HPO?
* Are there any industry dependency around choosing HPO tools? For example, everyone in xx industry would pick Optuna as a go-to or everyone running xx experiments would use Sigopt.

Would love to hear your experiences! Thanks",MachineLearning,18,7,1734073092.0,1hd6pjv,Maleficent_Ad5541,https://www.reddit.com/r/MachineLearning/comments/1hd6pjv/d_importance_of_hpo_per_field_model_type/,Discussion
[R] An Evolved Universal Transformer Memory,,MachineLearning,20,2,1733957010.0,1hc6bs8,hardmaru,https://arxiv.org/abs/2410.13166,Research
[D] Have we officially figured out yet how O1 models differ from previous models?,"Edit: I have misworded the title as if OpenAI would confirm how O1 was implemented. I have changed the text to reflect what I meant say.



I really want to deep dive into the technicals of how the O1 models perform better than previous models.

Have researchers come to any definitive agreement as to what OpenAI could have possible done to achieve O1?

From reading online I hear about MCTS, COT... etc, but are any of these methods in large agreement by researhers?
",MachineLearning,19,30,1733485071.0,1h7zfjg,Daveboi7,https://www.reddit.com/r/MachineLearning/comments/1h7zfjg/d_have_we_officially_figured_out_yet_how_o1/,Discussion
[P] A complete transformer model built in Excel,,MachineLearning,18,4,1732987545.0,1h3hj6j,Revolutionary-Way290,https://x.com/ProfTomYeh/status/1859282491955130452,Project
[D] X List to follow for ML research?,"Hey guys i'm just getting into Ml (been in the field for about 6 months now) and i want to keep up with it in a better way, but there's so much stuff to follow on X that im confused, any lists to recommend? ",MachineLearning,19,9,1730643669.0,1ginx35,jinstronda,https://www.reddit.com/r/MachineLearning/comments/1ginx35/d_x_list_to_follow_for_ml_research/,Discussion
[R] optimizing transformers,"Hello, I’m currently aiming to work on optimizing transformer models, specifically in multi-view images and/or cross-attention networks. I've noticed that cross-attention layers add up a lot of parameters, which can slow down the training process. I’m exploring ways to reduce the computational complexity to increase the speed (for now and subsequently without sacrificing too much performance sometime later). I'm starting to look into:

1. low-rank matrix factorization - I’ve been reading about how it can be applied to reduce the size of the projection matrices (e.g., the projq, projk, projv in cross-attention). Does anyone have experience using low-rank factorization specifically in cross-attention mechanisms?
2. other param reduction techniques - Aside from low-rank factorization, are there other methods I could explore for reducing the number of parameters in transformer models, like sparsity and pruning—do you have recommendations or experiences with these?
3. overcoming redundancy in multi-view scenarios - Given the multi-view nature of my problem, I suspect there’s some redundancy in how cross-attention processes the different views. Has anyone worked on reducing redundancy across views in transformer-based networks? What techniques worked best for you?

I’m starting to look into CVPR, NEURIPS, ECCV, etc, but any insights, advise, experiences, or papers you can share would be greatly appreciated! Thanks in advance!",MachineLearning,19,4,1727650063.0,1fsgz5i,Cool-Economy3492,https://www.reddit.com/r/MachineLearning/comments/1fsgz5i/r_optimizing_transformers/,Research
[D] machine learning system design,"I’m not into reading books but recently started reading this book, I’m just wondering if anyone else read this and found it useful. Is there any other book you’d recommend me to try next? I’d like to hear your thoughts. Thank you!",MachineLearning,19,17,1726361890.0,1fh0pb7,dcsr98,https://www.reddit.com/r/MachineLearning/comments/1fh0pb7/d_machine_learning_system_design/,Discussion
[R] Prompt Cache: Modular Attention Reuse for Low-Latency Inference,,MachineLearning,18,1,1724006442.0,1evgi49,AhmedMostafa16,https://arxiv.org/abs/2311.04934,Research
What type of model architecture is best suited for generative music? [D],"
In a previous life before $$$ became a necessity & I started software, I was a musician. I played guitar, piano, drums, have a solid understanding of music theory, and produced a few of my own tracks.

Not that much of that likely matters for building my own generative music model, but it definitely means my interest is there :)

Would anyone have recs where to start, specifically what type of model architecture? GPT is suggesting RNNs, Transformers, or VAEs. Has anyone experimented with various models & could give some guidance on architecture selection + how to process data appropriately?",MachineLearning,19,7,1723523347.0,1eqyz02,redditTee123,https://www.reddit.com/r/MachineLearning/comments/1eqyz02/what_type_of_model_architecture_is_best_suited/,Discussion
[R] Annotation Vocabulary (Might Be) All You Need,"Paper link: https://www.biorxiv.org/content/10.1101/2024.07.30.605924v1

Abstract:
>
Protein Language Models (pLMs) have revolutionized the computational modeling of protein systems, building numerical embeddings that are centered around structural features. To enhance the breadth of biochemically relevant properties available in protein embeddings, we engineered the Annotation Vocabulary, a transformer readable language of protein properties defined by structured ontologies. We trained Annotation Transformers (AT) from the ground up to recover masked protein property inputs without reference to amino acid sequences, building a new numerical feature space on protein descriptions alone. We leverage AT representations in various model architectures, for both protein representation and generation. To showcase the merit of Annotation Vocabulary integration, we performed 515 diverse downstream experiments. Using a novel loss function and only $3 in commercial compute, our premier representation model CAMP produces state-of-the-art embeddings for five out of 15 common datasets with competitive performance on the rest; highlighting the computational efficiency of latent space curation with Annotation Vocabulary. To standardize the comparison of de novo generated protein sequences, we suggest a new sequence alignment-based score that is more flexible and biologically relevant than traditional language modeling metrics. Our generative model, GSM, produces high alignment scores from annotation-only prompts with a BERT-like generation scheme. Of particular note, many GSM hallucinations return statistically significant BLAST hits, where enrichment analysis shows properties matching the annotation prompt - even when the ground truth has low sequence identity to the entire training set. Overall, the Annotation Vocabulary toolbox presents a promising pathway to replace traditional tokens with members of ontologies and knowledge graphs, enhancing transformer models in specific domains. The concise, accurate, and efficient descriptions of proteins by the Annotation Vocabulary offers a novel way to build numerical representations of proteins for protein annotation and design.


We are proud to announce the release of our latest work! Please give it a read, share, and ask any questions !",MachineLearning,19,2,1722448039.0,1egthqg,TeamArrow,https://www.reddit.com/r/MachineLearning/comments/1egthqg/r_annotation_vocabulary_might_be_all_you_need/,Research
[D] Is SAM2 the new SOTA in object tracking?,"I've tried most of the popular object tracking frameworks like bytetrack, botsort, deepsort (all using detectron and ultralytics-YOLOv8 backends for object detection) but none have performed nearly as good as what I tested in the SAM2 demo - [https://sam2.metademolab.com/demo](https://sam2.metademolab.com/demo) and this was ofcourse without any finetuning or messing around with the parameters.

I know it's not the main purpose of the model, and that production results might be slightly different. But could a combination of YOLO for fast object detection and classification, with SAM2 on top for tracking (annotate SAM2 tracking points with centres of YOLO bboxes) give you the best object detection + tracking?

P.S., Unlike me, if you've had better experiences with any object tracking frameworks, please share about it.",MachineLearning,19,9,1722334732.0,1efprgq,Eoncarry,https://www.reddit.com/r/MachineLearning/comments/1efprgq/d_is_sam2_the_new_sota_in_object_tracking/,Discussion
[D] Normalization in transformers,"After the [first](https://www.reddit.com/r/MachineLearning/comments/1e1sy5u/d_an_interesting_property_of_selfattention_layers) theoretical issue with my transformer, I now see another. The original paper uses normalization after residual addition (Post-LN), which led to training difficulties and later got replaced by normalization at the beginning of each attention or mlp block/branch (Pre-LN). This is known to work better in practice (trainable without warmup, restores highway effect), but it still doesn't seem completely ok theoretically.

First consider things without normalization. Assuming attention and mlp blocks are properly set up and mostly keep norms, each residual addition would sum two similar norm signals, potentially scaling up by something like 1.4 (depending on correlation, but it starts at sqrt(2) after random init). So the norms after the blocks could look like this: [1(main)+1(residual)=1.4] -> [1.4+1.4=2] -> [2+2=2.8] etc. This would cause various problems (like changing the softmax temp in later attention blocks), so adjustment is needed.

Pre-LN ensures each block works on normalized values (thus with constant - if slightly arbitrary - softmax temperature). But since it doesn't affect the norm of the main signal (as forwarded by the skip connection) but only the residual, the norms can still grow, albeit slower. The expectation is now roughly: [1+1=1.4] -> [1.4+1=1.7] -> [1.7+1=2] -> [2+1=2.2] etc - with a final normalization correcting the signal near output (Pre-LN paper).

One possible issue with this is that later attention blocks may have reduced effect, as they add unit norm residuals to a potentially larger and larger main signal. What is the usual take on this problem? Can it be ignored in practice? Does Pre-LN work acceptably despite it, even for deep models (where the main norm discrepancy can grow larger)? There are lots of alternative normalization papers, but what is the practical consensus?

Btw attention is extremely norm-sensitive (or, equivalently, the hidden temperature of softmax is critical). This is a sharp contrast to fc or convolution which are mostly scale-oblivious. For anybody interested: consider what happens when most raw attention dot products come out 0 (= query and key is orthogonal, no info from this context slot) with only one slot giving 1 (= positive affinity, after downscaled by sqrt(qk_siz) ). I for one got surprised by this during debug.",MachineLearning,19,8,1721979895.0,1ecict8,lostn4d,https://www.reddit.com/r/MachineLearning/comments/1ecict8/d_normalization_in_transformers/,Discussion
[D] Constantly updating knowledge graph,"I currently have a chat assistant that uses Neo4j's knowledge graph + GPT-4o to answer user queries. The results have been more than amazing so far and way better than I initially expected. Sometimes I even miss the hallucinations, but thats a good problem to have. 

I also implemented a vector database using MARQO, which takes over if the graph fails. This RAG stores historical messages paired with question and answer pairs. It works great as well, but not as good as knowledge graphs. I can easily update and add more content to the RAG system, but as far as I understand, constantly updating the knowledge graph would lead to a bunch of low-quality data that wasn't validated, and that will only lower the quality of the results. 

What would be the best way to address this so I could constantly keep updating the system with the newest data while not losing the great quality it produces now? Maybe the better choice would be to combine both of these systems and take 5 results from one and 5 from the other, creating a single system where I could only update the RAG with new data constantly and only update the knowledge graph from time to time so the data remains as good as possible? 

I want to automate it as much as possible, so I'm looking for the best solution.",MachineLearning,19,2,1721418387.0,1e7cluy,Matas0,https://www.reddit.com/r/MachineLearning/comments/1e7cluy/d_constantly_updating_knowledge_graph/,Discussion
[D] Why do DINO models use augmentations for the teacher encoder?,"As in title - DINO and DINOv2 use augmentations for inputs that go into the teacher networks. Why is this? Doesn't it make more sense to generate teacher representations from the ""cleanest"" possible version of the data? Would really appreciate getting to hear what the intuition is behind what they did.",MachineLearning,18,7,1719648445.0,1dr6aad,clywac2,https://www.reddit.com/r/MachineLearning/comments/1dr6aad/d_why_do_dino_models_use_augmentations_for_the/,Discussion
[N] ESM3: Simulating 500 million years of evolution with a language model,"Blog post: https://www.evolutionaryscale.ai/blog/esm3-release

Pre-print (pending approval): https://evolutionaryscale-public.s3.us-east-2.amazonaws.com/research/esm3.pdf

Abstract: 

> More than three billion years of evolution have
produced an image of biology encoded into the
space of natural proteins. Here we show that language models trained on tokens generated by evolution can act as evolutionary simulators to generate functional proteins that are far away from
known proteins. We present ESM3, a frontier
multimodal generative language model that reasons over the sequence, structure, and function
of proteins. ESM3 can follow complex prompts
combining its modalities and is highly responsive
to biological alignment. We have prompted ESM3
to generate fluorescent proteins with a chain of
thought. Among the generations that we synthesized, we found a bright fluorescent protein at far
distance (58% identity) from known fluorescent
proteins. Similarly distant natural fluorescent proteins are separated by over five hundred million
years of evolution

The first huge release by EvolutionaryScale after spinning off Meta. 

Weights and code are released, but with **big** caveats

From HuggingFace:

https://www.evolutionaryscale.ai/legal/community-license-agreement

**The Big Picture:**

The EvolutionaryScale AI Model is only available under this Community License Agreement for non-commercial use by individuals or non-commercial organizations.
You may not use the EvolutionaryScale AI Model or any derivative works of the EvolutionaryScale AI Model or its outputs:

a. in connection with any commercial activities, for example

b. to develop any product or service such as hosting the AI Model behind an API; or

c. in connection to drug development; or

d. without attribution to EvolutionaryScale and this Community License Agreement; or

e. to train any other large language model, any technology for protein representation learning or protein generation or any other AI-powered third party model similar to EvolutionaryScale’s AI Model, even for non-commercial usage.

You can publish, share and adapt the EvolutionaryScale AI Model and its outputs for non-commercial purposes in accordance with the Community License Agreement",MachineLearning,19,11,1719330017.0,1do91g9,TeamArrow,https://www.reddit.com/r/MachineLearning/comments/1do91g9/n_esm3_simulating_500_million_years_of_evolution/,News
[R] New book! Design a Machine Learning System (From Scratch),"Hello everybody and thank you all for giving us a chance to share our latest MEAP release with the community.

[Design a Machine Learning System (From Scratch)](https://mng.bz/2KRa) by Benjamin Tan Wei Hao, Shanoop Padmanabhan & Varun Mallya

Our latest MEAP release teaches how to design a reliable ML system from scratch. It incorporates MLOps and DevOps along with a stack of proven infrastructure tools including Kubeflow, MLFlow, BentoML, Evidently, and Feast. 

Throughout the book, you will construct a delivery pipeline for an image classifier and a recommendation system, while learning best practices.

Gain hands-on experience with essential parts of the machine learning workflow, including orchestrating pipelines, model training, serving, as well as monitoring and explainability.

  
🚀 Take action now! Save 50% with code retanweihao50

📖 Get into the book: [https://mng.bz/PZBR](https://shortener.manning.com/PZBR)

📹 Check out this video to find out: [https://mng.bz/1GZq](https://shortener.manning.com/1GZq)

Thanks for reading.

https://preview.redd.it/u7p5fakfm45d1.jpg?width=718&format=pjpg&auto=webp&s=f2d7cf88fbb3c0e72d64d1ee42ae047ecbbb2ca5

Cheers,",MachineLearning,19,2,1717756010.0,1da7cgx,ManningBooks,https://www.reddit.com/r/MachineLearning/comments/1da7cgx/r_new_book_design_a_machine_learning_system_from/,Research
[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",MachineLearning,19,55,1717340419.0,1d6f7ad,AutoModerator,https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/,Discussion
"[P] ASL ⭤ English Translation w/ MediaPipe, PointNet, ThreeJS and Embeddings","Hey! I'm Kevin Thomas, a Grade 11 student at Burnaby South Secondary School (also home to British Columbia School for the Deaf)!

Over the last few months, I have been developing a tool that translates between American Sign Language (ASL) and English. Most existing ASL translation tools are built on the misconception that ASL is the same language as English. Basically, they only view Deafness as a disability and only seek to overcome the inability to hear, but not to translate to the language of ASL itself.

With guidance from my ASL teacher, I have been working on a project that facilitates this translation while respecting and preserving ASL as the primary language. For ASL reception, I augmented over 100,000 images of ASL alphabets using Google MediaPipe and trained a PointNet model to classify handshapes fingerspelled by Deaf individuals. For ASL expression, I augmented over 9,000 videos of ASL signs, embedded their corresponding words, and then used ThreeJS to sign words said by hearing individuals. I also used LLMs to improve accuracy and translate between English and ASL grammar.

Here is a demo (and explainer) [YouTube video ](https://www.youtube.com/watch?v=uuPxMWQRoXc)

Here is the [GitHub repository](https://github.com/kevinjosethomas/sign-language-translation)

I only started looking into ML/AI over the last few months! I would appreciate any feedback, opportunities or resources to continue learning and growing! Feel free to reach out to me in Reddit DMs or at kevin.jt2007@gmail.com! Also liking this [Linkedin post](https://www.linkedin.com/posts/kevinjosethomas_deaf-deafcommunity-asl-activity-7200579484385746944-J_hW?utm_source=share&utm_medium=member_desktop) will go a long way 🙏🫶",MachineLearning,20,7,1716786902.0,1d1k3nw,TrustedMercury,https://www.reddit.com/r/MachineLearning/comments/1d1k3nw/p_asl_english_translation_w_mediapipe_pointnet/,Project
[R] Variational Inference: Reverse KL vs. Forward KL,"Hi all,

I'm working on variational inference methods, mainly in the context of BNNs. Using the reverse (exclusive) KL as the variational objective is the common approach, though lately I stumbled upon some interesting works that use the forward (inclusive) KL as an objective instead, e.g \[1\]\[2\]\[3\]. Also in the context of VI for GPs both divergence measures have been used, see e.g \[4\].

While I'm familiar with the well-known difference between the objectives that the reverse KL is 'mode-seeking' and the forward KL is 'mode covering', I see some of these works making claims about downstream differences of these VI objectives such as (paraphrasing here) *""the reverse KL underestimates predictive variance"" \[4\]* and *""the forward KL is useful for applications benefiting from conservative uncertainty quantification"" \[3\]*.

I'm interested in understanding these downstream differences in the context of VI, but haven't found any works that explain these claims theoretically instead of empirically. Anyone who can point me in the right direction or have a go at explaining this?

Cheers

\[1\] Naesseth, Christian, Fredrik Lindsten, and David Blei. ""Markovian score climbing: Variational inference with KL (p|| q)."" *Advances in Neural Information Processing Systems* 33 (2020): 15499-15510.

\[2\] Zhang, L., Blei, D. M., & Naesseth, C. A. (2022). Transport score climbing: Variational inference using forward KL and adaptive neural transport. *arXiv preprint arXiv:2202.01841*.

\[3\] McNamara, D., Loper, J., & Regier, J. (2024, April). Sequential Monte Carlo for Inclusive KL Minimization in Amortized Variational Inference. In *International Conference on Artificial Intelligence and Statistics* (pp. 4312-4320). PMLR.

\[4\] Bauer, M., Van der Wilk, M., & Rasmussen, C. E. (2016). Understanding probabilistic sparse Gaussian process approximations. *Advances in neural information processing systems*, *29*.",MachineLearning,20,18,1716474978.0,1cyubdt,DriftingClient,https://www.reddit.com/r/MachineLearning/comments/1cyubdt/r_variational_inference_reverse_kl_vs_forward_kl/,Research
[Research] How Can Understanding Sparse Autoencoders in Claude 3 Sonnet Influence Practical AI Applications?,"I recently read the  paper ""Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet"" by Anthropic. The study explores how sparse autoencoders can extract interpretable, multilingual, and multimodal features from transformer models. 

[https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)  - paper link

Given that these features influence both the detection and generation of specific types of data (like text or images), I’m curious about the practical applications of this capability:

How can this  level of feature understanding help in customizing model outputs for specific tasks without extensive retraining? For example, could we steer a model more effectively during deployment based on identified features? Can this eliminate/identify/mitigate bias? ",MachineLearning,20,9,1716413726.0,1cyc0zs,mamphii,https://www.reddit.com/r/MachineLearning/comments/1cyc0zs/research_how_can_understanding_sparse/,Research
[N] Book Lauching: Accelerate Model Training with PyTorch 2.X ,"Hello everyone! My name is Maicon Melo Alves and I'm a High Performance Computing (HPC) system analyst specialized in AI workloads.

I would like to announce that my book ""**Accelerate Model Training with PyTorch 2.X: Build more accurate models by boosting the model training process**"" was recently launched by Packt.

This book is for intermediate-level data scientists, engineers, and developers who want to know how to use PyTorch to accelerate the training process of their machine-learning models.

If you think this book can help other professionals, please share this post with your community! 😊

Thank you very much!",MachineLearning,18,4,1715354641.0,1cos6bq,Various_Protection71,https://www.reddit.com/r/MachineLearning/comments/1cos6bq/n_book_lauching_accelerate_model_training_with/,News
[D] Fun little discovery: Gemini is surprisingly bad at following simple number sequences,"Try this: tell it to reply to you with the next number in a sequence, starting from 1, then you reply with the next one, and so on. After a few messages, it starts to output text instead of numbers, and after about 20 messages, it completely fails to keep following the sequence. 

The original goal was to find out how good is its long-term memory, which seems to be pretty bad. Given its huge context window, how can it lose focus so quickly?",MachineLearning,19,19,1715187132.0,1cn9ejf,ifilipis,https://www.reddit.com/r/MachineLearning/comments/1cn9ejf/d_fun_little_discovery_gemini_is_surprisingly_bad/,Discussion
How are large network attack datasets made? [p],"Hi, I’m working on a ML system for network intusion detection. I’ve come across huge free datasets that have been really helpful but I’ve come to a point in my project where I need to make my own. I see the millions of simulated attacks on a network and can’t imagine that this is sone by hand. If anyone has any ideas it would be appreciated. Thanks",MachineLearning,19,4,1714841208.0,1ck4ozs,OpeningDirector1688,https://www.reddit.com/r/MachineLearning/comments/1ck4ozs/how_are_large_network_attack_datasets_made_p/,Project
[R] Measuring Vision-Language STEM Skills of Neural Models,"Authors: Jianhao Shen, Ye Yuan, Srbuhi Mirzoyan, Ming Zhang, Chenguang Wang

* **Paper (Accepted by ICLR 2024):** [https://arxiv.org/abs/2402.17205](https://arxiv.org/abs/2402.17205)
* **Leaderboard:** [https://huggingface.co/spaces/stemdataset/stem-leaderboard](https://huggingface.co/spaces/stemdataset/stem-leaderboard)
* **Dataset:** [https://huggingface.co/datasets/stemdataset/STEM](https://huggingface.co/datasets/stemdataset/STEM)
* **Code:** [https://github.com/stemdataset/STEM](https://github.com/stemdataset/STEM)

**Abstract:** We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding of multimodal vision-language information of STEM. Our dataset features one of the largest and most comprehensive datasets for the challenge. It includes 448 skills and 1,073,146 questions spanning all STEM subjects. Compared to existing datasets that often focus on examining expert-level ability, our dataset includes fundamental skills and questions designed based on the K-12 curriculum. We also add state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our benchmark. Results show that the recent model advances only help master a very limited number of lower grade-level skills (2.5% in the third grade) in our dataset. In fact, these models are still well below (averaging 54.7%) the performance of elementary students, not to mention near expert-level performance. To understand and increase the performance on our dataset, we teach the models on a training split of our dataset. Even though we observe improved performance, the model performance remains relatively low compared to average elementary students. To solve STEM problems, we will need novel algorithmic innovations from the community.

https://preview.redd.it/wf7ssbf6llxc1.png?width=2430&format=png&auto=webp&s=871c18ed4ca64a3b35f4bdc99bb4f2ce50a201bb",MachineLearning,17,1,1714474528.0,1cgpy6q,shizue_yy,https://www.reddit.com/r/MachineLearning/comments/1cgpy6q/r_measuring_visionlanguage_stem_skills_of_neural/,Research
[D] UAI-2024 results waiting area,"Following the review phase([old post](https://www.reddit.com/r/MachineLearning/comments/1bt0nor/d_uai_2024_reviews_waiting_place/)), creating a thread for others like me waiting for the decision.

All the best!",MachineLearning,19,35,1714059537.0,1ccv3r8,PaganPasta,https://www.reddit.com/r/MachineLearning/comments/1ccv3r8/d_uai2024_results_waiting_area/,Discussion
"[D] How do you interpret GLU ""activations""?","I've been asking myself how to interpret GLU and GLU variants such as those common in modern Transformers.

I can see 2 layers MLPs activated by ReLU both as linear projections of nonlinear projections (from a vector space to a positive cone in another vector space, to another vector space), as well as sets of keys (weights to hidden neurons) and values (weights from hidden to output units), which is nice when compared to Attention and associative memories.

How do you interpret GLUs and FFNs with GLU variants? 
I can see a 3D vector being projected to another 3D vector (first linear transform) and being gated i.e. possibly projected to lie onto planes normal to the axes.
But I have a very hard time in seeing how the original vector determines both the intermediate vector and the shrinking/flattening of the sigmoid gate.
Other activation functions on the gate make it even harder.

What simple logic functions or geometric transformations can be implemented by a minimal GLU on 2-3 units, compared to a classic 2layerMLP?",MachineLearning,17,6,1735295637.0,1hnc7d9,Sad-Razzmatazz-5188,https://www.reddit.com/r/MachineLearning/comments/1hnc7d9/d_how_do_you_interpret_glu_activations/,Discussion
[D] encoder free vision language models?,"Happy holidays! Any interesting papers on encoder free VLMs? Recently looking at video VLMs and one biggest headache is encoder efficiency. Also, the end to end quality is very much limited to the quality of the visual encoder which is usually a CLIP style model. There are the Fuyu model series but this architecture doesn't seem to perform that well. There is a recent NeurlPS paper: [https://github.com/baaivision/EVE](https://github.com/baaivision/EVE) which looks interesting. Looking for comments and recommendations on this direction of work.

",MachineLearning,19,4,1735153481.0,1hm6qpu,encoreway2020,https://www.reddit.com/r/MachineLearning/comments/1hm6qpu/d_encoder_free_vision_language_models/,Discussion
[R] Improving Recommendations by Calibrating for User Interests,"Traditional recommendation systems often prioritize relevance, leading to overfitting on popular or primary interests and ignoring diversity. This article explores the paper '*Calibrated Recommendations as a Minimum-Cost Flow Problem*', a novel approach to calibrating recommendations by modeling the problem as a minimum-cost flow optimization. By balancing relevance with category-based user interest distributions, the system ensures variety without sacrificing quality. Experiments show this method outperforms greedy and baseline models, especially for smaller recommendation sets.

Full article here: [https://www.shaped.ai/blog/improving-recommendations-by-calibrating-for-user-interests](https://www.shaped.ai/blog/improving-recommendations-by-calibrating-for-user-interests)",MachineLearning,19,0,1734616414.0,1hhtd5e,skeltzyboiii,https://www.reddit.com/r/MachineLearning/comments/1hhtd5e/r_improving_recommendations_by_calibrating_for/,Research
[D] WWW 2025 Reviews (TheWebConference),The reviews will be available soon. This is a thread for discussion/rants. Be polite in comments.,MachineLearning,17,67,1733175280.0,1h56hno,New_Ice_2721,https://www.reddit.com/r/MachineLearning/comments/1h56hno/d_www_2025_reviews_thewebconference/,Discussion
[P] PerpetualBooster outperforms AutoGluon on AutoML benchmark,"
PerpetualBooster is a GBM but behaves like AutoML so it is benchmarked also against AutoGluon (v1.2, best quality preset), the current leader in [AutoML benchmark](https://automlbenchmark.streamlit.app/cd_diagram). Top 10 datasets with the most number of rows are selected from [OpenML datasets](https://www.openml.org/). The results are summarized in the following table for regression tasks:

| OpenML Task                                  | Perpetual Training Duration | Perpetual Inference Duration                                      | Perpetual RMSE | AutoGluon Training Duration | AutoGluon Inference Duration                                      | AutoGluon RMSE |
| -------------------------------------------- | --------------------------- | ----------------------------------------------------------------- | -------------- | --------------------------- | ----------------------------------------------------------------- | -------------- |
| [Airlines_DepDelay_10M](openml.org/t/359929) | 518                         | 11.3                                                              | 29.0           | 520                         | 30.9 | 28.8   |
| [bates_regr_100](openml.org/t/361940)        | 3421                        | 15.1 | 1.084  | OOM            | OOM                         | OOM                                                               |
| [BNG(libras_move)](openml.org/t/7327)        | 1956                        | 4.2 | 2.51   | 1922           | 97.6                        | 2.53                                                              |
| [BNG(satellite_image)](openml.org/t/7326)    | 334                         | 1.6                                                               | 0.731          | 337                         | 10.0 | 0.721  |
| [COMET_MC](openml.org/t/14949)               | 44                          | 1.0 | 0.0615  | 47             | 5.0                         | 0.0662                                                            |
| [friedman1](openml.org/t/361939)             | 275                         | 4.2 | 1.047   | 278            | 5.1                         | 1.487                                                             |
| [poker](openml.org/t/10102)                  | 38                          | 0.6 | 0.256   | 41             | 1.2                         | 0.722                                                             |
| [subset_higgs](openml.org/t/361955)          | 868                         | 10.6 | 0.420  | 870            | 24.5                        | 0.421                                                             |
| [BNG(autoHorse)](openml.org/t/7319)          | 107                         | 1.1 | 19.0    | 107            | 3.2                         | 20.5                                                              |
| [BNG(pbc)](openml.org/t/7318)                | 48                          | 0.6 | 836.5   | 51             | 0.2                         | 957.1                                                             |
| average                                      | 465                         | 3.9                                                               | -              | 464                         | 19.7                                                              | -              |

PerpetualBooster outperformed AutoGluon on 8 out of 10 datasets, training equally fast and inferring 5x faster. The results can be reproduced using the automlbenchmark fork [here](https://github.com/deadsoul44/automlbenchmark).

Github: https://github.com/perpetual-ml/perpetual",MachineLearning,17,0,1733166746.0,1h52zk8,mutlu_simsek,https://www.reddit.com/r/MachineLearning/comments/1h52zk8/p_perpetualbooster_outperforms_autogluon_on/,Project
[R] Best chunking method for PDFs with complex layout?,"I am working on a RAG based PDF Query system , specifically for complex PDFs that contains multi column tables, images, tables that span across multiple pages, tables that have images inside them.

I want to find the best chunking strategy for such pdfs.

Currently i am using RecursiveCharacterTextSplitter. What worked best for you all for complex PDF?

",MachineLearning,16,5,1733124261.0,1h4pkmh,ElectronicHoneydew86,https://www.reddit.com/r/MachineLearning/comments/1h4pkmh/r_best_chunking_method_for_pdfs_with_complex/,Research
[D] ACL Rolling Review October 2024,Discussion thread for ACL 2024 (ARR Oct) reviews.,MachineLearning,17,121,1732392079.0,1gy8ekt,AffectionateTip521,https://www.reddit.com/r/MachineLearning/comments/1gy8ekt/d_acl_rolling_review_october_2024/,Discussion
[D] How an efficient applied ML team is structured?,"Hi Everyone,

I am interested in your experience on how big(ger) ML teams are structured that are working well for companies that are building with ML (companies who use ML in multiple domains and they cover CV, NLP, ...)?
I tried to search for it, but there is not much info on efficient team structure. While structure can be defined by the company culture, I am sure you've seen patterns on how this can work well.

(I think a big team is at least 80 people with POs/PMs).

The most basic (and maybe the best?) is when the domains are divided (CV, NLP, etc.) where every domain has a lead and multiple seniors, mediors, juniors. Then besides the ML engineers, there is a separate division who work with the productization (creating rest APIs, etc.), which includes devops, and SWEs. 

",MachineLearning,19,4,1731868915.0,1gtke1b,gabegabe6,https://www.reddit.com/r/MachineLearning/comments/1gtke1b/d_how_an_efficient_applied_ml_team_is_structured/,Discussion
[P] I'm Fine Tuning a model fully trained on AdamW with SOAP optimizer and improved my validation loss by 5%,"Just wanted to share this Soap Optimizer, I'm really surprised how well is working on my project, it's a computer vision model that use Gradient Accumulation and it's managed to improve the training on it.

Paper: [https://arxiv.org/abs/2409.11321](https://arxiv.org/abs/2409.11321)

Code: [https://github.com/ClashLuke/SOAP/tree/patch-1](https://github.com/ClashLuke/SOAP/tree/patch-1)",MachineLearning,18,5,1730987694.0,1glqypg,CloverDuck,https://www.reddit.com/r/MachineLearning/comments/1glqypg/p_im_fine_tuning_a_model_fully_trained_on_adamw/,Project
[Project] Open source video indexing/labelling/tag generation tool.,"Guys, I'm looking for an open source tool or any repo that can help me generate tags for video to categorize multiple videos and do further analysis.

An equivalent of what I want is Azure AI clvideo inxer, but If there was such a open source tool, it will solve the problem.",MachineLearning,18,4,1729916313.0,1gccyhp,jokingwizard,https://www.reddit.com/r/MachineLearning/comments/1gccyhp/project_open_source_video_indexinglabellingtag/,Project
[D] What are some of the most interesting conferences for real world (applied) ML talks?,I know that for information retrival and recommender systems RecSys and SIGIR offer a few industry talks and I was wondering if you know about other ones where it's mostly about findings/research from the industry.,MachineLearning,17,6,1729179921.0,1g5txyp,gabegabe6,https://www.reddit.com/r/MachineLearning/comments/1g5txyp/d_what_are_some_of_the_most_interesting/,Discussion
[D] Understanding the loss in the CLIP model,"I am looking closely at the [CLIP paper](https://arxiv.org/abs/2103.00020). In Section 2.3 paragraph 4, it says: 

>  jointly training an image encoder and text encoder to maximize the cosine similarity of the image and text embeddings of the N real pairs
in the batch while minimizing the cosine similarity of the embeddings of the N^2 − N incorrect pairings. 

Looking at the [code from MLFoundations](https://github.com/mlfoundations/open_clip/blob/main/src/open_clip_train/train.py), line 290 onwards,

```
logits_per_image = logit_scale * image_features @ text_features.t()
logits_per_text = logits_per_image.t()

total_loss = (
	F.cross_entropy(logits_per_image, labels) +
        F.cross_entropy(logits_per_text, labels)
        ) / 2
```

The [alternative implementation, from Revant](https://github.com/revantteotia/clip-training/blob/main/train.py) has very similar code (lines 88 onwards)

My question is very simple - 

Do these lines of code correspond to what the paper states as maximizing the cosine similarity of the N real pairs and minimizing that of the other N^2 - N incorrect pairs? If this is indeed the case, please help me understand briefly how it is so. 
",MachineLearning,17,14,1728978672.0,1g42hnu,datashri,https://www.reddit.com/r/MachineLearning/comments/1g42hnu/d_understanding_the_loss_in_the_clip_model/,Discussion
[R] Is Mamba and SSMs on Language Modelling Task a Great Research Trajectory?,"I just came by Mamba and SSMs as my Professor said that I should try to explore it. I am a master's student for context and I just started my research journey, I originally wanted to do research on transformers LM like the rest of the students in my department do. Someone said that this traps me into doing something that someone hasn't done before and will make my study/research harder than it is supposed to be (and maybe end up yielding mediocre results). Do you guys have any opinion regarding this? Thank you.

",MachineLearning,18,25,1728276068.0,1fxzor7,worthlesspineapple,https://www.reddit.com/r/MachineLearning/comments/1fxzor7/r_is_mamba_and_ssms_on_language_modelling_task_a/,Research
[D] Creativity only comes from reinforcement learning?,"An interesting talk by Ilya on the role of RL in forming creative responses by LLMs or any AI system (e.g. AlphaZero for chess). I was wondering if this was really the case? I would think simply interpolating between data points from something like SFT would be creative as well.

Link to talk: [https://www.youtube.com/watch?v=OPZxs6IXH00&list=PLpvkFqYJXcreXgK6Cg9NVGvFANmdUczWa](https://www.youtube.com/watch?v=OPZxs6IXH00&list=PLpvkFqYJXcreXgK6Cg9NVGvFANmdUczWa)

(minute 14:00)

",MachineLearning,18,6,1726837775.0,1flbmde,CriticalTemperature1,https://www.reddit.com/r/MachineLearning/comments/1flbmde/d_creativity_only_comes_from_reinforcement/,Discussion
"[D] Kaggle competitions get owned by AI agents, possible?","I tried a Kaggle competition https://www.kaggle.com/competitions/playground-series-s3e19 on Google's Data Science Agent tool - basically I just dumped the description as prompt and uploaded the datasets there, and it generated this Jupyter notebook: https://colab.research.google.com/drive/17DkaHhcdiURHPtYBZoRvoDE9NaSzn4V4


I also tried it on ChatGPT but unfortunately I don't have Plus so the task was terminated in the middle (no model was trained). Anyone with Plus tried kaggle tasks on ChatGPT? Wondering how long will we see a bot win the competition, I imagine RL would play a huge role here.",MachineLearning,19,32,1726723091.0,1fkde5a,caterpillarous,https://www.reddit.com/r/MachineLearning/comments/1fkde5a/d_kaggle_competitions_get_owned_by_ai_agents/,Discussion
[D] How to Efficiently Store Pruned Weight Matrices in Practice?,"Hi everyone,

I’m currently working on pruning a neural network to make it more efficient by eliminating some connections (setting some weights to zero). However, I’m struggling with how to efficiently store these pruned weight matrices.

I understand that PyTorch, for example, supports storing sparse matrices, which works by keeping track of the non-zero values and their corresponding indexes. But here’s my concern: doesn’t storing the indexes of the non-zero weights negate some of the space-saving benefits? For instance, if half of the matrix consists of non-zero values, wouldn’t the saved space be offset by the need to store the indexes of these values?

Am I missing something about how pruning should work in practice, especially for cases where I have around 50% non-zero values in a matrix? How do you typically implement pruning in practice to actually save storage space? Any advice or suggestions on how to store these matrices efficiently would be greatly appreciated.

Thanks in advance!

TL;DR: How do you efficiently store pruned weight matrices without losing the space savings due to storing indexes for the non-zero values?
",MachineLearning,19,23,1726188462.0,1ffi51d,scarlettgarnett,https://www.reddit.com/r/MachineLearning/comments/1ffi51d/d_how_to_efficiently_store_pruned_weight_matrices/,Discussion
[R] AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation,,MachineLearning,17,2,1723689727.0,1esk9q4,hardmaru,https://arxiv.org/abs/2408.00764,Research
[D] Modeling a dynamic system using LSTM,"Dear all,

after looking this very well made [video](https://youtu.be/av8csD_yrgw?si=tOZs1-x-CVO2IyJz) about the modeling of a dynamical system using RNN and LSTM I decided to model my real system using the same concept. Basically I want to model the dynamic of my real robot in order to create a ""digital twin"" of the same. In other words, I want to recreate the same robot in a simulator, with virtual physics properties and move it as it were real.

My robot is driven using a joystick which output on every axes a float between -1.0 and 1.0. I collected the data (the real robot hat sensors already working and implemented). For simplification, let's say, that I want to drive the following joint coordinate by moving my joystick axis from left to right (Fig. 1).

[Fig. 1](https://preview.redd.it/gbmji1gjkwhd1.jpg?width=997&format=pjpg&auto=webp&s=e2f388f0bd7ed8065e44da7413d6b5fe6e52c518)

I collected one hour long the data, then I trained a LSTM with a hidden size of 32 using the following data:

* The input is a concatenation of the joystick input and the joint coordinate (the state of the robot)
* The target is representation by the state of the robot in the next step. I simply copied the columns of the state and shifted it one unit backward. Fig.2 shows probably better then 1000 words.

[Fig. 2](https://preview.redd.it/382ceklkkwhd1.jpg?width=270&format=pjpg&auto=webp&s=328fb9617e32fe34d41d41cd122d34e498b03428)

Then I created sequences of lenght 200 and trained my LSTM.  
The training converged very quickly and I was quite happy with the results. But somehow the virtual robot reacts strangely in the virtual environment. It jumps from one position to another with incredible speed and then moves very slowly. So it is not reacting as the real robot would move (the real one is more smooth during the movement).

Am I missing something important in this kind of problem?  
What should I still consider, in order to create a good digital twin of the real robot?

Note aside:

* despite the example above, I normalized all the movements into the range \[-1, 1\] or \[0, 1\]
* all the data has been collected using ethernet cable (so no delay due to wireless communication and so on)
* I used the LSTM classes of PyTorch and not a custom realization
* The data were collected by generating sinusoids inputs with different frequency and covering all the range of the joint.
* For the training I shuffled the data: randomly  a start index was chosen and a sequence of 200 element was cut and used for training.",MachineLearning,18,16,1723324525.0,1ep3kra,WilhelmRedemption,https://www.reddit.com/r/MachineLearning/comments/1ep3kra/d_modeling_a_dynamic_system_using_lstm/,Discussion
[D] RTX 4090 vs L40S for Server,"Hey everyone, our company is in the process of getting a new server for AI tasks, and we're debating between two GPU configurations:

1. 4x RTX 4090
2. 2x L40S

We're aware that the 4090s will have higher power consumption and a larger environmental footprint, but we're still considering them due to their cost.

our main use cases:

* Running and fine-tuning small LLMs for classification, embedding, and reranking tasks
* Potentially running and fine-tuning decoder-only models (llama, gemma, phi...)
* The GPUs will be in a Kubernetes environment

currently we are in a cpu setting in all our machines, so it is quiet a big of a change for us - therefore we want to make sure that we are not missing any important aspects.

we'd appreciate insights on the advantages of the L40S in a datacenter setting. What factors might we be overlooking? Are there any datacenter-specific features of the L40S that would be particularly beneficial for our workload?

Any guidance or experiences you can share would be greatly appreciated. Thanks!!",MachineLearning,18,32,1722941497.0,1elenk8,None,https://www.reddit.com/r/MachineLearning/comments/1elenk8/d_rtx_4090_vs_l40s_for_server/,Discussion
[R] Deep Learning Paper Summaries,"The Vision Language Group at IIT Roorkee has written comprehensive summaries of deep learning papers from various prestigious conferences like NeurIPS, CVPR, ICCV, ICML 2016-24. A few notable examples include:

* DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation, CVPR'23 [https://github.com/vlgiitr/papers\_we\_read/blob/master/summaries/DreamBooth.md](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/DreamBooth.md)
* Segment Anything, ICCV'23 [https://github.com/vlgiitr/papers\_we\_read/blob/master/summaries/Segment\_Anything.md](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Segment_Anything.md)
* An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion, ICVR'23 [https://github.com/vlgiitr/papers\_we\_read/blob/master/summaries/Textual\_inversion.md](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Textual_inversion.md)
* Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding, NIPS'22 [https://github.com/vlgiitr/papers\_we\_read/blob/master/summaries/imagen.md](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/imagen.md)
* An Image is Worth 16X16 Words: Transformers for Image Recognition at Scale, ICLR'21 [https://github.com/vlgiitr/papers\_we\_read/blob/master/summaries/Vision\_Transformer.md](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Vision_Transformer.md)
* Big Bird: Transformers for Longer Sequences, NIPS'20 [https://github.com/vlgiitr/papers\_we\_read/blob/master/summaries/Big\_Bird\_Transformers.md](https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Big_Bird_Transformers.md)

If you found the summaries useful you can contribute summaries of your own. The [repo](https://github.com/vlgiitr/papers_we_read) will be constantly updated with summaries of more papers from leading conferences.",MachineLearning,18,0,1719569392.0,1dqg67i,vlg_iitr,https://www.reddit.com/r/MachineLearning/comments/1dqg67i/r_deep_learning_paper_summaries/,Research
Human-Like Intelligence Exhibiting Models that are Fundamentally Different from Neural Networks [D],"I've always been interested in computers and technology. Ever since I began learning to code (which was about three years ago), the field of AI always fascinated me. At that time, I decided that once I gained enough knowledge about programming, I would definitely dive deeper into the field of AI. The thought of programming a computer to not only do something that it has been explicitly instructed to do but to learn something on its own ""intelligently"" seemed super interesting.

Well, about two months ago, I began learning about actual machine learning. I already had enough knowledge about linear algebra, multi-variable calculus, and other concepts that are prerequisites for any typical ML course. I also implemented algorithms like *k-means clustering*, *k-nearest neighbours*, *linear regression*, etc, both from scratch and using scikit-learn. About a month ago, I began studying deep learning. As I kept reading more material and learning more about neural networks, I came to the rather insipid realization that an artificial neural network is just an *n*-dimensional function, and ""training"" a neural network essentially means minimizing an *n*-dimensional loss function, *n* being the number of features in the dataset. I will grudgingly have to say the approach to ""train"" neural networks didn't quite impress me. While I did know that most of AI was just mathematics veiled behind the façade of seemingly clever and arcane programming (that's what I thought of ML before I began diving into the nooks and crannies of ML), I did not expect DL to be what it is. (I'm struggling to describe what I expected, but this definitely wasn't it.)

I see that the model of an ANN is inspired by the model of our brain and that it is based on the Hebbian theory. A complete ANN consists of at least an input layer, an output layer, and optionally, one or multiple hidden layers, all of which are ordered. A layer is an abstract structure that consists of more elementary abstract structures called neurons — a layer may have a single or multiple neurons. Each neuron has two associated numerical values: a weight and a bias, which are the parameters of the neuron and the ANN. An input to a neuron is multiplied by its associated weight; then, the bias is added to that result, and the sum is then inputted to an activation function; the output from the activation function is the output of the neuron. The training starts by feeding the training data into the input layer; from there, it goes into the hidden layer(s), and then finally gets to the output layer where each neuron corresponds to a particular class (I have no knowledge about how ANNs are used for regression, but I believe this is true for classification tasks). The loss is calculated using the final outputs. In order to minimize the loss, the weights and biases of all the neurons in the network are adjusted using a method called gradient descent. (I wish to include the part about backpropagation, but I currently do not have a concrete understanding of how it works and its purpose.) This process is repeated until the network converges upon an optimal set of parameters. After learning about the universal approximation theorem, I see and understand that through this process of adjusting its parameters, an ANN can, in theory, learn any function. This model, and extensions to this model like convolutional neural networks and recurrent neural networks can do certain tasks that make it seem that they exhibit human-like intelligence.

Now, don't get me wrong — I appreciate the usefulness and effectiveness of this technology and I am grateful for the role it plays in our daily lives. I certainly do find it interesting how connecting several abstract structures together and then using them to process data using a mathematical technique can bring about a system that outperforms a skilled human in completing certain tasks. Given all this, I natural question one would ask is ""Are there any other models that are fundamentally different from ANNs, i.e., models that do not necessarily use neurons, an ensemble of neuron-like structures connected together, or resemble an ANN's architecture, that can outperform ANNs and potentially exhibit human-like intelligence?"". Now that ANNs are popular and mainstream, they are the subject of research and improvement by AI researchers all around the world. However, they didn't quite take off when they were first introduced, which may be due to a myriad of reasons. Are there any obscure and/or esoteric ideas that seemed to have the same or even greater potential than neural networks but did not take off? Lastly, do you think that human-like intelligent behaviour has such an irreducible complexity that a single human may never be able to understand it all and simulate it using a computer program for at least the next 200 years?

 Note(s):

* Since there is no universally agreed-upon definition of the term ""intelligence"", I will leave it to the reader to reasonably interpret it according to what they deem suitable in the given context.",MachineLearning,14,69,1719399013.0,1dovn4n,kris_2111,https://www.reddit.com/r/MachineLearning/comments/1dovn4n/humanlike_intelligence_exhibiting_models_that_are/,Discussion
[D] How do ecomm companies like Amazon and Walmart generate complementary recommendations (or frequently bought together) nowadays?,I’ve recently started looking into recommendation systems. I know it was done using some statistical algorithm like FP-Growth and maybe Prod2Vec in the past.,MachineLearning,19,7,1719286219.0,1dnwfis,Abs0lute_Jeer0,https://www.reddit.com/r/MachineLearning/comments/1dnwfis/d_how_do_ecomm_companies_like_amazon_and_walmart/,Discussion
[P] AgileRL - evolutionary RLOps for state-of-the-art deep reinforcement learning,"Hi, I've posted before about our evolutionary hyperparameter optimization for reinforcement learning achieving SOTA results, but I'd like to share that our open-source framework has now had its v1.0.0 release!  
Please check it out! [https://github.com/AgileRL/AgileRL](https://github.com/AgileRL/AgileRL)

This library is initially focused on reducing the time taken for training models and hyperparameter optimization by pioneering evolutionary HPO techniques for reinforcement learning. Evolutionary HPO has been shown to drastically reduce overall training times by automatically converging on optimal hyperparameters, without requiring numerous training runs.

We are constantly adding more algorithms and features. AgileRL already includes state-of-the-art evolvable on-policy, off-policy, offline, multi-agent and contextual multi-armed bandit reinforcement learning algorithms with distributed training.

I'd love to get your feedback!",MachineLearning,18,2,1718992180.0,1dla20p,nicku_a,https://www.reddit.com/r/MachineLearning/comments/1dla20p/p_agilerl_evolutionary_rlops_for_stateoftheart/,Project
[R] [D] Sanity Check on use of biLSTM for time series prediction,"TLDR; [this paper](https://www.sciencedirect.com/science/article/abs/pii/S0893608022003938) uses biLSTM in a published paper and I think it violates causality.

Hi, I am struggling to convince myself I am not going mad. I am looking at [this paper](https://www.sciencedirect.com/science/article/abs/pii/S0893608022003938) published in Neural Networks, an Elsevier journal. In this paper they use a bidirectional LSTM model (+ some other novel stuff) to predict time series. This seems fundamentally wrong as biLSTM cannot/should not be used for time series prediction.

The best known use case for biLSTM is translating a phrase word by word when the entire sentence is known in advance. In this case the preceding and succeeding words can influence the meaning and so the translation of a focal word. A silly example would be translating this into Spanish

*I need a* ***shot***, *I got bitten by a dog*

If you are scanning through each word in turn to translate, you might suggest w\_4 (= 'shot') would translate to 'inyeccion' i.e. a vaccination. Knowing that w\_10 = 'dog' would have important predictive value here.

Likewise

*I need a* ***shot***, *let's go to a bar!*

w\_4 would probably translate to 'chupito' for a shot of booze because w\_9 = 'bar' has an influence.

So you can and should use a biLSTM here so you can scan what comes before and after the word to know the context. However, for a time series prediction, you don't know the future! The future cannot affect the present without violating causality. In the translation example the sentence is in fact already created in the person's head before the say/write it so the later words don't violate causality.

However in this paper they use biLSTM on general time series benchmarks and it seems totally unscientific! AM I missing something?",MachineLearning,18,28,1718975289.0,1dl3lui,rutherfordofman,https://www.reddit.com/r/MachineLearning/comments/1dl3lui/r_d_sanity_check_on_use_of_bilstm_for_time_series/,Discussion
[N] How good do you think this new open source text-to-speech (TTS) model is?,"Hey guys,  
This is Arnav from CAMB AI we've spent the last month building and training the 5th iteration of MARS, which we've now open sourced in English on Github [https://github.com/camb-ai/mars5-tts](https://github.com/camb-ai/mars5-tts)

I've done a longer post on it on Reddit [here](https://www.reddit.com/r/CAMB_AI/comments/1day7ta/introducing_mars5_opensource_insanely_prosodic/). We'd really love if you guys could check it out and let us know your feedback. Thank you!",MachineLearning,18,28,1718018506.0,1dcj439,MrHumun,https://www.reddit.com/r/MachineLearning/comments/1dcj439/n_how_good_do_you_think_this_new_open_source/,News
AI in biotech. [D],"Wondering if in biotech AI is a **marketing buzzword or it's have working applications**.

I am ML Engineer, not Biologist at all. I've found several AI solutions in biotech, but all of them seems not really bringing value, but more like trend. I hope I am wrong, but couldn't find good evidences.  
Solutions that I found are Material gen, Microsoft; Alpha Fold, DeepMind; EvBio.",MachineLearning,19,52,1717594088.0,1d8pxsw,IIISergeyIII,https://www.reddit.com/r/MachineLearning/comments/1d8pxsw/ai_in_biotech_d/,Discussion
[D] GT for Depth Estimation: LiDAR vs Stereo Depth?,"Why is it that most benchmarks for depth estimation (like nuScenes, KITTI, DDAD, ...) have ground truth depths from a LiDAR sensor instead from stereo depth of 2 cameras?  
Having cameras mounted on the mirrors of a car results in a baseline distance of ~2m. This would enable way denser depth measurements, with similar distance to SOTA LiDARs. I don't get why this isn't used more often - or am I missing something?",MachineLearning,18,13,1716918643.0,1d2pmr8,topsnek69,https://www.reddit.com/r/MachineLearning/comments/1d2pmr8/d_gt_for_depth_estimation_lidar_vs_stereo_depth/,Discussion
"Failing to replicate 'Deep Residual Learning' ""[P]""","Hi Everyone,

for learning purposes I've been replicating the methods from Kaiming He's 2015 ['Deep Residual Learning for Image Recognition'](https://arxiv.org/pdf/1512.03385). I've built the VGG inspired plain-CNN as well as the ResNet architectures (standard & bottleneck).

However, I have been unable to replicate the degradation (saturation of accuracy) problem highlighted in the publication.  The %-error figures in the publication show clear drops in %-error as training progresses followed by stagnation.

My figures appear to stagnate, but its clear the model is generalizing horribly to the validation data. I've included one of their figures as reference. Any recommendations to better replicate the error rate saturation from this paper? Note: For Kaiming He figure, bold lines are testing error & dashed are training.

Parameters:

* 162 Epochs w/ batch size of 128 for 64k iterations.
* Lr: 0.1
* Momentum: 0.9
* Weight decay: 0.0001
* Multi-step scheduler dividing the lr by 10 at 32k and 48k iterations

[My &#37;-error](https://preview.redd.it/du9j19ay322d1.png?width=729&format=png&auto=webp&s=15d461e662e5ca823bef852b73626c5cb3f043c8)

[From Paper](https://preview.redd.it/zsvtfxb0422d1.png?width=701&format=png&auto=webp&s=019edcbf04a5eff3ffb0902fbb3057fd501e9dd4)

**Edit:** Adjustment to training transformation by implementing normalization prior to random cropping & re-ran all models. First image is change to plain 18 error rate curve, second is all error rates for tested architectures.

https://preview.redd.it/g7gpcuql482d1.png?width=1000&format=png&auto=webp&s=0db10a04a16585a0401c56a7f4c2ae25f9f8249b

https://preview.redd.it/7d35ztql482d1.png?width=1000&format=png&auto=webp&s=a83604401d1971bc7472d0b08231985a222ba8f4

**Edit 2**: First, thank you everyone so much for the suggestions you have already made!

The following recommendations were implemented: padding of 4 actually implemented, Resize() removed as images are already 32x32, horizontal flip implemented before RandomCrop. I've included the resulting Plain18, Plain34, and cumulative Error Graphs. I'm not quite sure what is happening w/ Plain34...

Here is the codebase: [https://github.com/AnotherBotIGuess/CIFAR\_ResNet](https://github.com/AnotherBotIGuess/CIFAR_ResNet)  
I apologize for the poor code cleanliness...

https://preview.redd.it/3j60d23w1e3d1.png?width=1000&format=png&auto=webp&s=335b68987921b9f58f960d6160ce990318fdecbc

https://preview.redd.it/fzbid9e02e3d1.png?width=1000&format=png&auto=webp&s=09e78d54257270c0b19e3ba3f6a2cd048cf758de

https://preview.redd.it/9y93rzu02e3d1.png?width=1000&format=png&auto=webp&s=0726489d0ad7ca68730f492ddcf3496533898783",MachineLearning,18,29,1716418114.0,1cydq8g,AnotherBotIGuess,https://www.reddit.com/r/MachineLearning/comments/1cydq8g/failing_to_replicate_deep_residual_learning_p/,Project
[D] Does DSPy actually change the LM weights? ,"I always thought it's essentially glorified and structured prompt engineering (very useful still IMO), but it also claims in the docs that it fine-tunes and changes LM weights, and then absolutely refuses to elaborate on this in any of the sections in their docs.

I don't even understand how it can change the actual parameters of the LM, especially if we're using third party API calls for the LMs. 

By LM weights, I assume it means the weights of the last layers of the transformer model. When they describe optimizers, they say ""DSPy introduces new optimizers, which are LM-driven algorithms that can tune the prompts and/or the weights of your LM calls, given a metric you want to maximize.""

Am I misunderstanding what they mean by LM weights?

I'm sorry if this is a stupid question, but I just can't seem to find any information about this. Thanks in advance!",MachineLearning,18,3,1716137920.0,1cvsviu,chessnudes,https://www.reddit.com/r/MachineLearning/comments/1cvsviu/d_does_dspy_actually_change_the_lm_weights/,Discussion
[R] LLM4ED: Large Language Models for Automatic Equation Discovery,"**Paper**: [https://arxiv.org/abs/2405.07761](https://arxiv.org/abs/2405.07761)

**Abstract**:

>Equation discovery is aimed at directly extracting physical laws from data and has emerged as a pivotal research domain. Previous methods based on symbolic mathematics have achieved substantial advancements, but often require the design of implementation of complex algorithms. In this paper, we introduce a new framework that utilizes natural language-based prompts to guide large language models (LLMs) in automatically mining governing equations from data. Specifically, we first utilize the generation capability of LLMs to generate diverse equations in string form, and then evaluate the generated equations based on observations. In the optimization phase, we propose two alternately iterated strategies to optimize generated equations collaboratively. The first strategy is to take LLMs as a black-box optimizer and achieve equation self-improvement based on historical samples and their performance. The second strategy is to instruct LLMs to perform evolutionary operators for global search. Experiments are extensively conducted on both partial differential equations and ordinary differential equations. Results demonstrate that our framework can discover effective equations to reveal the underlying physical laws under various nonlinear dynamic systems. Further comparisons are made with state-of-the-art models, demonstrating good stability and usability. Our framework substantially lowers the barriers to learning and applying equation discovery techniques, demonstrating the application potential of LLMs in the field of knowledge discovery.",MachineLearning,18,2,1715766589.0,1csgx30,None,https://www.reddit.com/r/MachineLearning/comments/1csgx30/r_llm4ed_large_language_models_for_automatic/,Research
[R]  Matryoshka representation learning (MRL) for CLIP (& SigLip) ,"MRL \[1\] for CLIP allows smaller dimension embeddings to be used without loss in fidelity. Training is modified to optimize for truncated embeddings (multiple target dimensions at once) across both vision and text encoders.  

  
Key findings:

* Reducing embeddings size by 4x retains \~95 performance
* Projection layers for sub-embeddings did not help performance
* Works in and out (zero-shot) of domain on multi-modal retrieval
* Using too many sub-embeddings degrades performance (i.e. {512, 256, 128} vs {512, 256, 128, 64, 32, 16, 8}
* The number of sub-embeddings impacts convergence (same as above)
* Works with rank-tuning methods like GCL
* Relative importance (weighting, wi) of sub-dimensions matters (e.g. w1\*L\_512 + w2\*L\_256 + w3\*L\_128)
* MRL trained models can improve if the original sized embedding is used. i.e. performance is improved even if the smaller embeddings are not used.

Article:  
[https://www.marqo.ai/blog/matryoshka-representation-learning-with-clip-for-multimodal-retrieval-and-ranking](https://www.marqo.ai/blog/matryoshka-representation-learning-with-clip-for-multimodal-retrieval-and-ranking)

\[1\] MRL [https://arxiv.org/abs/2205.13147](https://arxiv.org/abs/2205.13147)",MachineLearning,19,1,1715734947.0,1cs8fzb,Jesse_marqo,https://www.reddit.com/r/MachineLearning/comments/1cs8fzb/r_matryoshka_representation_learning_mrl_for_clip/,Research
[P] Classification finetuning experiments on small GPT-2 sized LLMs,"I ran a few classification finetuning experiments on relatively ""small"" experiments that I found interesting and wanted to share:

|Model|Weights|Trainable token|Trainable layers|Context length|CPU/GPU|Training time|Training acc|Validation acc|Test acc|
|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|
|1|gpt2-small (124M)|pretrained|last|last\_block|longest train ex. (120)|V100|0.39 min|96.63%|97.99%|
|2|gpt2-small (124M)|pretrained|first|last\_block|longest train ex. (120)|V100|0.37 min|78.46%|80.54%|
|3|gpt2-small (124M)|pretrained|last|last\_layer|longest train ex. (120)|V100|0.33 min|78.65%|87.25%|
|4|gpt2-small (124M)|pretrained|last|all|longest train ex. (120)|V100|0.94 min|99.62%|96.64%|
|5|gpt2-medium (355M)|pretrained|last|last\_block|longest train ex. (120)|V100|0.91 min|87.50%|51.01%|
|6|gpt2-large (774M)|pretrained|last|last\_block|longest train ex. (120)|V100|1.91 min|99.52%|98.66%|
|7|gpt2-small (124M)|random|last|all|longest train ex. (120)|V100|0.93 min|100%|97.32%|
|8|gpt2-small (124M)|pretrained|last|last\_block|context length (1024)|V100|3.24 min|83.08%|87.92%|

1. Training the Last vs. First Output Token (row 1 vs 2): Training the last output token results in significantly better performance compared to the first. This improvement is expected due to the causal self-attention mask.
2. Training the Last Transformer Block vs. Last Layer (row 1 vs 3): Training the entire last transformer block is much more effective than training only the last layer.
3. Training All Layers vs. Last Transformer Block (row 1 vs 4): Training all layers shows a modest improvement of 2% over just training the last transformer block, but it requires almost three times longer in terms of training duration.
4. Using Larger Pretrained Models (row 1 vs 5, and row 1 vs 6): Employing a 3x larger pretrained model leads to worse results. However, using a 5x larger model improves performance compared to the initial model, as was anticipated.
5. Using a Model with Random Weights vs. Pretrained Weights (row 1 vs 7): Utilizing a model with random weights yields results that are only slightly worse by 1.3% compared to using pretrained
6. Padding Input to Full Context Length vs. Longest Training Example (row 1 vs 8): Padding the input to the full supported context length results in significantly worse

 

If you want to run these experiments yourself or try additional ones, here's a link to the code on GitHub: [https://github.com/rasbt/LLMs-from-scratch/tree/main/ch06/02\_bonus\_additional-experiments](https://github.com/rasbt/LLMs-from-scratch/tree/main/ch06/02_bonus_additional-experiments)",MachineLearning,20,7,1714221098.0,1cedfub,seraschka,https://www.reddit.com/r/MachineLearning/comments/1cedfub/p_classification_finetuning_experiments_on_small/,Project
[D] HyenaDNA and Mamba are not good at sequential labelling ?,"Hello guys, I've been working on a sequential labelling using DNA sequences as inputs. Lately there have been 2 foundation models released HyenaDNA (Based on Hyena operator) and Caduceus (based on mamba), I used both pretrained and from scratch models and performances are terrible even with pretrained ones. 

Does anyone have experience with this type of models, and what are the potential causes for performance drop ? I am literally getting zero performance for the minority class ? Does mamba deal poorly with class imbalance ?

",MachineLearning,18,13,1714075351.0,1cd13kf,blooming17,https://www.reddit.com/r/MachineLearning/comments/1cd13kf/d_hyenadna_and_mamba_are_not_good_at_sequential/,Discussion
Automated generation of categories for classification [D],"So I can use Bart zero-shot classification to quantify the relevance of an article to a predefined set of categories but I have a bunch of articles and I want to compute categories from them and then use those categories to classify lots of articles.

I thought maybe I could convert each article to a vector using a text embedding and then use an unsupervised learning algorithm to compute clusters of related articles and then project the groups back into text, maybe by recursively summarizing the articles in each group. However, I don't actually want the constraint that sets of categories must be disjoint which, I think, k-means would impose.

How else might this be accomplished?",MachineLearning,16,11,1734914817.0,1hkc5d1,PurpleUpbeat2820,https://www.reddit.com/r/MachineLearning/comments/1hkc5d1/automated_generation_of_categories_for/,Discussion
"[D] Model performs good on test, but fails in production ","Hi, I’ve developed churn prediction model with XGBoost on users weekly activity data. The training data is balanced (3.3k churned, 3k not churned). I’ve split the data into: train, validation and test sets. Getting ~90% precision & ~88% recall for train, validation and test sets. However, when running in production, I get ~1.5k users flagged as churn (we have total of 4k users). This can’t be true as we get maximum 250 churned users per month. Any suggestions on what I’m doing wrong? And what could be the solution?

Thanks ",MachineLearning,18,54,1733232655.0,1h5nfpt,Terrible_Dimension66,https://www.reddit.com/r/MachineLearning/comments/1h5nfpt/d_model_performs_good_on_test_but_fails_in/,Discussion
[N][R] Models are what they eat: automatic data curation for LLMs,"Sharing our most recent work at [DatologyAI](http://www.datologyai.com). Models are what they eat, and our mission is to make data curation for training large models as effective and easy as possible. 

Combining a bevy of approaches, including heuristic filters, model-based filters, embedding based curation, synthetic data, target distribution matching, and mixing ratios, we were able to massively improve training efficiency, performance, and inference efficiency. 

Comparing to our baseline and starting dataset -- exact deduplicated RedPajamav1, we can: 

* Reach the same performance 7.7x faster (and 3.4x faster than DCLM)
* Improve performance across benchmarks by 8.5% (and by 4.4% over DCLM)
* Train models with fewer than half the parameters which outperform larger models by >5% 

Check out our [high-level results here](https://www.datologyai.com/post/train-llms-faster-better-and-smaller-with-datologyai-s-data-curation), and if you want all the nitty-gritty details, check out our[ technical deep dive](https://www.datologyai.com/post/technical-deep-dive-curating-our-way-to-a-state-of-the-art-text-dataset).",MachineLearning,17,1,1732900557.0,1h2qmol,arimorcos,https://www.reddit.com/r/MachineLearning/comments/1h2qmol/nr_models_are_what_they_eat_automatic_data/,News
[R][D]Test time training for abstract reasoning,"[https://arxiv.org/pdf/2411.07279](https://arxiv.org/pdf/2411.07279)

By the way guys, do you know of any research on trying to slightly fine-tune a model on the question it is asked before having it answer? I mean it would probably work for in-context information retrieval, but I was wondering about its impact on more reasoning-heavy tasks. The compute overhang would be huge, still.",MachineLearning,19,6,1731712529.0,1gs9lao,None,https://www.reddit.com/r/MachineLearning/comments/1gs9lao/rdtest_time_training_for_abstract_reasoning/,Discussion
[D] NeurIPS After Dark Networking Event,"Just got an email about an official ticketed after dark NeurIPS networking event - this will be my first time attending/presenting, wondering if these events are worth going to. More generally, also interested in hearing about how to make the most of my time attending.",MachineLearning,18,7,1731379333.0,1gpamvn,gateofptolemy,https://www.reddit.com/r/MachineLearning/comments/1gpamvn/d_neurips_after_dark_networking_event/,Discussion
How do we move beyond neural networks [Discussion]?,"Hi there! I am currently a student, and have been working with NNs for a few years now. While I'm not denying that neural networks and their derivatives have been revolutionary (LLMs and the like), I can't help but feel like we're going to hit a brick wall soon with neural networks. To me, it feels like we need an entirely new approach, one that is better suited to the computers we have currently, to move to the next generation of models and AI. Is there any progress being made in such a direction (if so can you please mention it here), and what do you think is going to be the next step? Again, this is my opinion. I haven't been working on NNs for a lifetime, so would love to hear the community's thoughts on this. 

Clarification, by moving beyond NNs, my thought is that we don't model neurons and architectures after the human brain, but rather something different that doesn't rely on artificial neurons at all. (Again, don't know how it might be possible, which is why I am looking forward to hearing your thoughts). 

To me it feels like modeling neural networks after the human brain is inefficient because we are trying to imitate biology as it is the best thing we have. It's like if humanity developed a mechanical horse because the horse is the best method of transport in nature, instead of focusing our efforts on developing a car which our current tech is more suited to (just an example). Also, the recent incremental updates to LLMs and stuff seems to suggest that training larger models is not going to justify the immense amounts of data and resources that we put in very soon.

Personally, I think we should continue evolving neural networks to see where we hit the limit, and then hopefully we will have explored enough to know why they won't work for more advanced stuff, after which we can work on the next steps. Maybe we can even take the best parts of NNs and incorporate them into newer architectures.

Looking forward to hearing your thoughts on this. Once again, if you have any interesting new research regarding non NN based AI, can you please link them below? Thanks in advance.",MachineLearning,14,97,1729619262.0,1g9o9x3,mopasha1,https://www.reddit.com/r/MachineLearning/comments/1g9o9x3/how_do_we_move_beyond_neural_networks_discussion/,Discussion
[P]  New release for the World's *LEAST* popular LLM evaluation tool! ,"Just released a new version of [Ollama Grid Search](https://github.com/dezoito/ollama-grid-search), with [downloads](https://github.com/dezoito/ollama-grid-search/releases) for all major platforms.

According to some wise-guy on Discord, it's ""cute"" and ""laughable"", so make sure you don't miss out on the fun!

If you have no idea what this is, it's a Desktop open source app that allows you to:

* Evaluate multiple prompts and model combinations in a single operation
* Evaluate multiple combinations of parameters to verify the effect on inference outputs.

https://preview.redd.it/lcqqqco9fbwd1.png?width=1080&format=png&auto=webp&s=e8db16b7f980acbfd96adb65ccb6633cf52d3e93

If you are already a user (thank you!), here's the changelog for version 0.6.0:

# Added

* Added UI controls to re-run past experiments.
* Added controls to remove experiment files from the UI.
* Added button to copy an inference text to the clipboard.

# Changed

* Moved ""reload"" icon to improve layout.
* Improved experiment inspection UI readability.
* Streamlined State management.

# Fixes

* Fix HMR not working on MacOS (in development, of course).",MachineLearning,17,6,1729606222.0,1g9j2e7,grudev,https://www.reddit.com/r/MachineLearning/comments/1g9j2e7/p_new_release_for_the_worlds_least_popular_llm/,Project
[D] Non Fortune 500 ML use cases? ,"It feels really hard to find examples of mid level companies using ML in their every day operations.

The best success I've had is going to agencies websites and looking at their ""Case Studies"" section. Although these often omit a lot of data about the project itself.

Where do you go to find inspiration for whats possible in your industry? ",MachineLearning,15,13,1729026696.0,1g4ikg2,eh-tk,https://www.reddit.com/r/MachineLearning/comments/1g4ikg2/d_non_fortune_500_ml_use_cases/,Discussion
[D] NeurIPS 2024 Review Question ,"My initial reviewers addressed some weaknesses & concerns, but these were resolved in my rebuttals. They acknowledged and raised their score. 

My paper was ultimately rejected because the program chair introduced new weaknesses that are a result of misreading the paper, if these were stated in the original reviews, this would easily be resolved. Is there anything I can do to fix this program chair review?",MachineLearning,16,7,1727285443.0,1fpa7ua,sqweeeeeeeeeeeeeeeps,https://www.reddit.com/r/MachineLearning/comments/1fpa7ua/d_neurips_2024_review_question/,Discussion
[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",MachineLearning,17,16,1726366512.0,1fh23n3,AutoModerator,https://www.reddit.com/r/MachineLearning/comments/1fh23n3/d_selfpromotion_thread/,Discussion
[D] Time Series Forecasting: How do practitioners choose the best model?,"Asking forecasting practitioners out here -- when you use an AutoML for forecasting models, do you generally trust the model it suggests, or do you run ""a few best ones"" to figure out the one that suits you the most? I am asking this because AutoML models seem to have an accuracy-based focus; they would return the best model that would result in the best score as per the metric of your choice. But many times, correct me if I am wrong, these metrics may not directly help decide the best model for a practitioner. I was wondering what approach is used in general towards this.

NB: I understand many cloud-based forecasting services do not explicitly mention the model being chosen. However, how would you go about it if you were to run such a thing locally?

Thanks!",MachineLearning,17,12,1726206613.0,1ffnl2g,americast,https://www.reddit.com/r/MachineLearning/comments/1ffnl2g/d_time_series_forecasting_how_do_practitioners/,Discussion
[R] Methods for Pattern Matching with Multivariate Time series?,"Hi All,

I am trying to determine if a pattern in my vehicle dynamics is similar to other (multiple) vehicle dynamics patterns. For example, lets say I have a section of data that is for 5 seconds that represents swerving. How could I look through the data of a complete drive cycle of a trip to see if this swerving (or similar to an extent) occurs in this trip?

I have developed a couple methods to do this already, but I was wondering if there is something I should read up on so I'm not reinventing the wheel here!

Thanks for any help or guidance!",MachineLearning,18,17,1725890446.0,1fcq4v4,PreviousResearcher50,https://www.reddit.com/r/MachineLearning/comments/1fcq4v4/r_methods_for_pattern_matching_with_multivariate/,Research
[D] What are some good textbooks to get a deeper understanding on Control Theory?,Would like to learn more about it since it seems to be very relevant to Machine Learning broadly,MachineLearning,18,5,1725329990.0,1f7orq7,rulerofthehell,https://www.reddit.com/r/MachineLearning/comments/1f7orq7/d_what_are_some_good_textbooks_to_get_a_deeper/,Discussion
[D] Monthly Who's Hiring and Who wants to be Hired?,"**For Job Postings** please use this template

>Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]

**For Those looking for jobs** please use this template

>Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]

&#x200B;

Please remember that this community is geared towards those with experience.",MachineLearning,17,9,1725071415.0,1f5cy0v,AutoModerator,https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/,Discussion
[D] WACV 2025 Paper Reviews,WACV 2025 paper reviews are supposed to be released today. A discussion thread to discuss would be useful!,MachineLearning,16,87,1725027797.0,1f4wj0j,smokeriffs,https://www.reddit.com/r/MachineLearning/comments/1f4wj0j/d_wacv_2025_paper_reviews/,Discussion
[P] Looking for a gradient descent approach,"I had an idea for an approach to gradient descent that tries to 'jump' directly to a (predicted) location of a nearby minimum. It works by approximating the 2nd-5th order Taylor polynomial around a point, then solving for the minimum (if possible) and setting that point as the new x. Then, the process can be repeated. If the Taylor polynomial at any point is concave, then we can use more standard gradient descent methods.  
  
This seems like a rather simple approach, so I doubt it is novel, but I haven't been able to find anything like it online. Does anyone know what this approach is called or if it has been studied?

I was inspired by Newton's method for finding roots and a mild disdain for hyperparameter tuning.

  
Here are desmos demos for the quadratic and cubic Taylor approximations:

Quadratic Descent: [https://www.desmos.com/calculator/i2nsjaxzhy](https://www.desmos.com/calculator/i2nsjaxzhy)

Cubic Descent: [https://www.desmos.com/calculator/kgkbcfdn7t](https://www.desmos.com/calculator/kgkbcfdn7t)",MachineLearning,16,26,1723330356.0,1ep5od0,IgorTheMad,https://www.reddit.com/r/MachineLearning/comments/1ep5od0/p_looking_for_a_gradient_descent_approach/,Project
[R] Pre-prompting your LLM increases performance,"Research done at UoW shows that pre-prompting your LLM, or providing context prior to asking your question leads to better results. Even when the context is self generated.

[https://arxiv.org/pdf/2110.08387](https://arxiv.org/pdf/2110.08387)

For example asking,

""What should I do while in Rome?""

is less effective than a series of prompts,

""What are the top restaraunts in Rome?""

""What are the top sight seeing locations in Rome?""

""Best things to do in Rome""

""What should I do in Rome?""

I always figured this was the case from anecdotal evidence but good to see people who are way starter than me explain it in this paper. And while chain prompting is a little more time consuming there's chrome extensions like ChatGPT Queue that ease up the process.

Are their any other ""hacks"" to squeeze out better performance ?",MachineLearning,17,14,1721853203.0,1ebbtq7,CalendarVarious3992,https://www.reddit.com/r/MachineLearning/comments/1ebbtq7/r_preprompting_your_llm_increases_performance/,Research
"[D] Author of ReFT: Representation Finetuning for Language Models, at Oxen.ai Paper Club this Friday","Arxiv paper first author **Zhengxuan Wu** will join **Greg Schoeninger** in this Friday's  [Oxen.AI](http://Oxen.AI) Paper Club to explain how editing representations can be better than Parameter-efficient finetuning (PEFT) methods. [https://lu.ma/oxen](https://lu.ma/oxen)

**ReFT**: Representation Finetuning for Language Models.

Greg, 3 questions, 1 comment from reading just the abstract.

1. What exactly is meant by a ""representation""?     i.e., what part of the neural network is captured by what this paper refers to as a representation?
2. What is meant by an intervention in ""task specific intervention"" ?   I haven't heard that term before with pretraining or fine-tuning.
3. In API terms, would the point of this paper be like saying:    *""Instead of improving a RESTful API by improving the fidelity (depth) or breadth of sniff-able inputs and outputs, we will improve the API by directly changing the guts of the code for all existing inputs and outputs?""*

**Comment**) The abstract makes this paper sound very voodoo.   Hope testing was apples :: apples.

Look forward to your demystification on Friday, Greg.  So cool that you have the paper's first author joining you to help explain and answer questions.

**Deets**:

[https://lu.ma/oxen](https://lu.ma/oxen)

Friday July 19, 10:00 AM Pacific, 1:00 PM Eastern Time on Zoom

Paper: [https://arxiv.org/pdf/2404.03592](https://arxiv.org/pdf/2404.03592)

Gratitude:   Thank you Greg, u/FallMindless3563, Scott Howard u/sthoward, and the Oxen team for giving me an Easy button and for sharing your knowledge with the community while providing cool tools to curate datasets at oxen.ai.",MachineLearning,18,4,1721219561.0,1e5h1m8,ReluOrTanh,https://www.reddit.com/r/MachineLearning/comments/1e5h1m8/d_author_of_reft_representation_finetuning_for/,Discussion
[D] Ideas on how to improve time series forecasting with unknown data,"Hi all,
My company decided to add an analytical suite as part of our offering and I was tasked with creating a prediction solution.

My problem starts with the fact that I do not know what data I will be getting. It can be monthly financial aggregations like revenue, and it can be daily sales data.

I currently use implementwtions of ETS, SARIMAX, Holt-Winters and N-beats (just in case). I do automatic hyperparameter tuning with an expanding window, and then just pick the model with the best MAPE.

As for preprocessing, I remove outliers which are not seasonal and use Savitzky-Golay filters before feeding the data to the models.

Any suggestions regarding how to make this less of a Hail Mary?",MachineLearning,17,18,1721064009.0,1e40guh,Ok_Bottle2306,https://www.reddit.com/r/MachineLearning/comments/1e40guh/d_ideas_on_how_to_improve_time_series_forecasting/,Discussion
[R] Taxonomy for Data Transformations in AI Systems,"We spent a few years time writing the feature store paper just published in SIGMOD'24 (https://dl.acm.org/doi/10.1145/3626246.3653389) and one of its main contributions is the Data Transformation Taxonomy for AI.

The insight of the taxonomy is that not all data transformations in AI systems are equivalent. Some data transformations (aggregations, binning, data compression) produce features that can be reused in many models. Some data transformations (feature encoding/scaling, LLM text encoding) are specific to one model. Some data transformations in real-time AI systems require data only available at request-time.

As the research paper is very dense, here is a longer, more pedagogical version of the taxonomy:  
[https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems)",MachineLearning,17,0,1720076225.0,1dv0kgr,jpdowlin,https://www.reddit.com/r/MachineLearning/comments/1dv0kgr/r_taxonomy_for_data_transformations_in_ai_systems/,Research
Any cloud providers with 1 H100 allowing profiling? [D],"
Hello, does anyone know of a GPU cloud provider which 

* Rents a single H100 (as opposed to 8)
* Allows collecting profiling data as might be used by `ncu` to analyze kernel performance.

For instance, AWS and Lightning allow collecting profiling data, but I believe Lambda does not.

Update: The one that worked for me was nebius.ai",MachineLearning,18,15,1719938139.0,1dtq8hn,imurme8,https://www.reddit.com/r/MachineLearning/comments/1dtq8hn/any_cloud_providers_with_1_h100_allowing/,Discussion
[D] Deep Learning Project Hardware Requirements with $2K budget: large and complex dataset,"Although it's been more than 8 months since I got into the field of applied machine learning (and deep learning in particular) for the sake of defending my thesis on an ECG analysis algorithm, I have yet to figure out the hardware requirements for an optimal setup that would take into consideration an intelligent use of the research grant of two thousand dollars.

I'm not a US citizen, and our country does not have Nvidia suppliers. My laptop is weak with an Intel core i3 processor and 4GB of RAM. My options within the country are to either buy a new laptop or get a workstation for a little less than twice the price of a 16GB RAM and core i7 laptop. But I have read elsewhere that laptops aren't a great option for heavy DL projects, although I was thinking about the possibility of using an SSD to increase memory and time efficiency. Google Collaboratory seemed like a good option at first, but it has limitations when tackling such large projects, especially with the processing of data.

I have to apply deep learning to the complex dataset of electrocardiogram signals and my field of study is biomedical engineering which takes little account of these topics. It would be appreciated to get an insightful response to not blunder with the money. Much thanks for your time and consideration in reading this far.",MachineLearning,17,19,1719484249.0,1dpo1p2,r_agate,https://www.reddit.com/r/MachineLearning/comments/1dpo1p2/d_deep_learning_project_hardware_requirements/,Discussion
[D] Decoder only models for classification,"I was asked this in a couple of interviews - How do you decide between encoder only and decoder only architecture for a classification task?

From my understanding, encoder only architectures are used as they can fully capture the meaning within the text and adding classification layers on top of this representation would then give a good performance on classification tasks.

But the follow up question was regarding why decoders can't be considered since models like GPT seem to perform well on classification as well? I am unsure how to answer this since encoder only looks like an intuitive choice. Models like GPT perform well on a number of tasks including classification as they are trained on vast amounts of data. If the only data available to train a model from scratch is the classification dataset, can the decoder be used in this case as well?",MachineLearning,16,12,1719364499.0,1domam7,PretendBelt3387,https://www.reddit.com/r/MachineLearning/comments/1domam7/d_decoder_only_models_for_classification/,Discussion
[D] Difference between ICLR and AISTATS,"There is a somewhat [duplicated question](https://www.reddit.com/r/MachineLearning/comments/olmq3m/d_difference_between_aaai_iclr_and_aistats/) here, but I would like to bring this topic up again, since the September/October deadline is approaching and things might have changed now.

The big 3 ML conferences are ICML/NeurIPS/ICLR which divide the year into 3 deadlines. However, AISTATS also has a decent reputation. The deadlines of ICLR and AISTATS are quite close, so many people have to decide to submit their work to which of them.

ICLR rises so quickly because of popularity in deep learning (DL), but people nowadays seem to treat it the same as ICML/NeurIPS, and there seem to be quite some non-DL and theoretical ML papers there.

**Questions:** For purely empirical DL papers, it seems like a no-brainer to submit them to ICLR. What about (1) ML papers that have more theoretical results, or (2) ML papers with no DL (e.g., statistical ML)? **What are the pros and cons of submitting these works to ICLR and AISTATS?** Some aspects to consider:

* For these kinds of work, will AISTATS carry less prestige or receive less attention from the ML community?
* How are the experiences of submitting theoretical works to ICLR? (E.g., will reviewers there ask for many experiments?)
* Does industry/academia count more for ICLR, or treat ICLR and AISTATS the same (for more theoretical works)?

**Disclaimer:** Please stop saying ""the work itself is more important than the publication venue"", which is obviously true but not terribly informative.",MachineLearning,18,4,1719325455.0,1do796a,zy415,https://www.reddit.com/r/MachineLearning/comments/1do796a/d_difference_between_iclr_and_aistats/,Discussion
[D] Memory mechanism for Transformers,Hey folks! I am wondering what interesting work has been done to add a short term memory mechanism to transformers? Does someone know what the important work in this area is?,MachineLearning,16,18,1718994578.0,1dlb0wj,Janos95,https://www.reddit.com/r/MachineLearning/comments/1dlb0wj/d_memory_mechanism_for_transformers/,Discussion
[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",MachineLearning,17,102,1718550016.0,1dh9f6b,AutoModerator,https://www.reddit.com/r/MachineLearning/comments/1dh9f6b/d_simple_questions_thread/,Discussion
Looking for Time Series Resources [P],"Hello,

I am a Data Scientis with 5-10 years of working experience. I recently switched industry and now I have a lot of time series data challenges.

I want to read a book, take a course or take any other means to refresh and improve my knowledge on the topic.

I am interested in getting a deeper understanding of state of the art time series techniques.

(Back during studies, we delved from econometrics POV into ARIMA and VAR models. But there for sure are newer techniques that relate more to ML algorithm stack like LSTM or even CNN (ROCKET). And wtf is time warping.)

Can you recommend anything, that suits my needs?
",MachineLearning,17,5,1718187630.0,1de38nr,Antalagor,https://www.reddit.com/r/MachineLearning/comments/1de38nr/looking_for_time_series_resources_p/,Project
"[D] Can other areas researches such as the recent mapping of a cubic millimeter of a human brain tissue, help the Machine Learning field? ","https://www.scientificamerican.com/article/a-cubic-millimeter-of-a-human-brain-has-been-mapped-in-spectacular-detail/

Can the researches surrounding the human brain, such as this latest map of a human brain provide some insight for the Machine Learning field, in order to build more efficient models of AI/algorithms?

Lay person here.",MachineLearning,18,26,1717191635.0,1d56zg0,None,https://www.reddit.com/r/MachineLearning/comments/1d56zg0/d_can_other_areas_researches_such_as_the_recent/,Discussion
[R] Towards Optimal LLM Quantization,"picoLLM Compression is a novel LLM quantization algorithm that automatically learns the optimal bit allocation strategy across and within LLM's weights given a task-specific cost function. Existing techniques require a fixed bit allocation scheme, which is subpar.



Article: [https://picovoice.ai/blog/picollm-towards-optimal-llm-quantization/](https://picovoice.ai/blog/picollm-towards-optimal-llm-quantization/)



GitHub: [https://github.com/Picovoice/llm-compression-benchmark](https://github.com/Picovoice/llm-compression-benchmark)

",MachineLearning,16,0,1716994267.0,1d3e2r6,alikenar,https://www.reddit.com/r/MachineLearning/comments/1d3e2r6/r_towards_optimal_llm_quantization/,Research
"[P] MOMENT: A Foundation Model for Time Series Forecasting, Classification, Anomaly Detection and Imputation","A new foundation Time-Series model, suitable for multiple time-series tasks: 

[https://aihorizonforecast.substack.com/p/moment-a-foundation-model-for-time](https://aihorizonforecast.substack.com/p/moment-a-foundation-model-for-time)",MachineLearning,17,0,1716727588.0,1d10lma,apaxapax,https://www.reddit.com/r/MachineLearning/comments/1d10lma/p_moment_a_foundation_model_for_time_series/,Project
[D] Have someone tried to implement KANs from scratch?,"Recently I have been hearing a lot about this new architecture (kolmogorov-Arnold Networks) that might bring a new revolution in Deep Learning domain. 

Since many years MLP was the only architecture that was being used to solve any problem using neural networks, thus announcement of this new architecture is definitely a break through. Though many times in the past, lot of people had tried to do so but unfortunately they were unsuccessful.

If you still don't know about it, you could take help of following resources 👇🏻

Here is the research paper:
https://arxiv.org/abs/2404.19756

And the explanation video of this paper:
https://youtu.be/-PFIkkwWdnM

And if you have tried to implement it or found some video implementing it from scratch. Consider tagging the link in the comments. ",MachineLearning,15,7,1715670879.0,1crm54y,cyb0rg14_,https://www.reddit.com/r/MachineLearning/comments/1crm54y/d_have_someone_tried_to_implement_kans_from/,Discussion
[D] How do unets achieve spatial consistency?,"Hi,
I have been reading through unet pytorch implementations here https://github.com/lucidrains/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever „knows“ its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?

So when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?

I think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels „not enough“. 

Neither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.

So How can the unet achieve spatial consistency/spatial  auto-conditioning?

Thanks",MachineLearning,16,19,1715512124.0,1cq5g4r,Mr_Clueless_,https://www.reddit.com/r/MachineLearning/comments/1cq5g4r/d_how_do_unets_achieve_spatial_consistency/,Discussion
[D] How to train very shallow (dot product) networks with huge embeddings on a GPU cluster?,"In the olden days we used dozens of parameter servers and hundreds of CPU machines to train such heavy embedding light compute models and achieved impressive throughput. Nowadays with GPU clusters with high speed NVlink, looks like the throughput actually gets much worse. Of course I am talking about a dozen or so GPU machines each with say 8 A100. The tensor core utilization is very minimal (< 1%), but the GPUs are very busy due to all2all communication. I am trying to wrap my head around what the bottleneck maybe with the latter setup, is it simply that all2all (or ring all reduce etc) is intrinsically slower than parameter server when the number of parameters gets large, no matter how fast the nvlink is?",MachineLearning,17,6,1715405658.0,1cpa4io,Crazy_Suspect_9512,https://www.reddit.com/r/MachineLearning/comments/1cpa4io/d_how_to_train_very_shallow_dot_product_networks/,Discussion
[P] YARI - Yet Another RAG Implementation. Hybrid context retrieval,"
I made YARI. 

It features a hybrid fusion search between BM25 and Cosine Similarity and is built on top of Redis.

Uses: FastAPI, Celery and Redis.
OpenAI’s API support for embedding generation and prompt completion.

Please give me your feedback on it. 
Source: https://github.com/fighterbay/YARI",MachineLearning,18,6,1715085872.0,1cmb0x4,fighterbay,https://www.reddit.com/r/MachineLearning/comments/1cmb0x4/p_yari_yet_another_rag_implementation_hybrid/,Project
[D]What Nomenclature do you follow for naming ML Models?,"Hi All,

I am brainstorming some kind of a nomenclature for our team so that theres a standard way of naming ML models like their pickle files . Any inputs will be appreciated.

thanks",MachineLearning,15,12,1714149764.0,1cdq5cd,BravoZero6,https://www.reddit.com/r/MachineLearning/comments/1cdq5cd/dwhat_nomenclature_do_you_follow_for_naming_ml/,Discussion
"[D] How do you manage and track your large, evolving, image datasets?","I’m wondering how people manage the lifecycles of their large in-house datasets? Say >1TB and 100k files.

In my new role we have multiple production models trained from in house datasets ranging in size from a few thousand to a few hundred thousand images. We also have huge amounts of fresh data coming in, more than 1M images per day, and so we are constantly mining that and sending new tranches off to be annotated.

Until now the team has been largely left to their own devices to manage this and the results are predictable. In some cases we can’t associate our prod models with any specific data. Some of our core datasets exist only in people’s home directories, ripe to be wiped out by a single misplaced command. For one model, thankfully being sunsetted, both the training code and original training data are known to be lost.

Parts of the org have adopted DVC which seems pretty good until the number of files or overall size gets big. On one end, some stuff the entire dataset into just a few archives and track them. That minimizes frustrations with hashes but uses a lot of storage when only a few files get updated. On the other end, some people track every single file which lets files be individually updated but is a pretty big pain to check in and out. Others split the difference of these two approaches, tracking chunks of the dataset as archives hierarchically.

So how does your org manage this? What works and what doesn’t when working with these large & evolving datasets?",MachineLearning,17,8,1733789398.0,1haokqp,SirPitchalot,https://www.reddit.com/r/MachineLearning/comments/1haokqp/d_how_do_you_manage_and_track_your_large_evolving/,Discussion
[P] Text-to-Video leaderboard: Compare State-Of-The-Art Text-To-Video Models,"Unlike text generation, text-to-video generation involves balancing realism, alignment, and artistic expression. But which one is the most important in terms of output quality?

We don’t know, that’s why we created a voting-based Text-to-Video Model Leaderboard inspired by the LLM Leaderboard lmarena.ai.

Currently, the leaderboard features five open-source models: HunyuanVideo, Mochi1, CogVideoX-5b, Open-Sora 1.2 and PyramidFlow, but we’re aiming to also include notable proprietary models from Kling AI, LumaLabs.ai and Pika.art.

Here’s a link to the leaderboard: [link](https://t2vleaderboard.lambdalabs.com/leaderboard/).  
We’d love to hear your thoughts, feedback, or suggestions. How do you think video generation models should be evaluated?",MachineLearning,17,4,1733732225.0,1ha54m0,lambda-research,https://www.reddit.com/r/MachineLearning/comments/1ha54m0/p_texttovideo_leaderboard_compare_stateoftheart/,Project
[R] JAX vs TensorFlow-XLA ,"
Few months ago, I migrated from TF 2.0 to Jax. I found that jax is significantly faster than Tf. I noticed in the official documentation that it relies on XLA default that uses JIT compilation which makes execution faster. I also noticed that TF graphs also have option to enable JIT compilation with XLA. But still jax dominates TF with XLA. I just want to know why.",MachineLearning,17,7,1733540539.0,1h8j2e5,Odd-Detective289,https://www.reddit.com/r/MachineLearning/comments/1h8j2e5/r_jax_vs_tensorflowxla/,Research
[R] ReVersion: Learning Relation Prompts from Images for Controlled Diffusion Generation,"ReVersion introduces a novel approach for learning and transferring visual relationships using diffusion models. Rather than focusing solely on object appearance, it learns how objects interact with each other through relation prompts and specialized sampling techniques.

Key technical aspects:
- Uses frozen pre-trained text-to-image diffusion model as foundation
- Implements relation-steering through contrastive learning to guide prompts toward relationship-rich latent spaces
- Employs relation-focal sampling to emphasize high-level interactions over low-level details
- Creates relation prompts that capture spatial and interactive relationships between objects
- Introduces new benchmark dataset for evaluating relation inversion methods

Results:
- Outperforms existing methods in preserving object relationships while allowing appearance flexibility
- Shows strong performance on spatial relationships like ""on top of"", ""next to"", ""inside""
- Successfully transfers learned relationships to novel object pairs
- Maintains relationship consistency across different styles and contexts

I think this approach could be particularly valuable for improving automated image generation systems that need to handle complex scenes with multiple interacting objects. The ability to learn and transfer relationships, rather than just appearances, could help bridge the gap between current image generation capabilities and human-like understanding of how objects interact in space.

I think the relation-focal sampling technique could also have applications beyond just relationship learning - it might be useful anywhere we need to emphasize high-level features over low-level details in diffusion models.

TLDR: New method learns visual relationships from images using diffusion models, introduces relation-steering and relation-focal techniques, shows strong results on spatial relationship preservation and transfer.

[Full summary is here](https://aimodels.fyi/papers/arxiv/reversion-diffusion-based-relation-inversion-from-images). Paper [here](https://arxiv.org/abs/2303.13495).",MachineLearning,16,0,1733409005.0,1h7afj8,Successful-Western27,https://www.reddit.com/r/MachineLearning/comments/1h7afj8/r_reversion_learning_relation_prompts_from_images/,Research
"[D] We’ve crowd-sourced,  open-sourced, and made it easy to find so many tools to build with, but where is all this effort for context/scraping?","We have so many repos and libraries available to us for building, deploying, and using LLMs for tasks. We have hubs for models, plug-in-play libraries for things like LoRA and RAG, containerization for deploying models with APIs, extensions to integrate LLMs into IDEs and workflows, and plenty more. There’s stuff for managing and  orchestrating agents. 

Suffice to say, we have tons to open source tools to work to start working on both niche and general uses for LLMs.

That’s all great, but what I’m always having to build from scratch is getting context. Be that tools for online searches, webpage parsing (even common webpages that I know people would love to be easier to use for context), document parsing, etc.

I’ve been seen more cool projects pop up, but I’ve been seeing those projects provide details or implementation less and less on how they are finding, accessing, retrieving, and processing context.

There are plenty libraries to build tools for this purpose, but I just see less and less people sharing those.

Now I understand the context different projects need can be pretty niche, so reusability could be sparse. 

But is my perception wrong? Are there open-source resources for finding existing context extraction/scraping implementations or places to submit your own to make it easier for others to find?",MachineLearning,16,1,1732292786.0,1gxbrdm,Oscilla,https://www.reddit.com/r/MachineLearning/comments/1gxbrdm/d_weve_crowdsourced_opensourced_and_made_it_easy/,Discussion
[P] Two new open-weight (Apache 2.0) foundation models for multimodal product embeddings,"Today we open-weight (Apache 2.0) released the two best embedding models for ecommerce search and recommendations available anywhere. Marqo ecommerce models significantly outperform models from Amazon, Google, Cohere and Jina (see below).

\+ Up to 88% improvement on the best private model, Amazon-Titan-Multimodal (and better than Google Vertex, Cohere).

\+ Up to 31% improvement on the best open source model, ViT-SO400M-14-SigLIP.

\+ 5ms single text/image inference (A10g).

\+ Up to 231% improvement over other bench-marked models (see blog below).

\+ Evaluated on over 4M products across 10,000's of categories. Eval datasets are open sourced [here](https://huggingface.co/collections/Marqo/marqo-ecommerce-embeddings-66f611b9bb9d035a8d164fbb).

\+ Detailed performance comparisons across three major tasks: Text2Image, Category2Image, and AmazonProducts-Text2Image.

\+ Released 2 evaluation datasets: GoogleShopping-1m and AmazonProducts-3m.

\+ Released [evaluation code](https://github.com/marqo-ai/marqo-ecommerce-embeddings).

\+ Apache 2.0 [model weights available on Hugging Face](https://huggingface.co/collections/Marqo/marqo-ecommerce-embeddings-66f611b9bb9d035a8d164fbb) and to test out on Hugging Face Spaces.

  
Blog: [https://www.marqo.ai/blog/introducing-marqos-ecommerce-embedding-models](https://www.marqo.ai/blog/introducing-marqos-ecommerce-embedding-models)

GitHub: [https://github.com/marqo-ai/marqo-ecommerce-embeddings](https://github.com/marqo-ai/marqo-ecommerce-embeddings)

Hugging Face: [https://huggingface.co/collections/Marqo/marqo-ecommerce-embeddings-66f611b9bb9d035a8d164fbb](https://huggingface.co/collections/Marqo/marqo-ecommerce-embeddings-66f611b9bb9d035a8d164fbb)",MachineLearning,16,4,1731449698.0,1gpx4jz,Jesse_marqo,https://www.reddit.com/r/MachineLearning/comments/1gpx4jz/p_two_new_openweight_apache_20_foundation_models/,Project
[P] ML and LLM system design: 500 case studies to learn from (Airtable database),"Hey everyone! Wanted to share the link to the database of 500 ML use cases from 100+ companies that detail ML and LLM system design. The list also includes over 80 use cases on LLMs and generative AI. You can filter by industry or ML use case.

If anyone here is designing an ML system, I hope you'll find it useful!

Link to the database: [https://www.evidentlyai.com/ml-system-design](https://www.evidentlyai.com/ml-system-design)

Disclaimer: I'm on the team behind [Evidently](https://github.com/evidentlyai/evidently), an open-source ML and LLM observability framework. We put together this database.",MachineLearning,16,1,1730992542.0,1glsrh6,dmalyugina,https://www.reddit.com/r/MachineLearning/comments/1glsrh6/p_ml_and_llm_system_design_500_case_studies_to/,Project
[R] xLSTM hidden state is not used,"Hello to everyone, i was reading the xLSTM paper [**https://arxiv.org/pdf/2405.04517**](https://arxiv.org/pdf/2405.04517) in particular the section about mLSTM and I was wondering, where the hidden state is used? What is its utility? Normally it is used to compute the output gate.

https://preview.redd.it/7m51jnjhdzvd1.png?width=939&format=png&auto=webp&s=fc84cdcaac47110af22a86b86ff5390ef4e53a37",MachineLearning,17,5,1729460448.0,1g89uuh,splashhhhhhhhhhhh,https://www.reddit.com/r/MachineLearning/comments/1g89uuh/r_xlstm_hidden_state_is_not_used/,Research
"[R] DART can generate high-quality human motions in real-time, achieving over 300 frames per second on a single RTX 4090 GPU! It combines text inputs with spatial constraints, allowing for tasks like reaching waypoints and interacting with scenes.","1. Here's a link to the Project Page: https://zkf1997.github.io/DART/
2. Here's a link to the paper: https://arxiv.org/html/2410.05260v1

3. Here's My Question: I'm trying to recreate this paper in the form of a browser based app for me to play around with. Where would I find the motion data and text annotations necessary to train the VAE?",MachineLearning,17,1,1729147217.0,1g5kq1v,Hrombarmandag,https://www.reddit.com/r/MachineLearning/comments/1g5kq1v/r_dart_can_generate_highquality_human_motions_in/,Research
[D] focusing on one model at a time vs keeping up with state-of-the-art models?,"Current development of ML models are super fast that there are ""state-of-the-art"" models almost every week (I am not referring to ""state-of-the-art"" models claimed by the authors, I am referring to the models that become a hot topic in the field which everyone talks about), I feel that if I do not follow the discussion closely, I cannot keep up with them.  

  
I am thinking what would be a good way to **really learn and internalize these knowledge**, would it be good to just follow all hot papers/discussions such that my knowledge is not out of date, or I really need to sit down and **get my hands dirty** on some important models (e.g. ResNet, Diffusion model) **one at time** before I actually move to the next one?

  
Can you guys share what you think?",MachineLearning,15,7,1729088737.0,1g50n7m,Illustrious-Pay-7516,https://www.reddit.com/r/MachineLearning/comments/1g50n7m/d_focusing_on_one_model_at_a_time_vs_keeping_up/,Discussion
[D] A suitable system for Nvidia A100 40G,"# 

I’ve been in the machine learning space since 2019, and my current 4090 setup is no longer sufficient for my growing needs. Here's what I focus on:

* Fine-tuning large language models (LLMs)
* Training Hidden Markov Models (HMMs)
* Training Long Short-Term Memory networks (LSTMs)

Handling diverse data types including text, images, videos, and audio

I don't have much knowledge in rack servers or workstations, I would be looking to add more cards in the near future too(H100 or A100), and I have a budget of around $3000-4000 for the rack server or workstation(apart from the cards obviously)

PS. Evaluated almost every card out there, A100 was the winner in the majority of aspects so it became a clear choice. The runner up was L40S tho. Open to suggestions on the card preference too.",MachineLearning,16,18,1728336446.0,1fyj5sw,Dapper_Ad79,https://www.reddit.com/r/MachineLearning/comments/1fyj5sw/d_a_suitable_system_for_nvidia_a100_40g/,Discussion
Multimodal Fusion [P],"Hello, Im trying to do fuse together two image classification models, one is trained with RGB images while the other was trained using SAR images, both types of images come from the same data-set and represent the same.

Is this the correct way to implement late fusion? Im getting the same results with average, max and weighted and Im worried something is wrong with the way I did it. 

https://preview.redd.it/fot9jlznn2pd1.png?width=694&format=png&auto=webp&s=cde86f005a728d1782e95437cc357da7def854b5

  
",MachineLearning,17,8,1726449183.0,1fhsi7n,Icy_Dependent9199,https://www.reddit.com/r/MachineLearning/comments/1fhsi7n/multimodal_fusion_p/,Project
"[P] AI plays chess 6x6, new algorithm","I created a new machine learning algorithm to play board games and trained it to play 6x6 chess. After five days of training on a consumer-grade PC, I can't win a single game against it. Here's the GitHub link with the implementation, weights, and an interactive demo: [https://github.com/omikad/probs](https://github.com/omikad/probs) . Please advice other board games to try",MachineLearning,15,22,1725134396.0,1f5w23s,Putrid-Start-3520,https://www.reddit.com/r/MachineLearning/comments/1f5w23s/p_ai_plays_chess_6x6_new_algorithm/,Project
[P] txv : An explainability package for ViTs,"txv is a vision transformers explainability package. It provides CAM like visualization for vision transformers.

pip install txv

Github repository : https://github.com/LokeshBadisa/txv
Homepage : https://lokeshbadisa.github.io/txv/
Documentation : https://lokeshbadisa.github.io/txv/api_reference
Tutorials : https://lokeshbadisa.github.io/txv/tutorials",MachineLearning,16,4,1723219932.0,1eo3s7h,None,https://www.reddit.com/r/MachineLearning/comments/1eo3s7h/p_txv_an_explainability_package_for_vits/,Project
Survey Paper over Neuro-Symbolic AI with  Knowledge Graphs,,MachineLearning,17,2,1722423980.0,1egke1v,joestomopolous,https://arxiv.org/pdf/2302.07200,None
[R] Inverse GAN preserving weights of generator,"I'm looking for a GAN model where the generator can be inverted back to the latent space using the same weight parameters that the original GAN uses to generate fake data. Most invertible GAN papers that I looked at uses a separate encoder or some optimization problems.

I read this [survey](https://arxiv.org/pdf/2101.05278) paper on inverse GANs, but could not find a model that uses the same weights.

Note: I am aware that Diffusion Models could be a good fit for this. But I do not want to use a model with timesteps, looking for a single pass model.",MachineLearning,16,27,1722171618.0,1ee6w2s,Alternative-Talk1945,https://www.reddit.com/r/MachineLearning/comments/1ee6w2s/r_inverse_gan_preserving_weights_of_generator/,Research
[R] Scaling Diffusion Transformers to 16 Billion Parameters,"**TL;DR** Adding Mixture-of-Experts into a Diffusion Transformer gets you an efficient and powerful model.

**Paper:** [https://arxiv.org/pdf/2407.11633](https://arxiv.org/pdf/2407.11633)

**Abstract:**

>In this paper, we present DiT-MoE, a sparse version of the diffusion Transformer, that is scalable and competitive with dense networks while exhibiting highly optimized inference. The DiT-MoE includes two simple designs: shared expert routing and expert-level balance loss, thereby capturing common knowledge and reducing redundancy among the different routed experts. When applied to conditional image generation, a deep analysis of experts specialization gains some interesting observations: (i) Expert selection shows preference with spatial position and denoising time step, while insensitive with different class-conditional information; (ii) As the MoE layers go deeper, the selection of experts gradually shifts from specific spacial position to dispersion and balance. (iii) Expert specialization tends to be more concentrated at the early time step and then gradually uniform after half. We attribute it to the diffusion process that first models the low-frequency spatial information and then high-frequency complex information. Based on the above guidance, a series of DiT-MoE experimentally achieves performance on par with dense networks yet requires much less computational load during inference. More encouragingly, we demonstrate the potential of DiT-MoE with synthesized image data, scaling diffusion model at a 16.5B parameter that attains a new SoTA FID-50K score of 1.80 in 512×512 resolution settings. The project page: [this https URL](https://github.com/feizc/DiT-MoE).

**Visual Abstract:**

https://preview.redd.it/cq6yoqoeched1.png?width=1135&format=png&auto=webp&s=1985119b5150c76bb9807f4df45d7bb44e02bd2a

**Visual Highlights:**

https://preview.redd.it/8xf8egk9dhed1.png?width=1109&format=png&auto=webp&s=6e25b12d9a89d78847945068469f83cb45ef1eab

[1S, 2S and 4S in the middle panel refer to the number of shared experts](https://preview.redd.it/16h6rx7kdhed1.png?width=1135&format=png&auto=webp&s=d5700237d07a6d0631aa2595566122194427a795)

[MoE decreases training stability, but not catastrophically](https://preview.redd.it/fiax2361ehed1.png?width=1145&format=png&auto=webp&s=76d78d811ff9727c600a7502ff164d550ed1aae0)

https://preview.redd.it/s6cchx2nehed1.png?width=983&format=png&auto=webp&s=c426ce2f1362bace2b4d3abef8d7e5607d0ff405

",MachineLearning,16,2,1721833924.0,1eb3wos,StartledWatermelon,https://www.reddit.com/r/MachineLearning/comments/1eb3wos/r_scaling_diffusion_transformers_to_16_billion/,Research
[D] Optimizing models with C++/C,"Do you actually use C++ to optimise models in average solutions companies? 
If yes, is there a resource that you think is good to learn how to do that?",MachineLearning,14,25,1721742311.0,1ea87qq,AdOk6683,https://www.reddit.com/r/MachineLearning/comments/1ea87qq/d_optimizing_models_with_cc/,Discussion
[R] Can a computer vision pipeline be represented in ONNX?,"A pipeline described using PyTorch and torchvision, something like:

Contrast enhancement (histogram operation) —> edge enhancement (sobel etc) —> Neural Network (simple classification CNN)

I am interested in deploying such a computational graph to hardware using TVM.

Edit: huzzah it works (I think) please reply if I’m missing something. I’ll upload a git link for google collab notebook and test image.

Edit 2: I used histogram equalisation —> pre-trained imagenetv1 ResNet-18 classifier. Works as expected when executing ONNX graph via ONNX runtime in python. View .onnx file on Netron.",MachineLearning,14,15,1720772684.0,1e1cgqe,None,https://www.reddit.com/r/MachineLearning/comments/1e1cgqe/r_can_a_computer_vision_pipeline_be_represented/,Research
[R] Papers proposing ideas without results ? ,"Given the resources required to train some deep learning models, especially LLMs, how common is it to find a paper that proposes an idea without being able to show results due to unavailable resources ? In other words, if a researcher has a promising idea that they would like to propose for other research teams with more resources to possibly work on, is there any way they can do so while maintaining their contribution when/if positive results are shown ?  ",MachineLearning,16,37,1718728782.0,1div5a6,SingularityCharity,https://www.reddit.com/r/MachineLearning/comments/1div5a6/r_papers_proposing_ideas_without_results/,Research
"[News] Athens NLP Summer School, September 19-25 2024","https://preview.redd.it/nfjcry734b7d1.jpg?width=1033&format=pjpg&auto=webp&s=2118d94d8e415554e5de6e13055335e1ad711e50

**Link:** [**https://athnlp.github.io/2024/**](https://athnlp.github.io/2024/)

We're excited to invite everyone interested in Natural Language Processing (NLP) and Machine Learning (ML) to the 2nd Athens Natural Language Processing Summer School (AthNLP 2024). The event will be held at the campus of NCSR “Demokritos"" in Athens and is organized by NCSR ""Demokritos"", Athens University of Economics and Business, RC ""Athena"", and Heriot-Watt University. 

Building on the success of the first AthNLP in 2019, AthNLP 2024 will cover a range of NLP topics with a focus on ML methods. Expect morning lectures on theory, afternoon lab sessions on implementation and experimentation, and evening talks on research topics, along with demos and posters from participants and industry research labs.

Topics include classification, sequence prediction, linear models, neural networks, encoder-decoder architectures, machine translation, large language models, and multimodality.

# Target Audience

**Preliminary schedule:** [Check here](https://athnlp.github.io/2024/schedule.html)

* Researchers and graduate students in NLP and Computational Linguistics
* Computer scientists with an interest in NLP and ML
* Industry practitioners seeking a deeper understanding of these subjects

No prior knowledge of NLP and ML is required, though basic mathematics and Python programming skills are assumed.

# Important Dates

* **Application Deadline:** June 20, 2024 *(likely to be extended)*
* **Decision:** June 30, 2024
* **Registration:** July 30, 2024
* **Summer School:** September 19-25, 2024

# Features

* Social Event, daily lunch, and coffee breaks included in the fee
* Lectures by leading researchers in ML and NLP
* Optional poster sessions for students to showcase their work
* Demo day with technical companies and research institutions

# Confirmed Speakers

* Antonis Anastasopoulos, George Mason University
* Raquel Fernández, University of Amsterdam
* Ferenc Huszár, University of Cambridge
* Martin Krallinger, Barcelona Supercomputing Center
* Mirella Lapata, University of Edinburgh
* Ryan McDonald, ASAPP
* Aida Nematzadeh, Google DeepMind
* Vlad Niculae, University of Amsterdam
* Barbara Plank, Ludwig Maximilian University of Munich
* Anna Rogers, IT University of Copenhagen

# Participation Fees

* **300 EUR** for students (scholarship application avaialble)
* **400 EUR** for university professors or public institute researchers
* **500 EUR** for all others

For any questions, contact us at: [athnlp2024@athenarc.gr]()

We look forward to seeing you there!",MachineLearning,18,0,1718706624.0,1dineuj,yannisassael,https://www.reddit.com/r/MachineLearning/comments/1dineuj/news_athens_nlp_summer_school_september_1925_2024/,News
[R] AlphaMath Almost Zero: process Supervision without process,,MachineLearning,15,2,1718604297.0,1dhr5sy,hardmaru,https://arxiv.org/abs/2405.03553,Research
[R] Understanding LoRA: A visual guide to Low-Rank Approximation for fine-tuning LLMs efficiently. 🧠,"TL;DR: LoRA is Parameter-Efficient Fine-Tuning (PEFT) method. It addresses the drawbacks of previous fine-tuning techniques by using low-rank adaptation, which focuses on efficiently approximating weight updates. This significantly reduces the number of parameters involved in fine-tuning by 10,000x and still converges to the performance of a fully fine-tuned model.  
This makes it cost, time, data, and GPU efficient without losing performance.

[What is LoRA and Why It Is Essential For Model Fine-Tuning: a visual guide.](https://codecompass00.substack.com/p/what-is-lora-a-visual-guide-llm-fine-tuning)

https://preview.redd.it/v2plu0mvvw6d1.png?width=1456&format=png&auto=webp&s=e5f74bcb777d305c08bc74274b0c8a7cc63c973e

",MachineLearning,17,0,1718534093.0,1dh4s3x,ml_a_day,https://www.reddit.com/r/MachineLearning/comments/1dh4s3x/r_understanding_lora_a_visual_guide_to_lowrank/,Research
[D] ML System Engineering,"The recent WWDC event showcased the extraordinary system engineering by Apple that allows for user-friendly products, while still managing to use resource-intensive on-device language models (3-7B parameters). This was quite inspiring for me, especially as a PhD student where most of my projects only end up as a research paper!

I have a good theoretical background in ML, DL, and RL, and good knowledge of most state-of-the-art approaches. However, I have zero experience in getting deployable products out that use ML for decision-making in the backend. 

I was wondering if people here could point me to some good resources to learn more about ML System design and MLOps, and maybe some ideas for projects to get some more experience.",MachineLearning,18,2,1718206145.0,1de9glz,Personal_Click_6502,https://www.reddit.com/r/MachineLearning/comments/1de9glz/d_ml_system_engineering/,Discussion
[R] Google study says fine-tuning an LLM linearly increases hallucinations? 😐,"They prepare a QA task to observe hallucinations, on both Known examples (training instances similar to the info that the model has seen during its initial training) and Unknown examples (that introduce new info that the model hasn't been exposed to before).

They see that:

1. Unknown examples in the fine-tuning dataset bring down performance, the more you train, because of overfitting. They lead to hallucinations and reduce accuracy. Known examples positively impact performance.
2. Early stopping helps avoid this, which might mean that Unknown examples are neutral in shorter training.
3. The slower fitting of Unknown examples also indicates that models struggle to acquire new knowledge through fine-tuning.

Paper: [https://arxiv.org/pdf/2405.05904](https://arxiv.org/pdf/2405.05904)

I share high quality AI updates and tutorials daily.

If you like this post and want to stay updated on latest AI research, you can check out: [https://linktr.ee/sarthakrastogi](https://linktr.ee/sarthakrastogi) or my Twitter: [https://x.com/sarthakai](https://x.com/sarthakai)",MachineLearning,15,0,1718196691.0,1de5wa2,sarthakai,https://www.reddit.com/r/MachineLearning/comments/1de5wa2/r_google_study_says_finetuning_an_llm_linearly/,Research
[R] Bridging empirical-theoretical gap in neural network formal language learning,[https://arxiv.org/abs/2402.10013](https://arxiv.org/abs/2402.10013),MachineLearning,16,6,1717767809.0,1dab1ef,inland-1,https://www.reddit.com/r/MachineLearning/comments/1dab1ef/r_bridging_empiricaltheoretical_gap_in_neural/,Research
[R] Limits of Deep Learning: Sequence Modeling through the Lens of Complexity Theory,"Paper link: [https://arxiv.org/abs/2405.16674](https://arxiv.org/abs/2405.16674)

X thread: [https://x.com/NikolaZubic5/status/1797567892646470137](https://x.com/NikolaZubic5/status/1797567892646470137)",MachineLearning,15,1,1717599541.0,1d8s0yp,NikolaZubic,https://www.reddit.com/r/MachineLearning/comments/1d8s0yp/r_limits_of_deep_learning_sequence_modeling/,Research
[D] Comparing Darknet/YOLO and YOLOv10,"I recently published a video on YouTube showing some difference between Darknet/YOLO and Ultralytics/YOLOv10.

TLDR:  Darknet/YOLO is still faster and more precise than the most recent Python-based YOLO frameworks.

https://www.youtube.com/watch?v=2Mq23LFv1aM

If anyone is interested in Darknet/YOLO, I used to maintain a post full of Darknet/YOLO information on reddit.  I haven't updated it in a while now, but the information is still valid:  https://www.reddit.com/r/computervision/comments/yjdebt/lots_of_information_and_links_on_using_darknetyolo/",MachineLearning,15,2,1717539881.0,1d8a22c,StephaneCharette,https://www.reddit.com/r/MachineLearning/comments/1d8a22c/d_comparing_darknetyolo_and_yolov10/,Discussion
[P] Baysian bandits item pricing in a Moonlighter shop simulation,"I built a toy shop, modeled after the Moonlighter game's, and a Bayesian bandits agent to choose and price items for sale via Thompson sampling. 

Customer reactions (i.e. 'angry', 'sad', 'content', 'ecstactic') to these items at their shelf prices updated ideal (i.e. highest) price probability distributions (i.e. posteriors) as the simulation progressed. 

The algorithm explored the ideal prices of items and quickly found groups of items with the highest ideal price at the time, which it then sold off. This process continued until all items were sold.

The graph represents competitions between items to be placed on a shelf for sale.

The dots are prices (y-values) sampled from ideal price distributions (i.e. posteriors) for each item (color) in each competition (x-value). 

The highest sampled price, in each Thompson sampling contest, won and ended up on a shelf. 

The customer reactions I mentioned to items on the shelves updated bounds of these distributions, represented by lines of the same color.

For more information, many more graphs, and the link to the corresponding Github repo containing working code and a Jupyter notebook with Pandas/Matplotlib code to generate the plots, see my write-up: https://cmshymansky.com/MoonlighterBayesianBanditsPricing/?source=rMachineLearning",MachineLearning,17,0,1717349121.0,1d6idf5,JaggedParadigm,https://i.redd.it/177z3fjn074d1.jpeg,Project
[D] k=1 in KNN ,"Good evening , I tested the knn algorithm on an unbalanced test set after having trained it on a balanced one ; I get k=1 as the optimal parameter in terms of accuracy and I confirmed this result using cross-validation. Is it strange to have this value or not ?",MachineLearning,15,11,1716938497.0,1d2xrb8,Nice-Fisherman-1269,https://www.reddit.com/r/MachineLearning/comments/1d2xrb8/d_k1_in_knn/,Discussion
[R] Testing theory of mind in large language models and humans,,MachineLearning,15,1,1716705866.0,1d0vhzj,AhmedMostafa16,https://www.nature.com/articles/s41562-024-01882-z,Research
[Project] YOLOv8 quantization project,"I quantized YOLOv8 in Jetson Orin Nano. I exported it with TensorRT (FP16, INT8) and compared the performance. Based on YOLOv8s, the mAP50-95 of the base model is 44.7 and the inference speed is 33.1 ms. The model exported with TensorRT (FP16) showed that mAP50-95 was 44.7 and the inference speed was 11.4 ms. The model exported with TensorRT (INT8) showed that mAP50-95 was 41.2 and the inference speed was 8.2 ms. There was a slight loss in mAP50-95, but the inference speed was drastically reduced. There was a problem with calibration by exporting it with TensorRT (INT8), but the loss of mAP50-95 was minimized by increasing the calibration data. I tested with all base models of YOLOv8 as well as YOLOv8s.

[https://github.com/the0807/YOLOv8-ONNX-TensorRT](https://github.com/the0807/YOLOv8-ONNX-TensorRT)",MachineLearning,17,6,1716446351.0,1cymc41,Loud-Insect9247,https://www.reddit.com/r/MachineLearning/comments/1cymc41/project_yolov8_quantization_project/,Project
[Discussion] MICCAI 2024 decisions,"Hi all,

I thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.

I got a rebuttal invitation for an application paper and all the reviewers mentioned ""lack of technical novelty"" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?",MachineLearning,16,37,1715652616.0,1crh21y,possiblymonk,https://www.reddit.com/r/MachineLearning/comments/1crh21y/discussion_miccai_2024_decisions/,Discussion
[R] Better & Faster Large Language Models via Multi-token Prediction,"**Paper**: [https://arxiv.org/abs/2404.19737](https://arxiv.org/abs/2404.19737)

**Abstract**:

>Large language models such as GPT and Llama are trained with a next-token prediction loss. In this work, we suggest that training language models to predict *multiple* future tokens at once results in higher sample efficiency. More specifically, at each position in the training corpus, we ask the model to predict the following n tokens using n independent output heads, operating on top of a shared model trunk. Considering multi-token prediction as an auxiliary training task, we measure improved downstream capabilities with no overhead in training time for both code and natural language models. The method is increasingly useful for larger model sizes, and keeps its appeal when training for multiple epochs. Gains are especially pronounced on generative benchmarks like coding, where our models consistently outperform strong baselines by several percentage points. Our 13B parameter models solves 12 % more problems on HumanEval and 17 % more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to 3 times faster at inference, even with large batch sizes.",MachineLearning,16,4,1715335171.0,1coluve,None,https://www.reddit.com/r/MachineLearning/comments/1coluve/r_better_faster_large_language_models_via/,Research
[D] Does it make sense to talk about the probabilities of models?,"[https://lunaverus.com/programLikelihoods](https://lunaverus.com/programLikelihoods)

> There is a neat way to frame unsupervised learning as likelihood maximization, but not in the usual way where you just compute the likelihood of the data using a model and ignore the likelihood of the model itself. Rather, this is the combined likelihood of model and data...

Does it make sense to talk about the probabilities of ML models?",MachineLearning,16,3,1714179998.0,1ce1zfs,bouncyprojector,https://www.reddit.com/r/MachineLearning/comments/1ce1zfs/d_does_it_make_sense_to_talk_about_the/,Discussion
[D] Encode over 100 million rows into embeddings ,"Hey everyone,

I'm working on a pipeline to encode over **100 million rows** into embeddings using **SentenceTransformers**, **PySpark**, and **Pandas UDF** on **Dataproc Serverless**.

Currently, it takes several hours to process everything. I only have one column containing sentences, each under 30 characters long. These are encoded into **64-dimensional vectors** using a custom model in a Docker image.

At the moment, the job has been running for over **12 hours** with **57 executors** (each with **24GB of memory and 4 cores**). I’ve partitioned the data into **2000 partitions**, hoping to speed up the process, but it's still slow.

Here’s the core part of my code:

    F.pandas_udf(returnType=ArrayType(FloatType()))
    def encode_pd(x: pd.Series) -> pd.Series:
        try:
            model = load_model()
            return pd.Series(model.encode(x, batch_size=512).tolist())
        except Exception as e:
            logger.error(f""Error in encode_pd function: {str(e)}"")
            raise

The `load_model` function is as follows:

    def load_model() -> SentenceTransformer:
        model = SentenceTransformer(
            ""custom_model"", 
            device=""cpu"", 
            cache_folder=os.environ['SENTENCE_TRANSFORMERS_HOME'], 
            truncate_dim=64
        )
        return model

I tried broadcasting the model, but I couldn't refer to it inside the Pandas UDF.

Does anyone have suggestions to optimize this? Perhaps ways to load the model more efficiently, reduce execution time, or better utilize resources?",MachineLearning,16,4,1733477377.0,1h7xnce,nidalap24,https://www.reddit.com/r/MachineLearning/comments/1h7xnce/d_encode_over_100_million_rows_into_embeddings/,Discussion
